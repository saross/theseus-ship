'20250214 - discussion and utility.m4a'

Um
Okey dokey. Try this again.

So the structure of the paper is we have our discussion of why these things are not agentic. We have our discussion of the mundane utility and our experiences. Um
and our experiences of using it for research. And then we have the discussion of what what do we need in a tool that genuinely does the research?

How do we facilitate what these things are good at? How do we interact with these things knowing that they are deficient in judgment?

And so the first thing that we need is a cursored to-do list. [əˈkɜːsəd təˈduː lɪst] Right? The the first thing that we need is to to actively manage each step in the to-do list so that we can get the user's judgment as to correctness. Because the thing is is that for research, there will be dead ends, there will be blind alleys, and this is not, this is not the, 'Oh, I'm not able to access that on an unencrypted drive' that we saw in 7 February launch. [lɔːntʃ]

This is going to be, 'Oh, this line of thinking is going to be wrong'. This output is bad. This is not appropriate. I should remove this. Because fundamentally, fundamentally what we see is that these um
What language models are not capable of doing self-critical. They don't exhibit judgment. But more to the point, they don't have the inductive capacity, the intuition or whatever, to know when they should. And therefore, until we figure out some new technological advance that is not merely predicting the next token, as useful as that is, we need to anchor um that on human judgment.

So, we need to be able to move around our to-do list, right? It is it is insufficient to as as as the current {quote} agentic {end quote} things do, just take it step-by-step, right? Like the idea of a waterfall model of research is laughable. Ooh, yes. Uh, we should have that a note towards um the problems of SDLCs and why that's such a problem for us.
Um.
But.
This to-do list should should first off, allow us to see what's in the context window and to see what we're working on. But the the fundamental thing that these large language models don't have is the ability to forget. To judge that a certain line of thinking is bad and then to remove it. Because we know by virtue of how they work that these large language models will anchor on text in their context window. Which means that as they anchor on this text, if it's the wrong text, not only not only is it excluding um content that we want, but it's biasing the output in ways that's not appropriate. And so therefore, when we build literature for our deep research, we effectively need a an indication of what sources it has considered, what it's using, and more to the point, what it's disusing.
Um, and so.
So, effectively, considering that we're playing the game of of um limited context windows, and it's it's hilarious that 200,000 tokens is limited, but what we're talking about, I've I've easily blown the context window out with trying to drop in nine papers. So, 200,000 tokens is not sufficient to do research in. Therefore, we need a way of managing our tokens.
And what this means, and we also need introspection. We need to be able to validate this. We need, ha, transparency. Not merely in the encoded thoughts, but okay, at this step, we have a this paper, we are pulling these quotes from it. Here is our summary and understanding. Here is where in our document we should be using this paper. When we get to this part of our document, load the entire paper into the context window for this paragraph.
Right? And so this is why outlines are so useful is because they allow us to go, ah, when we get to that part of the outline, when we get to that part of the mechanism, whatever, that's not the right word, but whatever, go change your context window, your working context window, to go spin off a line of thought or whatever, to apply that paper to this paragraph within the context of the section and and the document. And so we need but we also need human judgment, right? The first draft is always shit.
The first lines of research are seldom the most correct lines. And so therefore these systems have to leverage human judgment. And and not in the double confirm that Deep Research does right now, right? Because Deep Research right now takes this, takes the entire context window, you push Deep Research, and then it it crunches down what it thinks its task is, and then you say go, or you give it a bit of feedback and it goes. That's not what I'm talking about, right? This is not agentic, it is not goal following. We cannot treat it as goal following. We must treat it as a tool. It's just a tool. It is a tool that can be useful, it's a tool that can be thoughtful. It's just a tool. Treat it that way.

Don't remember if I talked about deleting. But we also need tombstones. We need to have indications of thoughts that we've had that are bad. And we have to be able to remove them durably from the context window, but we need to have proof that we've had this thought and chosen not to have it.
Um which is ironic, but whatever.
Um.

Okay. What time is it?
and need to wrap up.
Um.
We have the outline, we have forgetting, we have... I don't know where we have forgetting. We have user judgment.

And then we have Cheers. And then we have

And then we have different stages of interaction where finding literature, extracting the literature, building the outline, building the first draft, revising the draft, editing, and then outputting should all be documented. And so this is why we we can't necessarily follow a waterfall SDLC when we're producing 10,000 word research reports. We have to consider instead that we have to treat each step in its own step, that we need to have different mechanisms for keeping track of this information. We need to choose which of these pots we're pulling from. And at every stage, we have to rely on human judgment knowing that the AI's judgment is deficient, but more to the point, not only is it deficient, it doesn't know that it's deficient, it doesn't know when to apply the judgment. Cool.

'20250214 - mundane utility.m4a'

I suppose I should speak to the mundane utility section.
Yeah, looks like. Okay.

So if we're talking about if we're talking about use cases, I suppose the first place we need to be talking about is data collection.
Um.
So this is this is specifically this is specifically based on the SoftwareX um run on 14th February.
And so so this this the SoftwareX run was the opposite of the Internet Archaeology one, right? So.
Um.
The SoftwareX run was the opposite of the Internet Archaeology run. SoftwareX has uh, 30-odd volumes filled with people writing about the software tools. But very few of them are archaeology, historical science related.
Therefore, my task to it was go find all of the articles in this journal that relate to big old tool list. [bɪɡ əʊld tuːl lɪst]
And And to a first approximation, it did that.
But the interesting thing here is it did that indirectly because those articles were not trivially visible because Elsevier blocks OpenAI's crawler. Uh, we need a to-do here um to compare against Computer Use.
And so it hit what seemed like a plausible set of articles, and even hit a plausible set of borderline articles.
I am not certain, um, and quite frankly, this is this is a a to-do Sean to to see what his double check um cross-check could show, because because if this list [ðɪs lɪst] Um, because if this was a useful series of articles, then in 20 odd minutes, three of which were blocked, we had a topic-specific reading.
US UK this this looking for systematic code that the include better find more on.
Because if we presume if we presume this to be the case, then you say, okay, Deep Research, go find me all articles from this journal that relate to this topic.
Now, the difference between this search and Jacob's search is one, this was bounded and well-specified. And quite frankly, a relatively small dataset.
Which means that if somehow you can hook the computer up to Web of Science, then we can use the regular bibliography approach. Um, to do reference peculiar bibliographies in [ə] reference section for appendices. Um.
For Winner. Because the problem with getting a paper and then finding all papers that reference it, and all papers it references, and through generations is that most of them most of them aren't aren't on research topics. Just going, yeah. So that most of them aren't on the research topic that we actually care about. Right? And so if we figure in a position Obviously, all of this is predicated on open access papers or papers that you have institutional access to. Noting that SoftwareX papers are open access, but because SoftwareX is not discoverable to OpenAI, they are only discoverable through inference. Um, which is stupid, because if it's an open access paper, then it whatever that's not relevant. Um.
Once we figure out how to confine for every article in a journal, Oh, I know what. No, I am dreaming. It's like this sort of filtering, this sort of filtering relation is useful in the aggregate. Um, When we told it to get real dynamic responses, but for every university in Australia, I know it did 17 of 35. Um, we don't know why it didn't do the other 17, sure, let's whatever. Um, although I do know that 17 is halfway + 0.5, [pɔɪnt faɪv] so I suspect maybe doing an AB test internally. But this discussion of utility here creates connects nicely back to and the offline connects nicely back to knowing if it's done something. Um, maybe submit feedback. Um, This systematic give me, you know, every single instance of X because X can be specified, I think that we also need to highlight that the literature review mode and to do Sean get that listed back way in on the the GitHub site. Um, but the literature review mode actually hides more than it helps. Sure, we get a 10,000 word um thank you median essay and report and uh we needed to give acknowledgements to CoPilot Vision for not analogy. Um.
But the the fact that go through every paper and determine those ones about this paradigm is useful. This is of course also a question given that our PhD sequence suggests knowing about literature. Um, should they not simply be reading every paper? And like, is it not better to have them do this literature review by hand? But I think that that is sufficient for effective use cases. Obviously how you adjust for your knowing. Like this is important. This discussion of utility and how it connects nicely back to knowing if it's done something should be part of the PhD supervisor's um toolkit. And then pulling back comments like this uh is useful where my previous notes fix that the third activity students do badly and give you awesome results that have no bearing on reality is that sure, marginal papers or borderline papers, it is far more interesting for the PhD student to read the output models and then to verify and cross-check the output models than simply reading I don't know. Actually prod, um get this, can you, can you engage with some co-pilot argumentation here? Now I see that the canonical places and it comes back to this one. Yeah. But I think that that is sufficient for effective use cases.



'20250217 - Judgement epistemic humility.m4a'

Judgment as epistemic humility, I.e., the ability to know when the relative epistemic quality of each of its um starting over. Okay, so large language models lack judgment. In this case, we can we can model the judgment that they lack in in three axes. And and um, uh, to-do note, uh, cross reference that student judge formalism uh paper that came out recently. Um. Fuck.
Ah, my brain's not here. So, one, they epistemic uh they lack epistemic humility. So, they don't have a sense of not knowing. Um. Pause.
Eh, is this still going?
Yes. Okay.
Pants, pants, pants.
And so here, we can kind of talk about the the problems of confabulations. Um, because again, the the issue is not the confabulation itself. This is like people are allowed to be wrong. There's there's nothing wrong with being wrong.
There's nothing wrong with being wrong.
The epistemic humility action or fraction of of judgment though is the ability to doubt your own words. Now, obviously, as these things cannot elide their own context windows, which I'm actually kind of surprised hasn't come up, but whatever. Um, this would need to be an intentional step. The problem is large language models exhibit very poor epistemic humility on their own output. And and I I I describe this ow as a class. Um, just as that they are bad at detecting their own output. They're also bad at calling doubt on their own output. And again, with all of this, the impulse for judge the impulse to judge, the impulse to self-critique does not does not is not a pattern exhibited in their training data. And for obvious reasons, published work doesn't have that thought pattern in it because it is the consequence of that thought pattern. Um, uh some reference for episodic versus procedural memories. Um, and so when we talk about this, this in terms of a failure of judgment, it is both and and then the subset of epistemic humility, it is both the lack of the impulse to self-critique and lack of the ability to self-critique. Um.
And while and while asking a model to engage in proleptic reasoning, uh reference my recent paper to do. Um, proleptic reasoning is an argumentation approach if you already have sufficient epistemic grounding. And it is identifying this precursorial whatever that word, yeah, sure why not. Precursorial lack, precursor lack? Eh, one of them. that is the problem.
The second problem is the lack of induction. And again, 03 mini high showed that it's better at pattern matching when prompted to do so. Um, maybe have an appendix on on Zendo, right? But like a lot of the point of induction is not, 'Oh, I am going to try to perform induction right now'. It is not a for the vast majority of cases, specific intentional activity. More it is that we can claim that induction induction quad judgment is the precursor to common sense. Um, it is the it is the precursor to noticing that there is something in what you have produced because again, all of this is all of this is in the context of producing the next token. We're not we're not doing any theory of mind stuff here.
But induction in this instance is is the act of noticing that while one of the elements of what you've written looks plausible, it lacks coherency relative to what you have written. It it you you notice it in terms of uh judgment and induction, induction is noticing the deviation from pattern and the impulse of inquiry for that deviation. Okay. Awesome. I didn't actually say what the third category was. Um.
Let's work this out from first principles. So we have epistemic hu- humility. We have induction. And And yes, we can say here, induction is the impulse precursor to common sense. It is the impulse notice noticing when something you've written doesn't follow a pattern.
We can say that the third option here or the the the third element of judgment as needed in LLMs is ironically, we could say something like agency. We can't say that, that that word's overloaded in this context.
We can kind of talk about Rumsfeld's epistemology here and talk about known unknowns and known knowns and so on and so forth.
And how epistemic humility is in many ways a acknowledgement of known unknowns.
But the third aspect of judgment that even reasoning models are lacking is And we can't really we can't really say goal checking, right? Like we we can't we can't make this a circular argument.
All right, let's just work through this in a case study, right? So, thinking out loud, they always give you an answer.
Right. So, so we can say they always give you an answer and that answer is always presented as correct.
And we can say that that evinces a lack of epistemic humility.
But it feels like there's a different category here.
It feels like that that category is not just epistemic humility.
Ah, prospective versus retrospective. So there there is a a prospective epistemic humility which is to say I don't know enough to answer that question.
And then there then there's the faux humility that we see in reasoning models, especially in stuff like R1, where you have 'Wait, but what about this?', right? And and and that language of 'Well, what about this?' displays epistemic humility.
But I would actually assert that that performance of epistemic humility is the pretense of epistemic humility rather than the fact of it. Because the thing is, when when R1 is doing, 'Well, what about X?' It is effectively It is effectively trying to cause itself to describe an option space of possible answers.
It is it is effectively motivating a deductive reasoning process here. And LLMs are entirely adequate at deductive reasoning, right? They can transform data. That if they have to motivate themselves to explore a decision tree or an option set with with the performance of humility, sure, whatever.
But the reason why it is the performance of epistemic humility and not actual epistemic humility, is that process never ever results in it saying, 'I don't know'. I or I mean, look, and and we can turn that down. We could say, 'Oh, I don't have enough information to to come to this conclusion.' It never it never actually acts upon its notice of deficiencies. It always uses that language of 'Well, what about?' to create more options, and those options can be contrary to the other options and I have no fundamental issue with this performance of of epistemic humility just like it can engage in the performance of judgment.
It is the phrase, 'The proof of the pudding is in the eating'. It is Yes, it is the phrase, 'The proof of the pudding is in the eating'. It is when we are engaged in something that we are uncertain on, or more to the point, when we judge that our knowledge of the thing is insufficient to epistemically differentiate between possible realities, we run experiments. And this is the fundamental flaw of deep research. It's Aristotelian. Ooh, I like this. Okay, so this is another one, that two an AI, text and authority has primacy over reality. I mean, it it has to. It's that's that's baked into the thing itself, which means that oh I love this. AI's are fundamentally Aristotelian that we make arguments or sorry that AI makes arguments by looking for authorities. And and and this is in mostly in deep research that we've noticed that there's no real appeal to authority in, for example, the model Claude. Um, sorry Claude, this is me talking about not you, not to you. Um.
But these models never These models never engage with the real.
These models never engage with the real.
I mean, obviously, they're they're they're digital thing. But this they never Besides saying 'I don't know', they also never say 'I can't know'. Yes, okay, and this is the third category.
Right? That when we It is the Aristotelian fallacy or paradigm work on it, um, that if an August source makes the claim, we don't have to investigate the claim.
Right? And so the fundamental thing here, Oh, I should pause.
So to conclude, we can say that the Aristotelian fallacy is that the world doesn't matter. That an AI will never ever look at its words and go, 'Nah, those those weren't good. Let me throw them out instead of giving them to the user'. And it will never go, 'Well, there are two possible courses of action here. Here's what you should be looking for in the world to differentiate between them'. It does sometimes say that there's two opposing arguing authorities, but again, this is still the Aristotelian issue is that we can we can acknowledge fighting authorities, we just can't acknowledge an authority being wrong about the world unless it says so. And so it'll never say, 'Well, go try this, see if it works, report back'. It will never say, 'Well, I can't get that paper. Could you go get it for me?'
And so the flaw in judgment is that it always has to deliver a coherent and final answer. And again, all of this can be circumvented if the human is injecting this into their prompts. And so if we if we cause it, it can certainly perform all of these functions. The proof of this is not does it, can it pretend, can it can it have pretense to these functions. The basis of this is does it act this way in the absence of other stimuli? And even even if we push it, does it act appropriately? Because the problem is is that in many of these tests, it can use the words of the thing without engaging in the fact of the thing. But that's another discussion.



'20250220 - Obs deep research.m4a'

Sound check, sound check.
Sound check, sound check.
Okay.

So, observations of Deep Research.
Okay, so Anthropic's Computer Use was about 80% of the way to a workable prototype. Um.
It simply was too expensive for its capabilities. Elicit was an exceptionally disappointing experiment. Again, talking about the capabilities of their LLMs. Like setting apart everything else, setting apart their problematic reliance on Semantic Scholar and so on and so forth.
So the largest specific fundamental flaw of all of these systems is their corpus.
Um, even Yeah, is their corpus.
For Deep Research, the Internet Archaeology iterator seemed quite promising.
Although the the frequent cajoling to be systematic really started showing the seams.
However, it was the exploration of the Journal of Open Source Software that really demonstrated the fundamental flaws of the system. Because here's the thing, corpuses can be fixed, right? Corpus access can be fixed. The fundamental lack of systemization, even when explicitly instructed to probably as a function of some of the back-end prompting that OpenAI has added means that it simply will not work down a list of sources, which are open, right? This is a list of sources that was found by Deep Research. They're all in the same journal, they are all well formatted.
And so therefore, this fundamental flaw of the lack of systemization shows that Deep Research is not presently at the level of a research assistant. Um, I will also note that there are some really interesting introduced biases um from OpenAI's {quote} policies {end quote} that when it is thinking, especially as when it is mulling over the policy, the text of the policies infects the output. So, as an aside, I was transcribing a recent presentation of mine about AI, and OpenAI O1 Pro quite clearly took the text from one of the policies and put it in the place in in um what not place, inferred. My inference is that OpenAI took the place of one of the policies and replaced it with one of my questions. And so the problem is again and again, because we don't have a separation of concerns and because and because all of these All of So because we don't have a separation of concerns, because we don't have letting the sources speak for themselves, the current mechanisms of Deep Research are fundamentally flawed. Furthermore, and a interesting deviation from Claude, getting O1 to work on its own prompts does not, in fact, improve output quality. It has a very bad estimation of its own output.
And so while while there is why would you do that? Um.
And so while we have these things, and while I assert that O1 and O3 mini high represent a difference in degree and in kind from prior models such that the way I interact with them has become a function of the way I would interact with PhD students. The fundamental limitations, especially the frequent invocations to the spirits um to to think harder, which as an aside made me feel like a Doc Smith character, suggest that these models still lack the judgment and these models still lack the insight to know when they have done a good job.
Um. So what we what would we need in a system for it to improve. We need multiple supervisory threads. If policy enforcement is the deal that it needs, which is fine, it needs a supervisory thread to review and edit. It needs a intrusive thought thread for when it's looking for validation. And the validation should not be wholly contained within the context window, but instead the validation uh should be threaded out, then there needs to be a corpus where under the context of going to find things, it needs to be to build a document store where it can collect and build a corpus without necessarily all of that corpus living in the context window at any given time. We need to be able to inspect that corpus in a much better than train of thought mode. And furthermore, the finding and the validation should be um threaded out, then there needs to be a corpus where under the context of going to find things, it needs to be to build a document store where it can collect and build a corpus without necessarily all of that corpus living in the context window at any given time. We need to be able to inspect that corpus in a much better than train of thought mode. And fundamentally, it needs to dis- um it needs to figure out when its first draft is unacceptable and when it needs more research. But it also needs to be able to indicate sources in its corpus should not be considered for a draft. It needs more structure, it needs to be able to make outlines and write to the outlines. It needs to have an obvious editing pass. It needs to be a real boy.



'20250225 - current capabilities tool-using nonagentic.m4a'

Okay.
At current capabilities, as of February 2025, the tool using, but not agentic, Deep Research and its clones by Perplexity and Hugging Face are deeply flawed, deeply impressive tech demos.
In this paper, we will discuss why we believe that the term agentic does not appropriately apply to these tools, save as a term of marketing, and why we believe that these tools represent a difference in degree and in kind much like the the tech demo of Bing Sydney in February 2023.
To be clear, these tools have the capabilities to genuinely assist researchers in research, so long as said researchers do not believe any of the marketing hype of the companies that produce them. By default, the 'literature reviews' {in quotes} produced by these tools are descriptive, superficial, and fundamentally misdefined.
To remind ourselves of methods classes, the point of a literature review is effectively to do two things. One, it is to situate the research within a specific and known scholarship. This is both an argument what? This is both an argument by authority, i.e., these other scholars have have investigated the same thing, have engaged with the same thing. Therefore, this is an appropriate thing for me to be doing as it is a claim to disciplinarity. The way I will be investigating it is this, the research program in the Lakatoshian sense with its concomitant hard core and soft outer belt is that. All of these things are accomplished by the first prong of the literature review. The second prong of the literature review is informally speaking, why do we care? Or why has this question not already been answered? Or again, where's the novelty? The fundamental requirement of both of these prongs is judgment. It is insufficient to merely describe what papers have said. A literature review is not a descriptive summarization of the literature. A literature review is the process by which researchers build a case for why this question should be answered. Now, can we say that a descriptive literature review is appropriate for other fields? No. I don't need to say that. I know what I'm doing. Um.
So, what we what would we need in a system for it to improve? It needs multiple supervisory threads. If policy enforcement is the deal that it needs, which is fine, it needs a supervisory thread to review and edit. It needs a intrusive thought thread for when it's looking for validation. And the validation should not be wholly contained within the context window, but instead the validation uh should be threaded out. Then there needs to be a corpus where under the context of going to find things, it needs to be to build a document store where it can collect and build a corpus without necessarily all of that corpus living in the context window at any given time. We need to be able to inspect that corpus in a much better than train of thought mode. And fundamentally, it needs to dis- um it needs to figure out when its first draft is unacceptable and when it needs more research. But it also needs to be able to indicate sources in its corpus should not be considered for a draft. It needs more structure. It needs to be able to make outlines and write to the outlines. It needs to have an obvious editing pass. It needs to be a real boy.


'20250228 - Agentic.m4a'

Okay.
Okay.
These systems claim to be agentic, specifically possessing agency.
If we define possessing agency as the ability to do task decomposition, i.e., take a task, split it up into subtasks, and figure out a route that could plausibly accomplish those subtasks, we could call these systems agentic. However, agency should not be viewed as a function of task following, but it should be viewed as a function of goal accomplishing. And this this distinction is important and nuanced.
Uh, I'll clean it up in the text. But the subtask the subtask mechanism that has been popularized by things like Langchain, the um, the reification of chain of thought in reasoning models, we can have other words for these things than agentic.
Because my fundamental critique of the agentic definition is if we claim a space delineated by XML tags with 'think' in it is reasoning, sure, fine. Reasoning is a component of intelligence.
And the problem here is that if we are asserting components of intelligence stand in for the whole thing, then it's not going to work.
Ah, that's messy. I don't like it. Anyway, we'll figure it out. Um.
Observations of Deep Research. Okay, so Anthropic's Computer Use was about 80% of the way to a workable prototype. Um.
It simply was too expensive for its capabilities. Elicit was an exceptionally disappointing experiment. Again, talking about the capabilities of their LLMs. Like setting apart everything else, setting apart their problematic reliance on Semantic Scholar and so on and so forth.
So the largest specific fundamental flaw of all of these systems is their corpus.
Um, even Yeah, is their corpus.
For Deep Research, the Internet Archaeology iterator seemed quite promising.
Although the the frequent cajoling to be systematic really started showing the seams. However, it was the exploration of the Journal of Open Source Software that really demonstrated the fundamental flaws of the system. Because here's the thing, corpuses can be fixed, right? Corpus access can be fixed. The fundamental lack of systemization, even when explicitly instructed to, probably as a function of some of the back-end prompting that OpenAI has added, means that it simply will not work down a list of sources, which are open, right? This is a list of sources that was found by Deep Research. They're all in the same journal, they are all well formatted.
And so therefore this fundamental flaw of the lack of systemization shows that Deep Research is not presently at the level of a research assistant. Um, I will also note that there are some really interesting introduced biases um from OpenAI's {quote} policies {end quote} that when it is thinking, especially as when it is mulling over the policies, the text of the policies infects the output. So, as an aside, I was transcribing a recent presentation of mine about AI, and OpenAI O1 Pro quite clearly took the text from one of the policies and put it in the place in in um what not place, inferred. My inference is that OpenAI took the place of one of the policies and replaced it with one of my questions. And so the problem is again and again, because we don't have a separation of concerns, and because and because all of these All of because we don't have a separation of concerns, because we don't have letting the sources speak for themselves, the current mechanisms of Deep Research are fundamentally flawed. Furthermore, and an interesting deviation from Claude, getting O1 to work on its own prompts does not, in fact, improve output quality. It has a very bad estimation of its own output.
And so while while there is Oh, why would you do that? Um.
And so while we have these things, and while I assert that O1 and O3 mini high represent a difference in degree and in kind from prior models, such that the way I interact with them has become a function of the way I would interact with PhD students. The fundamental limitations, especially the frequent invocations to the spirits um to to think harder, which as an aside made me feel like a Doc Smith character, suggest that these models still lack the judgment and these models still lack the insight to know when they have done a good job.
What would we need in a system for it to improve? First and fundamentally, for Deep Research to be effective, it cannot have a single train of thought. It needs multiple supervisory threads. If policy enforcement is the deal that it needs, which is fine, it needs a supervisory thread to review and edit. It needs a intrusive thought thread for when it's looking for validation. And the validation should not be wholly contained within the context window, but instead the validation uh should be threaded out. Then there needs to be a corpus where under the context of going to find things, it needs to be to build a document store where it can collect and build a corpus without necessarily all of that corpus living in the context window at any given time. We need to be able to inspect that corpus in a much better than train of thought mode. And fundamentally, it needs to dis- um it needs to figure out when its first draft is unacceptable and when it needs more research. But it also needs to be able to indicate sources in its corpus should not be considered for a draft. It needs more structure. It needs to be able to make outlines and write to the outlines. It needs to have an obvious editing pass. It needs to be a real boy.


'20250316 - skip.m4a'

Okay.
Agentic definitions in so far as the original question relates to possessing Okay, the original question was about epistemic humility, the ability to find myself clearly defining myself. Judgment versus information quality versus confidence quality. But epistemic humility is where we're going to go. Okay, general ability. My definition of agentic is when a process has the task following capabilities wherein always consists in its core processes. It receives user input. It may or may not ask for clarification. It may or may not rephrase the plan. But once it is executing that plan, once it is collecting resources and transforming resources into some kind of output, it then relies upon is transformed into its answer. It then relies on the {quote} text in its context window {end quote}. This stop is transformed This stop is transformed it then relies on text in its context window as its source of content to transform into its answer. Excuse us. Thank you. Is that an observation? These tools are called deep research in all their various modes. The implication of these tools or the the default output of these tools is the report and that is not appropriate. Um. It does not sufficiently address conflicting context. It does not sufficiently address contradictions, just says choose whatever is better. Um. However, we must therefore that if a human has made the decision to prompt them to do this, the most likely outcome of this is that they will perform an action that looks as if they are doing this. When I say that they don't exhibit judgment, it is that the impulse to edit, the impulse to self-critique does not does not is not a pattern exhibited in their training data. And for obvious reasons, published work doesn't have that thought pattern in it because it is the consequence of that thought pattern. Um. Uh, some reference for episodic versus procedural memories. Um. And so while asking a model to engage in proleptic reasoning, uh reference my recent paper to do. Um proleptic reasoning is an argumentation approach if you already have sufficient epistemic grounding. And it is identifying this precursorial whatever that word, yeah, sure why not. Precursorial lack, precursor lack? Eh, one of them. that is the problem. It is the phrase, 'The proof of the pudding is in the eating'. Yes, it is the phrase, 'The proof of the pudding is in the eating'. It is when we are engaged in something that we are uncertain on, or more to the point, when we judge that our knowledge of the thing is insufficient to epistemically differentiate between possible realities, we run experiments. And this is the fundamental flaw of Deep Research. It's Aristotelian. These models never engage with the real. I mean, obviously, they're they're they're digital thing. But this they never Besides saying 'I don't know', they also never say 'I can't know'. That when we it is the phrase, 'The proof of the pudding is in the eating'. That if an August source makes the claim, we don't have to investigate the claim. And so if this pattern of the primacy of source based on our evaluation of the source quality is not how current paradigms in the Kuhnian sense or research programs in the Lakatoshian sense, um, defend their theories. Uh, I don't re Oh, I'd love to footnote um that aside here, but I think I think we don't want to go too PhilSci here. Um, okay, so fundamentally, we need to acknowledge that large language models at present, even with {quote} reasoning {end quote} capabilities and {quote} agentic {end quote} systems, because even though that they can perform a deductive task decomposition, the fact that they cannot effectively exercise judgment, specifically that they cannot they do not as a matter of course exercise judgment over what they have written, nor do they nor are they able to assess the quality of their own inference. What this means is is that a large language model when prompted to edit its own writing, um will double down or intensify but will never question the basic precepts. When I talk about editing here, especially in the context of {quote} Deep Research {end quote}, I am not talking about the process by which we improve the tone or grammar of the output. I'm talking about the question of, 'Do we consider that this process is adequate?' And so the task decomposition shown by {quote} reasoning {end quote} models shown by telling it to think step by step, shown it to say do functional decomposition, any one of these things is certainly a component of good research, of of the process, or a component of intelligence. But even if it's just a component of intelligence, calling these things agentic leaves us a very large gulf between agentic goal following and goal setting systems. And despite claims by OpenAI that we can consider AGI a function of monetary output, which is a silly definition, if we say that in this hierarchy, task decomposition to reasoning, reasoning to goal following, claiming claiming um claiming that a agentic system may either act as our agent or act as something with agency, noting that those are two very, very different things. Both of them require the system to have the capability of exploring if the goal which has been set by external agency has been accomplished, and if it has been accomplished well. We must differentiate goal accomplishment with the basic QA processes of Anthropic's Computer Use checking the screenshot for the output of one of its commands. Here, we are not talking about the observe-orient parts of Carnapian or UDA loops. Right? Even even in that most simplistic, most simple task, the causal arrow is reversed where the system decides on the rightness of a task, clicks on a thing, and then justifies itself. This causal arrow is not agency. Nor is it deciding nor is it able to make an appropriate judgment call if if it is proceeded towards its goal. It is able to say whether or not it has succeeded towards what it thinks its steps are, but it is not able to situate itself effectively into reality. And this lack of situation means that it cannot perform as our agent, nor does it have self-agency in determining how to accomplish, accomplishing, and then assessing if a task has been accomplished. Therefore, it is inappropriate to call our current infrastructure agentic. And therefore we can say that AGI, Artificial General Intelligence, is amongst other qualities not articulated here, necessarily goal setting as well as goal following. I would note, however, that an effective goal following system would likely be capable of accomplishing the monetary gains that Microsoft has set as their definition of AGI. Simply by processing or adding more computers. And so therefore, our philosophical basis and our philosophical definitions should be considered a completely disjunct ontology from those being peddled by Silicon Valley. To say this again, for Deep Research to, in fact, be a useful research assistant, it needs a number of components. The first and fundamental is the ability to figure out what the research questions are within the prompt. So, not what the prompt's questions are, but what the fundamental research questions implied by the prompt such that research can be done. Noting that a literature review, if done well, is a persuasive document which situates a text in a literature, identifies the research questions, identifies the context in which the research questions will be answered, and most importantly, identifies the gaps. This requires epistemic humility that these systems flatly do not exhibit. They can certainly play at it, but then playing at epistemic humility is not, in fact, the same as them having it. The performance of a virtue is not having the virtue, which is kind of the whole problem with large language models. So, the first component is for them to look at any stage of output and decide if that stage of output, again, to have judgment, to induce from data the general pattern, they must decide if that output is appropriate within the implied tacit context of the querent's question. If a system, if Deep Research cannot return, 'Ha, I seem to have gone down a blind alley. Let me, can you help clarify some things that I'm confused about so that I can search again?' it's not Deep Research. And now, let me let me be super clear here. We are not saying that it was unable to find things. Right? We're not saying, 'Oh, I didn't get any search results. You're user, hey user, your query is bad'. I'm saying the assumptions that I made in my searching were incorrect. Self-criticality here is essential. Along that line of self-criticality, we must then look at editing. And so when these systems engage in


\section{Section 6 questions}

From AI:

For your walk to work, I'd recommend focusing on **Section 6: Research Tool Design Requirements**. This constructive section needs more specific details about how research tools should be designed to compensate for AI judgment deficiencies.

Specifically, please elaborate on subsection 6.2 (Research Process Support):

```
\subsection{Research Process Support}
- Stage-based design (literature finding, extraction, etc.)
- Non-waterfall approach to research flows
- Structured to-do lists with human checkpoint integration
- Outline-driven context switching
- Documentation of decision points
```

Consider addressing these questions in your dictation:

1. What would an effective stage-based design look like in practice? How should transitions between stages work?

2. You've mentioned that research can't follow a waterfall SDLC - what alternative workflow model would be more appropriate? How would it accommodate iteration and refinement?

3. How exactly should human checkpoints be integrated? At what specific points in the research process is human judgment most critical?

4. How should outline-driven context switching work in practice? How would the system know which sources to load based on outline position?

5. What documentation of decision points would be most valuable for researchers reviewing an AI's process?

This section bridges your theoretical critique with practical recommendations, making it particularly valuable for the paper's contribution.

\subsection{Transcripts}


'20250318 - section 6.2 part 1.m4a'
'20250319 - Section 6 .m4a'
'20250321 - Section 6 -.m4a'
'20250324 - human judgement.m4a'
'20250324 - Section 6 q 3.m4a'

\section*{Question 1: What would an effective stage-based design look like in practice? How should transitions between stages work?}
\subsection*{Question 2: You've mentioned that research can't follow a waterfall SDLC - what alternative workflow model would be more appropriate? How would it accommodate iteration and refinement?}
%Combined Q1 and Q2 as they were answered together in the audio

Okay.
Uh, so to conclude, we can say that the Aristotelian fallacy is that the world doesn't matter.
That an AI will never ever look at its words and go, nah, those those weren't good. Let me throw them out instead of giving them to the user.
And it will never say, well, there are two possible courses of action here. Here's what you should be looking for in the world to differentiate between them.
It does sometimes say that there's two opposing arguing authorities, but again, this is still the Aristotelian issue is that we can we can acknowledge fighting authorities, we just can't acknowledge an authority being wrong about the world unless it says so. And so it will never say, well, go try this, see if it works, report back. It will never say, well, I can't get that paper. Could you go get it for me?
And so the flaw in judgment is that it always has to deliver a coherent and final answer.
And again, all of this can be circumvented if the human is injecting this into their prompts. And so if we if we cause it, it can certainly perform all of these functions.
The proof of this is not does it, can it pretend, can it can it have pretense to these functions. The basis of this is does it act this way in the absence of other stimuli?
And even, even if we push it, does it act appropriately?
Because the problem is is that in many of these tests, it can use the words of the thing without engaging in the fact of the thing. But that's another discussion.
Section six, um questions again. Let's see here.
You've mentioned that research can't follow a waterfall SDLC. What alternative workflow model would be more appropriate? How would it accommodate iteration? Fine.
This question is the wrong question, fundamentally. It is that deep research {quote} workflows cannot follow waterfall methodology.
Because they work out their plan first-drafty [fɜːst drɑːfti], they collect some amount of data. But they're not like the critique of this is that they are not systematic about the data that they are collecting. It is it is rare it is seldom that the case that the first couple of hits on the topic. Now, again, this may or may not be bubbling up, um, it's probably using a synthetic summary of the hits because we don't necessarily want the search result in the context window. Yet.
Basically, rather than treating Deep Research as a single train of thought that wanders and hops and skips through the literature, it is something that can be systematic, that can create sub processes that can go out, preferably using the academic's either files or credentials, go find real papers, go use Web of Science, so on and so forth, and then builds a wiki as its reference. Because it is important to have a journal of what worked and what didn't work. The fact that the process of producing these reports is hidden away removes significant amounts of utility and plausibility from these reports. Especially with how naff [næf] the the reports are. And so having having a wiki that can be navigated as part of this is fantastic. Obviously, it should be a set of markdown pages that can be committed to a repo. But even even more simply than that, it is a user's personal knowledge repository. Because quite frankly, if they have done a deep research run on one thing, then having the relics of previous runs around to support future runs, if and only if the user judges is appropriate, of course. But having having the the runs of previous or the the wikis of previous runs around is a great way to not hit the same page again and again and again. And so effectively, effectively what we are doing in this stage is building an annotated bibliography of the works consulted, not cited, consulted. An annotated bibliography of works consulted with quotes and discussions of how the quotes are apposite to the research questions and their operationalization.

\section*{Question 3: How exactly should human checkpoints be integrated? At what specific points in the research process is human judgment most critical?}

Uh, the question here, the question here is around most critical. Um.
And I would say that there there are two places where presently human judgment is entirely lacking in the context of Deep Research. The first is on initial search results. And the second is on the writing design. The outline, the the the step before integrating the sources and the structure and the arguments into the user's output. So let's start with the first part. In the first part, it should happen after the AI has compiled its first set of sources by category that it has searched. Because it is seldom that the the case that the first couple of hits on the topic. It is seldom that the case that those first hits are actually the most responsive. So what the stage-based design would look like is fundamentally, the utility of human judgment here besides the fact that that's the point. But the utility of the human judgment is in precedent setting. It is a way of getting further prompts from the user such that we have a better understanding of what it is they desire. There are two places where this is essential. The first is on initial search results. Did it get it? And we need to be, we need to be quite clear here. This is not, is it done? It is more, here are the sections, here are the results that I found. Is this the kind of thing I should be looking for? Effectively, effectively, this is asking the question of, is each category appropriate in degree and or in kind as as an answer for this part of the query? And and if effectively what we are doing here is we're breaking apart the research question into sub-research questions until such time as we have a research question that can plausibly be answered. Sub-research question, sub-sub-research question, whatever. They can be plausibly answered with one or two quite similar queries. Now, whether or not that will be is another question. But effectively what this is doing is is getting the user's judgment as input as to the utility not of the sub-research questions, but of the responsiveness of the literature that's been found to any of those sub-research questions. Now, obviously here, there are opportunities for human judgment even the simplest, 'Do you find this quote persuasive as an answer to this sub-research question?' Yes, no. Right? Like we we don't need to have really complicated outputs. We can just have, 'Hey, am I on the right track?' Yes, no. Obviously, these are precedent-setting. Um, and the user can feel free to start typing long-run answers or what have you. But the other critical place where research questions need to be is in preparing the output. Specifically, putting together the outline to write the answer to. The model makes lovely messy outlines at points if it chooses to. Specifically, the outline, the the the step before integrating the sources and the structure and the arguments into the user's output. Here, we are not talking about the observe-orient parts of Carnapian [kɑːˈneɪpiən] or UDA loops [uːdə luːps]. Right? Even even in that most simplistic, most simple task, the causal arrow is reversed. where the system decides on the rightness of a task, clicks on a thing, and then justifies itself. This causal arrow is not agency. Nor is it deciding nor is it able to make an appropriate judgment call if if it is proceeded towards its goal. It is able to say whether or not it has succeeded towards what it thinks its steps are, but it is not able to situate itself effectively into reality. And this lack of situation means that it cannot perform as our agent, nor does it have self-agency in determining how to accomplish, accomplishing, and then assessing if a task has been accomplished. Therefore, it is inappropriate to call our current infrastructure agentic. And therefore we can say that AGI, Artificial General Intelligence is amongst other qualities not articulated here, necessarily goal setting as well as goal following. I would note, however, that an effective goal following system would likely be capable of accomplishing the monetary gains that Microsoft has set as their definition of AGI. Simply by processing or adding more computers. And so therefore, our philosophical basis and our philosophical definitions should be considered a completely disjunct ontology from those being peddled by Silicon Valley. To say this again, for Deep Research to, in fact, be a useful research assistant, it needs a number of components. The first and fundamental is the ability to figure out what the research questions are within the prompt. So, not what the prompt's questions are, but what the fundamental research questions implied by the prompt such that research can be done. Noting that a literature review, if done well, is a persuasive document which situates a text in a literature, identifies the research questions, identifies the context in which the research questions will be answered, and most importantly, identifies the gaps. This requires epistemic humility that these systems flatly do not exhibit. They can certainly play at it, but then playing at epistemic humility is not, in fact, the same as them having it. The performance of a virtue is not having the virtue, which is kind of the whole problem with large language models.

\section*{Question 4: How should outline-driven context switching work in practice? How would the system know which sources to load based on outline position?}

So, this first stage requires searching. This is not merely task clarification. And the point of this is that at any stage of all of these, we can go back up a stage. We can discover that, oh, our original research question was wrong. We can discover that, oh, there is a part of the literature that we hadn't considered. Um, and so fundamentally, what we are doing at every single possible stage is building, is verifying the research questions through negative inference. And that's actually extremely valuable. And so that's stage one. Stage two is effectively the building out of a plan to answer the user's question. Which again needs to get feedback from the user because is the AI corrigible [ˈkɒrɪdʒəbl̩]? Not in terms of its final instruction, but in terms of process. This specific part is also a fundamental rejection of technoscholasticism. Do wonder how that word will come through on the transcript. Um, you know what, I'm just going to have to say it two words in the transcript: techno scholasticism. Sure. Um. And so this second phase is a phase of growing many distinct investigative branches and sorting them. So, what works are able to answer what parts of the question? Furthermore, and also throughout all of this, the user must not be passive. Because the most useful thing is to leverage the judgment of the user here. And so there should be throughout all of this confirmations. Is this passage responsive to your question? Is this what you're talking about? The the most important thing is defining the borderline stage of this. Now, obviously, this is multi-threaded. We should not lock on user input. But basically a common sense thread looking at what the user is replying versus what all the sub-agents are working on and going, 'Hey, am I wasting my efforts?' Like what we care about here is identifying cognitive mismatch, but also what we care about is drilling in on what the user is actually asking. And the best way to do that is because we we've already done an operationalization, we've already tried asking the user, is to, well, here is a giant list of sources. Also presented one at a time, because quite frankly, if you present people with a giant list, they won't engage with it. And more to the point, you can really generalize from answers. So, so this stage is going, 'Hey, here's this work, here is this quote. Here is how we think it is responsive to the research question. Is it? Is this useful? Should I use this for background? Should I use this in the final piece? Am I on the right track?' Now, obviously, our goal here in this second, second and a half stage, whatever, is to better calibrate the user's judgment. Effectively, this is a question of corrigibility [kɒrɪdʒəˈbɪlɪti]. Right? Can we correct the AI's operation within this context? So, if we provide the user with, 'Well, here is a draft outline of approximately 10 sections', then we need to get the user's feedback on the degree and the kind and the section ordering of the sub-research questions because we have assumed that it must be responsive to the core research question that was the output of stage one.

\section*{Question 5: What documentation of decision points would be most valuable for researchers reviewing an AI's process?}

Fundamentally, what searches were done, why, what did they result in, what was the evaluation of each result of the search systematically positive and negative, then how was the user's query operationalized into research questions, how were those research questions broken down into smaller and smaller research questions? What precedent did the user set? What conclusions did the model draw from the user's choices? What is the model's operationalization and understanding of the original query? Excellent. And that finishes this.



\section{Section 5 Questions}

For your walk to work, I'd recommend focusing on **Section 6: Research Tool Design Requirements**. This constructive section needs more specific details about how research tools should be designed to compensate for AI judgment deficiencies.

Specifically, please elaborate on subsection 6.2 (Research Process Support):

```
\subsection{Research Process Support}
- Stage-based design (literature finding, extraction, etc.)
- Non-waterfall approach to research flows
- Structured to-do lists with human checkpoint integration
- Outline-driven context switching
- Documentation of decision points
```

Consider addressing these questions in your dictation:

1. What would an effective stage-based design look like in practice? How should transitions between stages work?

2. You've mentioned that research can't follow a waterfall SDLC - what alternative workflow model would be more appropriate? How would it accommodate iteration and refinement?

3. How exactly should human checkpoints be integrated? At what specific points in the research process is human judgment most critical?

4. How should outline-driven context switching work in practice? How would the system know which sources to load based on outline position?

5. What documentation of decision points would be most valuable for researchers reviewing an AI's process?

This section bridges your theoretical critique with practical recommendations, making it particularly valuable for the paper's contribution.

\section*{Section 5: Reflective Analysis - Question 1}
\subsection*{Question Text}
What specific examples from your autoethnographic testing best demonstrate technoscholasticism in action? Can you describe instances where AI tools privileged textual authority over critical assessment?

\subsection*{Transcription}
Okay.
So, the first example of this is when I used OpenAI's Deep Research to gather the literature on Digital Humanities in Australia.
Now, to be clear, I did this with some malice aforethought. Um. Not least because I had done this research by hand two years ago. And and fundamentally, the flaw with this is that over the last decade, decade and a half, many universities across the world announced a great fanfare a digital humanities major or lab or what have you. Um, as a to-do, make sure that I actually go look in the uh the DH outputs. But they they announce with great fanfare a lab. The problem is, they don't announce when things go fallow. Um. And and so fundamentally, the the picture that Deep Research painted was one of a vibrant digital humanities across many universities. Whereas in many ways, after quite a lot of excitement in the 2010s, it left to go downhill before the pandemic and was basically killed during the pandemic. So, this is a great This is actually a great example of technoscholasticism. Because it fundamentally the these claims are not assessed in any sort of context. That just because the claim is made does not mean that it still holds true in this moment. And so the lack of temporal criticality beyond that, however, yeah, I would say that I have seen no evidence that any models that I have worked with that are that are connected to the web for Deep Research kind of operations are um exhibit non technoscholastic tendencies. Um, I mean, look, the the point of this is it can. Right? There's no effectively judgment.

\section*{Section 5: Reflective Analysis - Question 2 \& 3}
\subsection*{Question Text 2}
How did I observe the AI systems conflating description with insight? What markers distinguish genuine insight from sophisticated description?
\subsection*{Question Text 3}
In what ways did the AI tools fail to identify true knowledge gaps? They simply aggregated existing views, or did they attempt to highlight contradictions?

\subsection*{Transcription}
Okay. Um. Section five, question two. How did I observe the AI systems completing description with insight? What markers uh distinguish genuine insight from sophisticated description? And question three, in what ways did the AI tools fail to identify true knowledge gaps? They simply aggregated existing views or did they attempt to highlight contradictions?
Huh. Okay. Um. No, that's just a really good question. Um. So, fundamentally, this is the summarization problem. Right now, models are very effective at deductive and tautological transformations. So, pointing at my my students annotated bibliography, Claude 3.5 is very effective at saying how a given specific text answers basic questions. So, so so my my try that again. The fundamental thing that I observed in Arts 3500 with my ancient history cohort when they were building annotated bibliographies of 160 all 160 sources talking about Plato's madness was that there was no judgment required, no induction required. Um. The students said, 'Does this source talk about Plato's madness?' If so, what statements does it make? Sort of thing. I'd have to pull from the the the prompts, right? But but we're talking about basic declarative questions, right? We're not claiming what was the point of this piece? We're not saying what was the fundamental insight of the piece. We're saying, 'Are there sentences that match this condition?' And obviously large language models right now can do bidirectional inference. That is description. Insight, however, requires contextualization and tacit knowledge. And again, judgment. Because insight is not, 'How does this paper ask this question?', but 'Is this a useful paper?' And what is the most useful statement of that paper? Because talking about the most useful statement of that paper, we're no longer relying on the claims of the paper, right? The paper has its own opinions about what it thinks is important. And if it is well written, those are well signposted. But not all papers are well written and not all good papers or great papers are well written. And if we're talking about the fundamental insight, then it is one, is this the most appropriate paper to answer this question within the context of the broader literature? It is what is the actual piece of new knowledge contained within this paper, regardless of its claims. It is the pushing back against claims. It is the 'You know what? I disagree.' right? So it is it is looking at all of the necessary formalisms that the paper has to go through and and discarding them and knowing those. Um. It is looking at the intellectual pedigree of the paper. Who are they citing? What is the context of the author? It is looking for context clues in the quality of the journal. It is contextualizing them from those papers that cite it. So insight in many ways requires the tacit knowledge of the surrounding disciplinary context that large language models are so spectacularly bad at. Because large language models are fundamentally credulous. They believe what they are told. And this issue of belief of of is this paper's claim credible and useful within our context that large language models aren't great at? Because this this speaks to um what I'll be getting into in section six, but the the Carnapian confirmation problem. That that Deep Research tries to accumulate evidence until some vibe-based criterion is met. And then that's that. If we presume this to be the case, then we say, okay, Deep Research, find me all articles from this journal that relate to this topic. Now, the difference between this search and Jacob's search is one, this was bounded and well-specified. And quite frankly, a relatively small dataset. These claims are not assessed in any sort of context. Because the corpus is so problematic that Elicit doesn't in fact have a great deal of surface area to work from. And it's it's kind of false flag. The second issue is that Elicit's Deep Research has a textual framing that is limited to 10 papers for that output. And so we we were given a month free, I used it for about an hour and then um ignored it. Um. It operationalized uh uh set of criteria, I just didn't do a very good job of assessing papers against that criteria. Again, it's not specialized for humanities research. Um. But the models that they were using were not very impressive. So elicit's technoscholastic bent the failure mode for elicit encompasses technoscholasticism is that process never ever results in it saying, 'I don't know'. I or I mean, look, and and we can turn that down. We could say, 'Oh, I don't have enough information to to come to this conclusion.' It never it never actually acts upon its notice of deficiencies. It always uses that language of 'Well, what about?' to create more options, and those options can be contrary to the other options. And I have no fundamental issue with this performance of of epistemic humility just like it can engage in the performance of judgment. Its technoscholastic front is that the most useful insight from that paper, were no longer relying on the claims of the paper.

\section*{Section 5: Reflective Analysis - Question 4}
\subsection*{Question Text}
Did you observe any instances where the AI tools appeared to challenge their technoscholastic tendencies? Were there conditions under which they performed better?

\subsection*{Transcription}
Okay. Um. Section five, question four. Did you observe any instances where the AI tools appeared to challenge their technoscholastic tendencies? Were there conditions under which they performed better? Cool, cool, cool. No. However, we did manage to work around the problem. So I think what's interesting here is not only did we not observe this, but we saw that their own estimations of their own capabilities were woefully badly calibrated. So when we got 01 Pro to flush out the prompts for Deep Research, it made this wonderful seven-step research plan. Wonderful in a bit of quotes there. But like it was it was quite good. It would go find the things and then step, step, step, step, step. After about a week and seven iterations of the prompt, instead, instead what we found that worked was one fraction of step one, which is to say that Deep Research, when given an extremely elaborate single prompt, because of course it doesn't allow for the more effective back and forth. Um, and this single prompt effectively was, go find a comprehensive list of all references to tools such and such and return them in a CSV. That it basically did. Now, there are data quality issues here, severe data quality issues. Um, fairly severe hallucination issues, but it certainly didn't challenge its own tendencies at all. Um, but that's how we worked around it. Um. Which is is maybe have an appendix on on Zendo, right? But like because it couldn't assemble a corpus to search. This approach was fundamentally flawed, and maybe I should I should investigate that. Maybe that's where the problem is. But my critique of the processes that I've seen in Elicit, in OpenAI, in Perplexity, is that they effectively random walk through the tree of possibilities with a vaguely Carnapian confirmatory bias where they they collect evidence until some internal vibe threshold is met, and then that's that. It is rare that the case that a research process is seldom that the case that the first couple of hits on the topic. Gemini 2.5 does not exhibit judgment. But it does exhibit decent inductive capabilities. So, that's nice, I don't like it anyway, we'll figure it out. Um. So, the corpus is bad. Um. It does not have acceptable levels of coverage in the humanities, um, for all sorts of reasons that aren't relevant to this paper. Uh, beyond that though, it tends to act on abstracts rather than full papers. Again, because it doesn't have the the non-abstracts or papers or books or what have you. And the third dimension of this is that its AI-generated TLDR tags no-notable inferences. And so the corpus is so problematic that Elicit doesn't, in fact, have a great deal of surface area to work from, and it's it's kind of false flag. The model may need to be prompted with scheduled tombstones to consider alternate interpretations. This is where prompting can play a massive actual role. But fundamentally, the choice of using anchor text is superior to the use of embedding search for this problem. Must keep in mind the goal and the process, not the output itself. Computer Use honestly struggled to use the computer. Um. It simply wasn't at the level of calling APIs, text gets fitness scholasticism because it couldn't assemble a corpus to search. So if we prove that it can it is responsive, then besides iterating through the outline search, what we should be doing is looking at all and links and interpretations from the referencing document and all inbound links and interpretations to that document. Because because the kind of proleptic argument of following claims forwards and backwards and so on, can be very effective when we're building when we're building these two. So if the system during research should be just building a wiki and fundamentally, it needs to dis- um it needs to figure out when its first draft is unacceptable and when it needs more research. But it also needs to be able to indicate sources in its corpus should not be considered for a draft. It needs more structure. It needs to be able to make outlines and write to the outlines. It needs to have an obvious editing pass. It needs to be a real boy. Cool. Okay. Um, I think that's I think that's that one.








'20250325 - Sect 5 q 1.m4a'
'20250331 sec 5 q 2.m4a'
'20250401 - Section 5 Question 3 - Literature – Tuesday at 9-36 am.m4a'
'20250402 - Section 5 question 4 - Research – Wednesday at 9-26 am.m4a'

\section*{Section 3: Methodology - Question 1}
\subsection*{Question Text}
Why is autoethnography particularly appropriate for studying AI research tools? How does it capture dimensions that other methodological approaches might miss?

\subsection*{Transcription}
Okey dokey. Uh, section three, question one. Why is autoethnography particularly appropriate for studying AI research tools? How does it capture dimensions that other methodological uh practices might miss? This is the wrong question.
Uh.
Oh boy, this is absolutely the wrong question. So, Deep Research is weeks old. When we gained access to it, it was a week old.
Fundamentally, this research about doing research is autoethnographic because a lot of the effective use of AI, especially in research, is vibes based. The utility of this paper, the utility of this project is in me, a philosopher and AI researcher, reflecting on my use of AI to AI, um, for for purposes of research. Like the process that I'm doing right now where I'm getting AI to ask me questions on behalf of the paper is itself part of the research, because I'm seeing how well a paper can be constructed using this approach. I'm evaluating different models. And so, let's be clear. This is hypothesis generating research, right? This is purely qualitative. This is this is meta research. Um. And therefore, for me to do a case study that is at all persuasive, I would either need to do the research and have AI expertise, or I would need to find researchers who had equivalent or better skills in the textual technologies of prompting to do this research with, noting that OpenAI's Deep Research is \$200 a month. And so why autoethnographic? Well, fundamentally pragmatism, that a an autobiographical account of this is what I was thinking when I was doing the research, I think would lay the foundations quite well for discussing discussing um the the theme of technoscholasticism that I discovered. Do I claim that this is the way the world is? No. But looking at myself, looking at how what other folks consider an expert in AI uses AI for the purposes of research was pragmatically what was possible. And moreover, this would this is an assessment of vibes, right? This is an assessment of the patterns of fundamental capability that observing the tool and understanding the tool and evolving my methodology for using the tool is all part of it. Because even with the defense of of qualitative research, even with the defense of hypothesis generating research, many of these patterns of uh interaction are tacit patterns. They they we recruit researchers with enough time and capability would be tricky. But I think that that is sufficient for effective use cases. And then pulling back comments like this uh is useful where my previous notes fix that the third activity students do badly and give you awesome results that have no bearing on reality. Is that sure, marginal papers or borderline papers, it is far more interesting for the PhD student to read the output models and then to verify and cross-check the output models than simply reading I don't know. Actually prod, um get this, can you, can you engage with some co-pilot argumentation here? Now I see that the the canonical places and it comes back to this one. But I think that that is sufficient for effective use cases.

\section*{Section 2.2.2: Inductive Reasoning - Question 1}

\subsection*{Question Text}
How would you clearly define inductive reasoning in the context of scholarly research? Specifically, distinguish inductive reasoning from mere pattern recognition or predictive capability.

\subsection*{Transcription}
Um.
Sub-subsection 2.2.2 question one. How would you clearly define inductive reasoning in the context of scholarly research, specifically research sorry, specifically distinguish inductive reasoning from mere pattern recognition or predictive capacity? I mean, it's in the name. Um.
So, we can talk about inductive reasoning for hypothesis generating research, and we can talk about abductive reasoning for hypothesis testing research.
I mean, let's be clear, inductive reasoning is pattern recognition. Um, this this question is incorrect in many ways.
But
we can frame it as common sense, we can frame it as pattern recognition, we can frame it as the that's funny moment.
But what I would actually frame this as is
the noticing of the gaps. So, in the context of scholarly research, inductive reasoning
is
oh my back.
Ow.
What was I talking about?
So, inductive reasoning
is noticing when the observed facts do not um meet the predictions.
Wait, hold on. No, it's not pattern recognition. Pattern recognition
Uh. Okay. So we can say pattern recognition,
oh, this whole thing is stupid.
Come on.
Pattern recognition
ah. Pattern recognition is what traditional ML does. Um, it is finding
clusters of
observations of the world. Either the literature or from the mark one eyeball, or from scientific data, or what have you.
And it is the process of grouping.
Inductive reasoning
is not merely the process of grouping, but the process of articulating a covering rule.
So,
you use inductive reasoning
to notice a gap in the literature, articulate a gap in the literature,
and then turn it into a research question.
It is the process of generalization of disparate data, not merely grouping similar data.
It has relatively little to do with predictive I mean it is predictive, but
No, it's not really predictive. I mean, like obviously it's induction. Induction is predicting because you're making hypotheses, right?
But
Yeah.
The the prediction side of thing is there is something interesting here and that my effort may pay off.
Yeah, good enough. Um.
2 2 2 question two, how does the next token prediction architecture fundamentally constrain ability uh LLM's abilities to perform meaning, you know what? I am actually going to do this as a second recording. Cool.

\section*{Section 2.2.2: Inductive Reasoning - Question 2}

\subsection*{Question Text}
How does the "next-token prediction" architecture fundamentally constrain LLMs' ability to perform meaningful inductive leaps? Can you illustrate with a concrete example?

\subsection*{Transcription}
Okey dokey, uh, 222 question two.
How does the next token prediction architecture fundamentally constrain LLM's ability to perform meaningful inductive leaps when I always drop it?
Okay. So,
So there there are two fundamental constraints here that have been operant for all of the models that I've seen.
And
Let's speak about it elsewhere in the paper, but
it can't
move the cursor around
and it in in terms of its thinking, co-thinking.
Um
and it can't delete things. Right? And so
um
The requirement here of consistency with predicted tokens
enforces a level of coherency.
It is the difference between narrative and plot. It is the difference between
exploration and explanation.
That
Basically, it can't disregard
and
it can't really build backwards.
And so
Inforces a level of coherency.
It can't really explore, can't really build backwards. So,
This is this is where constraints in the model and the file and the data that it has access to comes in, right? But
most default models are bad.
And
when an LLM writes into its context window
when an LLM writes into its context window,
there's no internal noise.

\section*{Section 2.2.2: Inductive Reasoning - Question 3}

\subsection*{Question Text}
Could you speak about the importance of the impulse of inquiry in human induction? What precisely are AI systems missing here?

\subsection*{Transcription}
Um. 222 question three. Could you speak about the importance of the impulse of inquiry in human induction? What precisely are AI systems missing here?
I mean, the answer is epistemic humility. Um.
Which I'm pretty sure
is the the prior um section. But uh
It is the, huh, that's funny. I should poke at that more. Or
Where did you get that quote from? Probably meme or probably
we'll just search.
Does that's a good phrase?
Huh.
Ink driven systems. What in the world is this?
Um.
Okay, well, I should go poke at what O E I S is. Um. Anyways.
I would describe this as
people have used it. Anyways. Um.
Now, how do I describe this without anthropomorphizing? Because that is, of course, the challenge.
I mean, the impulse of inquiry is
to my mind
literally noticing that a question could exist.
It is the that's funny moment. It is the
ah. It is the noting of discrepancy between world model and facts. It is it is that moment in the Kunian sense
that
can I mean, it's not really revolutionary science, but right, it is it is the it is the failure to explain the observed facts. Right?
It is it is that tension between what we know and what we have
that is fundamental to research. It's the well, why do we care about this question? Why why should we ask this question question? Um.
And fundamentally, it is inductive. Well, I mean, we can we can call it abductive. Um.
But it is
It is a comparison between
generalizing a case over that which is observed versus that which has been described. And realizing that they don't match. Yeah.
Um. Cool.
Now, I am sweating.

\section*{Section 2.2.3: Correspondence with Reality - Question 1}

\subsection*{Question Text}
Clearly define your notion of "correspondence with reality" in scholarly research. How does this differ from mere textual coherence or "truthiness"?

\subsection*{Transcription}
223, question one. Clearly define your notion of correspondence with reality in scholarly research. How does this differ from mere textual coherence or truthiness?
This very question is good evidence of the need for correspondence with the real.
Um.
No I didn't. Whatever. Um.
Fine. That's good. Okay. So,
correspondence with reality
does not mean textual coherence. It does not mean internal consistency.
Nor does it mean um
truthiness. Yes, thank you, good job. Ah.
Right, so textual consistency. Yes, okay. So,
Cool. So I just made a quick check of textual consistency.
It is the ant- it it's not the antithesis, it is the opposite of textual consistency. So textual consistency is an internal coherence of presented fact. Um.
Inside the argument. And and there is the implication that if the article is coherent, then the text is good.
This is the fundamental flaw, one of the many fundamental flaws of AI. Um.
That
there is no world model, there is no outer world, there is no knock on the table. There is merely the words and the sophisticated but still present statistical relationships between words. Um, ooh, uh site my own paper, uh map with no territory. This is absolutely a map with no territory. Um,
Text neighbors for words etcetera. Um thing. Uh Hey Claude, when you get around to reading this, actually, hey, I have a Claude or Jipit when you get around to reading this, remind me to drop in my paper. Um. Yes, of course. Okay. Um.
So if it's not textual consistency, we care about truthiness. Well, no.
Ah. It is much better to say that correspondence with reality is an anti-bullshit in the Frankfurt sense. Right? So Frankfurt 2005 on bullshit talks about bullshit as being an indifference to the truth. Um, and then last year's paper Chat Chipin's bullshit, um, speaks to observing these various trends in Chat Chipit. Um.
Which perforce Yes, so this is an anti-bullshit requirement where
we care about a correspondence with the real, with the real. Because and so if we talk if we're talking about a correspondence with reality as a function of
a non-indifference through the truth,
it doesn't actually matter if the things are true per se, it matters if there is a
intention that they could be true. We can we can talk about this in a positivist sense, right? We can talk about this in terms of
falsifiable statements, we can do all of that sort of thing, but fundamentally,
the correspondence with reality here is
the specific and intentional regard for the world the words represent.
And in terms of authority, this is counter to the scholasticism tradition because effectively what we're saying
is that rather than respecting authority, we must hold authority to account, that authority can be correct, that authority can be wrong, but the arbiter of correctness
is the real.
This isn't really truthiness.
This is the necessary prerequisite to truthiness. There we go. I like that.

\section*{Section 2.2.3: Correspondence with Reality - Question 2}

\subsection*{Question Text}
How might the concept of "Carnapian confirmation" explain AI's tendency toward superficial validation rather than genuine empirical checks? Can you give an example?

\subsection*{Transcription}
Section 223, question two.
How might the concept of Carnapian confirmation explain AI's tendency towards superficial validation than genuine empirical checks? Can you give an example?
Uh, these questions suck. Okay.
Okay. So,
we have observed
We have observed that
deep research
engages in and this is Open AI or Perplexity or what have you.
Effectively stamp collecting.
Um
that
I mean, ironically enough, this is the same same behavior that an academic usually behaves in when when finding citations that it's oh, I make a claim, what citation backs up this claim. Um So there's a reason why they're mimicking this academic sin.
Um
But effectively this is So Carnapian confirmation talks about accumulating evidence until something becomes a fact.
So we have observed that deep research visits web pages until
I'm gonna have to anthropomorphize here. It feels confident in
until it feels confident in citing a thing.
This is not a tendency towards superficial validation.
I mean,
we we can say that there's no genuine empirical checks because there are no hands.
But again, this question is fundamentally wrong. It is not attention towards superficial validation. Also validation is explicitly the wrong word with regards to Carnap as validation is a very specific um term for Carnap. Um
But it's because the causal error arrows are wrong.
This is the question of historiography. Fundamentally, this is the question that
an AI seldom in our experience asks.
Questions of historiography. It is it seldom asking questions that
It is seldom asking questions
of who is the author of this source, can they know this? What have other people said in regards to this source. So you don't, you don't need to be able to knock on the table. What we need to be able to do is assess the credibility of a source as a function of other literature in tension with and supporting with the original source. So it is the historiographi- historiographical, there we go, assessment of
this paper makes this claim, can this paper make this claim?
It is specifically not saying this paper makes this claim, therefore the claim that I'm making is true. It is this paper makes this claim, this paper is supported by this surrounding ecosystem of papers. Therefore it is this passage from this paper supports my claim. It is the historiographical assessment, it is the textual assessment of the paper that makes it useful. Um
So, the example is the digital humanities uh example that I've used before. Um so, uh Japit, let's say oh three mini, whatever, probably a one, now I'm thinking about it. Set out to do a deep research run. Um and so it found a bunch of Australian research uh universities claiming deep research, claiming um digital humanities. However, it did not look at the dates nor the broader social context of these claims and realizing that these claims are far too old, that these courses have not been updated and that they are unremoved relics of the 2010s rather than a continuing field of scholarship. Cool.

\section*{Section 2.2.3: Correspondence with Reality - Question 3}

\subsection*{Question Text}
How could a genuinely empirical or experimental approach theoretically differ from current AI practices?

\subsection*{Transcription}
223, question three. How could a genuinely empirical experimental approach theoretically differ?
Excuse me. Theoretically differ from current AI practices. Um.
Honestly quite easily and this is this is what I put in in context to my the prior question of the historiographical assessment of source. Does this source, is this source supported? Does this source have, does the author of the source, is it pot- there we go. Is it possible for the author of the source to make the claim? Are they there when they observed it? It is
Yeah. Um It it is the specific contrary assessment of source credibility that would be useful here.
Um so it is both source credibility but also situating the source in the literature. So
Can can the source make the claim? How well does the source agree with its literature? Does the method of the source allow it to make the claim? Has it been contradicted? What is the timing of it? Why why is this found when other things aren't found? And I think the the the real flaw with this is contemplating negative space. So, LLMs do very poorly in terms of negative space. They do very poorly in terms of negative inference. And so
Noticing that a claim made on a web page five years ago, six years ago before the pandemic, is different than a paper written six years ago that might be good is actually quite tricky, especially when the web page may not have a date on it. And so
Effectively there needs to be a source validation. There needs to be a historiographical assessment of every source before it is incorporated.
Which cannot happen in the main context window.
So, does the source say what it can say? What are the quotes? What are the page numbers of the quotes? And then to contextualize the source within the broader literature, but also within the activities of the real such that we think that it still maintains co- a correspondence with the real during the time when the search is done rather than reflecting a prior and no longer relevant correspondence. So there's a temporality awareness as well as a context awareness as well as an author awareness as well as a literature awareness that this sort of scholastic assessment of credibility of source is fundamental to the textual humanities disciplines. Awesome.

\section*{Section 2.3: Nature of Research Work - Question 1}

\subsection*{Question Text}
Explicitly outline your understanding of a literature review as a persuasive rather than descriptive task. Could you unpack this difference in more detail?

\subsection*{Transcription}
2.3 question one.
Explicitly outline your understanding of a literature review as a persuasive rather than descriptive task. Could you unpack this difference in more detail?
Oh my god.
Okay. A literature review fundamentally does two things. One, fundamentally, it situates whatever we're writing within the context of the literature.
Right? So,
literature says this is the jargon, here's the research that came before and so on and so forth.
You would think that that would be descriptive. That is false. It's informative, but the literature review
for purposes of any research task is fundamentally persuasive. Because it is the evidence that warrants the claim of and therefore this paper makes the following claims.
Right? It is not it is not descriptive in its own guise. Its utility is not It's not an annotated bibliography.
Moreover, the literature review, besides situating the research in a broader context, introducing whatever frame you want, how how how you're approaching the problem, has to do two things. One, why has no one else or sorry, to demonstrate that no one else has solved this problem.
This is an inductive step. Right? This is to say here is all the surrounding literature of the problem, but what I am about to write is novel, pay attention. Now, to be clear, all these purposes are not simply meant for academic discourse.
But when you talk about research for purposes of business, the literature review has an even more pointed persuasive task. It is to offer evidence in support or against some claim.
And so therefore, being able to evaluate and situate the evidence in relation to the claim is essential.
But even
So it's informational, right? It is situational.
But the informative nature serves a broader purpose. And it is to serve as evidence for some argument.
A literature review is not a search.
And now, take it back. So,
Fundamentally, a literature review must make the case of why does this evident evidence support this claim? Why should we, the reader, care?
Um even when it's delivering the evidence, even when the point of it is purely to situate and deliver evidence, there is that persuasive line that must go beyond simple summarization. Cool.

\section*{Section 2.3: Nature of Research Work - Question 2}

\subsection*{Question Text}
How exactly does judgment function in the act of selecting and organizing literature to establish scholarly novelty?

\subsection*{Transcription}
2.3 question two. How exactly does judgment function in the act of selecting and organizing literature to establish scholarly novelty?
Fundamentally, this is the absence of evidence is not evidence of absence, right?
Um.
Basically by
by describing
This is where situating ourselves in literature is so important because if we say where we are, then that is evidence that we have done the necessary work around where we are to say that no one else has had this position. And so we prove novelty by providing context. The trick is this is a persuasive game as per my prior um discussions. This is a persuasive game
where if we use the right language, if we show an awareness of recent publications, if we show an awareness of the web of scholarship that supports If if we so shown awareness of the web of scholarship that supports the people that we are citing, then then we can say within this literature, within this discipline, no one else is talking about. It means that there is assurance in jargon in Shibboleths in context. And because we have evidence that we have situated ourselves in a particular position, then people are aware of the literature in the discipline can accept our claim for novelty.
Now, where does judgment come in? Honestly, this is more model of mind of mine, but judgment is is part of it. Because what we're talking about is what do what in our opinion, judgment, will our peers find persuasive? Because the thing is is that we can dress up a bad literature review in exactly the same language as we dress up a good literature review. The quality of the language, the quality of the framing would be about the same. But absent judgment, the literature review will be deficient.

\section*{Section 2.3: Nature of Research Work - Question 3}

\subsection*{Question Text}
Describe the intellectual act involved in situating a research problem within scholarly traditions. Why can't AI adequately perform this act currently?

\subsection*{Transcription}
Uh, 2.3, question three. Describe the intellectual act involved in situating a research problem within scholarly traditions. Why can't AI perform this act currently? Patience really. Um.
This is this isn't a failure of model. Um this is a failure of programming. So,
literally all that's required of this is having a to-do list. Um having your tasks and and iterating through your tasks and being systematic with it. Um.
This is actually the easiest of all the questions where it is step one, make sure you're using the right words, which is mostly difficult when the user's request isn't using the right words. So it is figuring out what disciplines speak to the user's request, and let's be clear, this this applies beyond the academic as well. So,
the user says something and the AI needs to figure out what disciplines, academic or technical or even shopping, can answer it, operationalize it, and then read the literature. Um.
So this the the read the literature part is the hard part. Um. Because effectively, this is going well, what papers? You know, it's first search and so here search is a problem because we don't have good access to books or journals. Um, Semantic Scholar does not count. Um.
But it then needs to go, 'Cool, these these things are speaking to my disciplines, these things are speaking to my topic'. What are the direct reference? What are the indirect reference? And it just needs to follow the scholarly graph. Um. There is an element of judgment here in the question operationalization, but we've covered that before. Um. A lot of this is social and programming problems. So, when it's following the literature, um if it doesn't have access to a paper, it needs to prompt the user to get access to the paper. When it's following literature, it needs to be systematic in following the literature. It can't just go, 'Oh well, I've collected enough evidence'. That can happen afterwards. Like, our our issue here
is not that it isn't capable of this.
Okay, where I got off.
Um So fundamentally this is to do this. What's the question? Situate. Honestly, it is go build an annotated bibliography. The the place where I am not confident
is if it can discard sources.
Right? Effectively it is go search, find papers, ask the user for papers that it doesn't have access to, involve the user in the research effectively. Um, all the other discussion that I've had around being involved in an argument for this. Uh. And fan out in the literature, look at the citations, build an annotated bibliography. Honestly building a .bib file would probably be the best way to do this. Um pulling out quotes that it can paraphrase, but being systematic about it. So, so it needs to to to
I mean look, this is going to be bouncing back and forth between my own thoughts. But effectively, it is it needs to be able to demonstrate to the user that what field their question belongs in, like discipline question, who in that field is talking about it, who their antecedents were, and the citations out until at least the second generation, probably the third. It needs to read all of them, needs to build an annotated bibliography, and then have quotes about the annotated bibliography. So just needs patience really, and that's a matter of programming and and that's the hard bit. But, we we've covered that.

\section*{Section 3.2: Tool Selection Criteria - Question 1}

\subsection*{Question Text}
Can you clearly list and justify the criteria used to select which AI tools were tested in your research? Which specific factors (technical, commercial, epistemological) influenced your choices?

\subsection*{Transcription}
3.2 question one. Can you clearly list and justify the criteria used to select which AI tools were tested in your research?
Um what we had access to, what we heard people talking about in February 2025.
So, we focused on OpenAI's announcement because OpenAI's announcement is clearly the most effective of all of them. Uh, Perplexity announced their imitation copycat in use that. Elicit announced their imitation copycat in use that. Google um didn't really make a needle, but we tried it. Um As of talking in April, Anthropic just announced research, but it's not available to us. So, it's literally just what we had access to.
Um and I mean, more more prosaically, it is what people online were talking about. Right? So, so if when reading Williamson, when reading Chu, when reading this, when reading um Mueller, if they talked about it in the first quarter of this year, I tested.
And more to the point, it's also me observing which ones I reach for when I do subsequent research. Um most of that's OpenAI.

\section*{Section 3.2: Tool Selection Criteria - Question 2}

\subsection*{Question Text}
How might your selection criteria shape the limitations or potential biases of your conclusions?

\subsection*{Transcription}
3.2 question two. How might your selection criteria shape the limitations or potential biases of your conclusions? Oh, quite a lot. Um, this is the fundamental failing of autoethnography, right? This is also the fundamental failing of trying to write a paper where literally between the starting of these recordings and where I am right now, what's available to me and their capacity has changed. Trying to hit a moving target. And so, effectively, this is the fruit on hand, right? We we're we're looking at what we have, we're not making comparisons between other techniques, other technologies, other analysis to everything that I have access to because my critiques are not based in the particular programming approach of one over the other, although I can certainly make those. My critiques are based in the fundamental capabilities of large language models. Um beyond that, my biases are academic. Uh beyond that, my biases are philosophical and from the humanities. But it's purely on.

\section*{Section 3.3: Research Tasks Investigated - Question 1}

\subsection*{Question Text}
Beyond literature reviews, explicitly define and describe all research tasks you tested AI systems on. For instance, detail tasks like "source analysis," "data collection," and "knowledge synthesis."

\subsection*{Transcription}
Section 3.3, question one. Beyond literature reviews, explicitly define all What? Beyond literature reviews, explicitly define and describe all research tasks you tested AI systems on. For instance, detail tasks like source analysis, data collection, and knowledge synthesis.
Okay. No, that that is a fair question. Don't like it, but it's a fair question. Um.
So, this is specifically in regards to deep research workflows.
Fundamentally, we found the most use, not the question, the question. The question is describe all the workflows. Okay, so we did um very well operationalized literature reviews. We did a great deal, so over 150 or so. Um, how would I describe that? structured searching? I mean it used deep research infrastructure. I would say data collection. specifically structured data collection. So go out and search, uh what we asked it for was go find the
go find the quote No. For a given tool specified, find all references to it, and then articulate them by year. So this is this is in terms of our actual research, which was software longevity. Um, I mean, toy examples like what hotel should I stay in. Um, pretty sure I did uh a cooking shopping example. I wasn't that impressed. Um, data, it's not cleansing, it's not really elaboration. What did I say? Enrichment, data enrichment. So I used um Hugging Faces' open deep research infrastructure to iterate over uh the open archaeo CSV of tools and enrich them. Um. Certainly didn't do knowledge synthesis or source analysis. Well, I mean, okay, then but we can also, so another task that I did was business case generation, that's the digital humanities example that I've used that I've alluded to a number of times. Um, which wasn't academic research so much as uh making a basically a briefing on a topic. Um, preparatory literature reviews. So like, what is the jargon around the topic? Not so much that that the with the idea that the literature review is useful, but that it was a useful briefing uh to to jump start. Um. But yes, actually, great question. Uh, with Elicit, I only did lit reviews. Uh, with Perplexity, I have Nope, that's not right. Uh, um. So this was an interesting one. Um, just the other day, I asked it to think to synthesize information on how to do a task. Um, specifically around getting rid of a lot of of uh adware on a fresh install of Windows 11. It did not impress me. Uh, Perplexity impressed me even less though. I mean, I say that, I still used What was the question? Oh, cool. Yeah, so that that is in fact the a good example of a failure. Um. Now, I didn't use it for that task.
What? Cool. Yeah, that was the question. Um, that's it.

\section*{Section 3.3: Research Tasks Investigated - Question 2}

\subsection*{Question Text}
Why were these tasks specifically chosen to evaluate the judgment capabilities of AI tools?

\subsection*{Transcription}
Section 3.3, question two. Why were these tasks specifically chosen to evaluate the judgment capabilities of AI tools? Wrong question. These were chosen to evaluate deep research. So, my critique is that AI lacks judgment. I've evaluated this with a number of uh benchmarks, so my induction benchmarks, so on and so forth. But my critique of the output of these deep research runs is that they are quite solid on deductive uh synthesis, um but fundamentally lack judgment. So, these tasks, these tasks weren't chosen to evaluate judgment. These were uh chosen to evaluate how well things like deep research can actually help with research TM. And so the rest of my paper is a critique on these things genuinely helping with my research. I'll repeat the prior answer.

\section*{Section 3.4: Evaluation Framework - Question 1}

\subsection*{Question Text}
What specific "judgment application points" did you identify during your evaluation? Clearly specify where human judgment intersects with AI workflow.

\subsection*{Transcription}
Question, what specific judgment application points did you identify during your evaluation? Ah, during your evaluation. Clearly specify where human judgment intersects with AI workflow. So this is not my questions and questions in in sections five and six where it's what it should be. This is the question of what it is. That's a lot harder.
So, in terms of judgment,
the fundamental judgment here is what tool, what service, what interface, and what modality of use? That's the same thing. What service? So, chatgpt.com, the API, etcetera. What interface? Do I click deep research? Um, do I do I engage all the various levers that have have become exposed? What model? Do I use JPT 4.5 as my supporting model? Do I use um O3 mini high? Do I use O1 Pro? And then above all else, how do I structure my prompt? When an AI is going to go run for 10 minutes, the quality of my prompt matters to a remarkably larger degree than the quality of my prompt normally, mostly because it it wastes my time. And so, the the two, I mean, there there are three, right? What do you reach for? What tool do you reach for? Um And I use tool in in the service plus um interfaces plus model sense. What do you decide to put in your prompt? And then fundamentally, do you accept the the output? And so, to degree, we can rely on the post facto um thoughts as exhibited by AI. Although they do remarkably poor job of exercising the human's judgment. Sorry, they do a remarkably poor job of of exercising the human's judgment. Um Because while they like no, that's not true. The thoughts are there to tell you if your prompt is bad. Not if
Cool, got cut off, but finish that thought. Um, so yeah, and and that was an audit line. And then fundamentally, the judgment of does this prompt do what I wanted it to do? So in many ways, it's the exact same um series of judgment application points as as the normal. Uh it's just it was it's a lot more expensive.

\section*{Section 3.4: Evaluation Framework - Question 2}

\subsection*{Question Text}
Define explicitly your "success criteria" for evaluating AI assistance in research. How did you decide when a task was performed "successfully" versus when it failed?

\subsection*{Transcription}
Section 3.4, question two. Define explicitly your success criteria. I know it when I see it. Like, All of this is vibes. It's all vibes.
So, I was looking for uh, I was looking for quality of answer, I was looking for comprehensiveness of search, I was looking for integration of sources, and I was looking for noticing the gaps. And, I mean, fundamentally, these things describe, that's not a surprise. Which is why,
It is the aesthetic judgment of utility. So, above all else, when I look at the answers from AI, the fundamental metric, that's not a metric. The fundamental assessment that I make is, is this useful? Was this worth my time? Did this tool provide me with mundane utility? Success and failure is not a binary. There's never a perfect success. There are absolute failures. Um, I mean, there could be a perfect success where it is brain outside brain full exocortex AGI magic full up dirt nap. But that's not where we're at. Yeah, that's easy.

\section*{Section 3: Methodology - Question 1}

\subsection*{Question Text}

\subsubsection*{q1 Claude}
What specific methodological justification underlies your choice of autoethnography as the primary investigative approach for studying AI research tools, especially given potential critiques of subjectivity in this method?

\subsubsection*{q1 o3}
What single, declarative claim should the opening paragraph advance to justify an auto-ethnographic approach as a credible mode of evaluating AI-research tools, and which two concrete episodes from your February–March audio logs most persuasively substantiate that claim? Please cite them by date/filename and the analytic insight each supplies.

\subsection*{Transcription}

**Colleague:** Recording.
**Philosopher:** Well no, I start recording on OBS.
**Colleague:** Yours, you start recording.
**Philosopher:** Okay, now just because um, uh, you know, I I I immediately recognize the value of 3.1, 3.3 and 3.4. Uh not quite sure what you mean by 3.2.
**Colleague:** Basically, what what AIs did we choose and why?
**Philosopher:** Why. Okay.
**Colleague:** And I mean honestly, these aren't subsections. Um.
**Philosopher:** Right.
**Colleague:** These are paragraphs.
**Philosopher:** Right.
**Colleague:** Um. Yeah. And and and we chose all of them. Why? Because we have grant money. Um, but you don't say that. Uh. Right? But like. Okay, so, so then what I do is I go um on section methodology to review blah and then match my register. Um. So, then I'm going to My and I are going to have a conversation where we talk about each question and the objective is for you to take the current outline or even that the entire section which is always supposed to be about finding words and and through asking us questions turn it into a paragraph by paragraph outline where we figure out the main claims points and evidence of each paragraph. Oh this Then, great. So that that's this is the standard vibe that I'm I'm I I use.
**Philosopher:** Yeah.
**Colleague:** Um. Yeah. No. And I
**Philosopher:** Turn on the extended thinking definitely.
**Colleague:** So so we're using O3 so I'm going to turn on extended thinking. There are strong arguments to be thoughtful about this choice. Um
**Philosopher:** Oh in Claude in Claude or more generally?
**Colleague:** In general. Um.
**Philosopher:** Uh-huh. I oh I usually do throw like uh in Claude I when I'm doing stuff like this I usually throw the um in the writing style I I switch it to formal and I find a slight improve like
**Colleague:** Yeah. I mean it it's
**Philosopher:** Yeah. low cost and seems to make a slight
**Colleague:** Well so so that's where we actually have to that's where you can pay attention to writing your own system prompt.
**Philosopher:** Yeah.
**Colleague:** Um so now with all of these things what you have to do is you have to say paper. And we're not going to use Gemini because it's a jerk. Um. And it's going to transcribe that which is fascinating. Um. And then we want Uh Cool. So. The what I find to be actually a lot more useful is doing this with two models because they ask different questions.
**Philosopher:** Okay. Okay.
**Colleague:** Um, so this invocation is the um think through it step by step that that I've found does what I want it to do. Mhm. Um yeah. Yep. Yep. Yep. Yep. Yep. Great. And then it has a bloody pointed question which is the whole point.
**Philosopher:** Yeah.
**Colleague:** And now Ugh. I don't know what it is but it's just like your word choice O3 it sucks. What is this garbage?
**Philosopher:** Huh. Yeah eh eh I don't know.
**Colleague:** What is this garbage? Like
**Philosopher:** It does it it it does act like everything's corporate marketing. Accelerate clarity.
**Colleague:** Exactly does. Okay.
**Philosopher:** It was um yeah. I I I found it very useful for like writing a headline of uh for a press release but you know otherwise it's annoying.
**Colleague:** God this is. Like whatever they've trained O3 it's not good.
**Philosopher:** You want to try you want to try 4.5 instead?
**Colleague:** No no no we're trying O3 like this is now research. Um it's just
**Philosopher:** Just doing that comparison between the the wording used in O3 and and Claude would be yeah is
**Colleague:** Yeah um interesting but. I mean honest okay so so no let let's be fair it it's complaining about not getting the transcript so we're going to give it its transcripts. The fact that it's um Fudd. Okay so as that's going let's talk uh to uh Claude's task. So. Okay. What specific meth- question one. What specific methodological justification underlies your choice of autoethnography as the primary investigative approach for studying AI research tools especially given potential critiques of subjectivity in this method?
**Philosopher:** Uh-huh. We know it when we see it. Um we are researchers doing research. We have a decade of experience in digital humanities and building tools.
**Colleague:** You gotta update that it's up to like fif- almost uh almost 15 years now.
**Philosopher:** Stop helping.
**Colleague:** Because we're getting old. uh but
**Philosopher:** Oh my god stop helping. Right so like we have we have 15 years each of experience in building software tools. If not us who? And the critiques of subjectivity like good So so this this oh my god this is my critique of of all those studies where it's like oh well we asked we asked the survey questions it's like well no you actually have to get your hands dirty this is this is Latour in the lab. This is what's actually going on and what are we Yeah it's a case study it's uh a case study by uh you know researchers with extensive experience in digital humanities and tool construction you know uh and um uh and socio-technical research I mean we've done socio-technical research and we can kind of guess what people are going to say if we ask them questions anyway no it's
**Colleague:** Yeah and and that's the thing is is we don't have the ability to sit with other people and watch them build a research paper.
**Philosopher:** Yeah.
**Colleague:** Like at the end of the day for us to gain the insight insights in this paper we have to be working on our other paper.
**Philosopher:** Right. So this is I I guess the the last thing that I'd add to that is that um that I think is worth pointing out anyway that um and I I I'm sure we could probably ask uh O3 or someone to find the reference but um uh uptake of uh large language models is extremely limited in history and there just aren't that many people in you know particularly your position where you've been intensively using the LLM for you know since twenty since late 2022 and you know uh so we're in uh you know between us between the our our digital humanities experience experience building building tools um you know uh experience with LLMs if you wanted to throw something in from my side experience working across the you know research sector in leadership positions at the university and national level you know that that we're well we are well placed to do a critical evaluation. Oh and your background in uh the history and philosophy of science. You know between those things we're well placed to do
**Colleague:** And even more to the point I'm good at prompting. It's an art. Like the fact that I words good with with AI actually matters.
**Philosopher:** Yeah and I I I think I draw that out by saying you know that what we wanted to do was really explore the frontier here like you know use the use the latest models use you know the premium paid versions of the latest models use the APIs and not have bad prompting get in the way. Yes. So yeah that's I I think that's you know very good for the you know for the justification.
**Colleague:** Yep. And so therefore we get to interrogate our own thoughts and our own observations as we build the research paper and that's an autoethnographic case study. We're not generalizing we're not saying that we're revealing any truths but also as we've written this three new models with new capabilities have come out so good luck doing a study.
**Philosopher:** Yeah.
**Colleague:** Cool. All right. Open AI uh O3's question. What single declarative claim should the opening paragraph advance to justify an autoethnographic approach as a credible mode of evaluating AI research tools and what? And which two concrete episodes from your February March audio logs most persuasively substantiate that claim? What the fuck is this asking?
**Philosopher:** I mean the the the first half of that question is what we've just um
**Colleague:** Yeah no we've answered this question.
**Philosopher:** answered. And
**Colleague:** But the second part
**Philosopher:** I don't really see the
**Colleague:** That doesn't matter because my audio logs are just answering questions.
**Philosopher:** Right like I don't see
**Colleague:** Yeah no that this is this is a bad question.
**Philosopher:** Right now now because what to justify to answer Claude's version to justify author or you know in the face of claims of subjectivity is we recognize the subjectivity. Here's why we think a subjective case study is appropriate that we do is is valuable.
**Colleague:** Yeah. All right cool. And now we hit stop recording.


\section*{Section 3: Methodology - Question 2}

\subsection*{Question Text}

\subsubsection*{q2 Claude}
Could you elaborate on how you structured your autoethnographic investigation specifically? What documentation methods did you employ beyond audio recordings, and how did you ensure analytical rigor despite the acknowledged subjectivity of your approach?

\subsection*{Transcription}

**Philosopher:** Cool. What did Can I see what did Claude say? When you when you
**Colleague:** Yeah. So, could you elaborate on how you structured your autoethnographic investigation specifically? What documentation methods did you employ beyond audio recordings and how did you ensure analytical rigor despite the acknowledged subjectivity of our approach? Uh, moo, unasked the question. This is subjective. We interrogated our own experiences by getting AI to ask us questions. Um, so the audio recordings were of me answering AI questions. And I mean, if if we're going to talk about that as rigor, that is a mind outside mind. Like, basically, it is a self-interview technique.
**Philosopher:** Yeah.
**Colleague:** Um, which is weird and not what I was anticipating answering at the start of the sentence, but insofar as I was treating myself as the subject, I was getting the AI to ask me questions around this paper to elicit my thoughts. And so there is no analytical rigor. It is full subjective subjective. What we care about is being persuasive because we demonstrate the the proper research mode. So there this isn't this isn't a qualitative um or quantitative analysis and there is no rigor. This is our experiences that are interesting and insightful.
**Philosopher:** Yep.
**Colleague:** Grumble. Um. But then yeah, and then it's also sorry.
**Philosopher:** Should you be should you be recording this so that you can
**Colleague:** Oh no, I am recording this.
**Philosopher:** Good. Okay, cool.
**Colleague:** Yeah, no no. Like that yeah. Um, and then, and then the other thing is is that because there's two of us, being forced to explain our thoughts to the other who's going through the same experiences means that there has to be almost an intersubjective agreement between us that yes, these claims are worth making. Mhm.
**Philosopher:** Yeah, no, I think that sounds that sounds really good.
**Colleague:** It's an intersubjective vibe agreement, right? Did we both have the same vibe using these tools? Yes. Can we communicate the thoughts of our vibe? Yes. Is there any rigor in this? No. Cool.

\section*{Section 3: Methodology - Question 3}

\subsection*{Question Text}

\subsubsection*{q3 o3}
Which three selection criteria (rank-ordered) best explain why you focused on OpenAI Deep Research, Perplexity, Elicit, and Hugging Face’s Open Deep Research—and what is the strongest counter-argument someone might raise about this sampling? Please give each criterion a one-clause justification (“because …”) so we can weave them directly into the paragraph.

\subsection*{Transcription}

**Colleague:** All right. Um. Okay, so at least it's going to proleptics. So, which three selection criteria? Why is O3 so obsessed with whatever? Uh which three uh selection criteria rank ordered? Oh my god. Best explain why you focused on OpenAI Deep Research, Perplexity, Elicit, Hugging Faces Open Deep Research? And what's the strongest counter argument someone might raise about the sampling? Please give each criterion a one clause justification. These are all the products on the market.
**Philosopher:** Yeah. No, I mean and we did use different I guess we should specify that we did different things with different products that we wanted to compare like for lit review we wanted to compare perplexity, elicit, you know, and the you know the open yeah. And you know, but then for um, for the other thing the like elicit and perplexity failed so badly at those things that we didn't bother trying to use them to like find instances of the tools that that you know, that deep deep research and hug and the open deep research are the ones that were good for for, you know, um
**Colleague:** Data collection.
**Philosopher:** If for data collection. Um. And then for for ideation, we we used the frontier models. We used Claude 3.7 and you know, Chat GPT.
**Colleague:** And the variation of OpenAI's models that change by the day.
**Philosopher:** Right.
**Colleague:** Um, so no, it's like it's availability. It is what services were offered by by either the publishers of frontier models or companies that have established track records.
**Philosopher:** Yeah.
**Colleague:** I I think I tried very briefly something in Scholar OS and was not impressed. Because at the end of the day, most of the secondary companies just scrape semantic scholar. It's it's boring. We also evaluated computer use I had hiccups this is great. We evaluated computer use, um, Claude's computer use, uh OpenAI's operator. Oh we should poke operator again to see if it's any good.
**Philosopher:** It wasn't very good the last time.
**Colleague:** Yeah, it wasn't persuasive. But like basically, we ran an initial test of is this at all worth our time. The only ones that came back with anything worth our time was OpenAI's deep research and um
**Philosopher:** The Hugging Face open
**Colleague:** The Hugging Face one. Um, mostly because the others just aren't aren't um
**Philosopher:** Yeah, and I mean and I have I've I've tagged those in I I'm I'm mostly done. I got a few left, but I've tagged most of them in Zotero to show what you know, which which um model found what. And you know the vast majority of the um the vast majority of the sources that we use that that I found for the paper were found with uh deep research uh compared to the you know compared to the other if we're talking about the lit review. Um that elicit and perplexity added very little uh to the lit review. Um and honestly in this, I think in other in other topics, I think I could have done uh better in um like Scopus and uh Google Scholar. But but this particular topic, I found particularly uh Scopus and what's the other one uh Web of Science like were not very good at surfacing um you know things on tool use on like uh tool longevity uh that I I try and I've got the I've saved the queries that I did that show that like deep research was actually quite good at this uh as far as the lit the lit search goes.
**Colleague:** Yeah. And I mean like the the point that we're making is is that this was just this is this was us answering the question um with what was available.
**Philosopher:** Yeah. And one thing that I think we should probably highlight is that we did uh disaggregate the research project into these different things. Like data set, you know, data set creation, uh you know, uh lit review, and then the ideation and writing process.
**Colleague:** Yeah. Okay, so this is actually important. Uh, our Fames account has Gemini Advanced with Deep Research with 25 Pro. Mhm. So, I am going to um drop in our our prompt um that we've done just because
**Philosopher:** Before you do that just uh thing. In the for the sake of like closing off this paper and getting it published as quickly as we can, do we just want to call it with the February models for the data cre- collection?
**Colleague:** I mean honestly, I I want to make sure that um uh the model like I want to make sure that deep research that Gemini's deep research um doesn't um have any astonishing capabilities here.
**Philosopher:** Uh Have you I guess if we do that we should probably also uh are you paying the \$200 yet for uh Claude uh like
**Colleague:** I I am using well the 100 but I haven't used their research yet. On like I'm I'm only doing Gemini Advanced because it's here.
**Philosopher:** Yeah.
**Colleague:** Um. And if I get access to Claude then I will I will do it there. But um
**Philosopher:** Yeah because Claude's got their deep research thing too.
**Colleague:** And and that's the thing is as we go new products will come and honestly unless there will be a extremely persuasive like the reason why I haven't bothered with Gemini is that it's It's not systematic, right? Like I'm looking at this and it did a it did a uh task deconstruction which is great. And then it did a web search using Google which surfaces the usual suspects. Um but it's not iterating. Mhm. Um and so honestly until a major new product is announced that's really good Mhm. And that's kind of why we have to get this paper done. But yeah, so so effectively this is use everything, make sure that we're on top of the latest releases until such time as it's time to write. We started writing what? A month ago?
**Philosopher:** Yeah.
**Colleague:** Yeah. Um. So yeah, like it looks like Gemini 25 Pro Deep Research is going to be pretty good. Do we have our in our data Do we have Fames in our data set? is actually the question.
**Philosopher:** Uh yeah, we did. We did get we we that was uh so
**Colleague:** So, it picked up so uh uh 01 deep research picked up the start in 2012, the e-Research conference in 2015, 2016, 2018, 2019, 2020, 2021, 2022, 2023. Yeah. So, eh. So so 01 did a it did exactly what we wanted it to do.
**Philosopher:** Yeah.
**Colleague:** Um and it looks like looks like uh Gemini Deep Research is retreding the exact same ground. And so the the point that we're making is how do these tools capabilities change? Mhm. Um rather than like there is a clear gulf of capability beyond between OpenAI's deep research and perplexity's deep research.
**Philosopher:** Right. Perplexity and elicit are nowhere you know just
**Colleague:** Yeah, they're just nowhere close. Um and and Google is catching up but it is catching up which means that our insights into deep research are um good enough. Like I I basically
**Philosopher:** Yeah I guess a good tradeoff here is probably going to be to uh run so we've got um Open open AI deep research, we've got Fames there. You run like you're doing now, you run Gemini on Fames and you run Claude deep research on Fames.
**Colleague:** And I will
**Philosopher:** And you and you just show and you show them side by side in a table.
**Colleague:** Oh that's actually quite good yes. Um.
**Philosopher:** And and you know if it's like what we think it's going to be they're going to be very similar and you say look these these are all at we're not there's no order of magnitude difference between any of these. They're all roughly the same so we're going to focus the rest of this paper on um you know on or this discussion on OpenAI deep research uh because it was it was available first and we used it.
**Colleague:** Yeah and like if if there's no difference in kind like there's certainly going to be difference in degree but there's no difference in kind between uh OpenAI deep research and well actually we don't know because it's still crunching along. Yeah. Um but like yeah I I think a table here is just right. Um assuming that Claude research comes through because that's of course its its own assumption. Yeah. Yeah. No and and hey that means we get to cite all the Fames stuff one last time.
**Philosopher:** Yeah.
**Colleague:** Woo. Um. Although the like the point that we're going to make is what did it miss? Mm. Um. Because
**Philosopher:** Yeah and I mean we should we we we probably should like uh you know put uh I mean I can pretty quickly generate you know the bibliography of all publications related to Fames but we have it it's in Zotero I think. And uh you know and say well you know even just looking at our paper we could do the we could do the articles and a selection of um like uh high higher profile conference presentations and say here's the Well there's the blog post right? Like we can we can just say what years were we active based on our own knowledge and we have a four column table.
**Philosopher:** Yeah.
**Colleague:** Um. Yeah okay. Cool. Um that was actually a useful digression.
**Philosopher:** Yeah I think that could be very powerful to have here is the master data set of what we know about our own project and here's deep here's OpenAI Google and and Anthropic's views of that like uh you know their results on that.
**Colleague:** Yeah. And what's the strongest counter argument? Um well we're money bags because we're spending hundreds of dollars a month. And the answer is yeah it'd be nice if it was cheaper. Um. Yeah. And what other counter arguments would there be? Well what about this tool that I've just invented? Yeah this is moving week by week. We're not going to catch everything. We're trying to catch things that are mentioned by those This is a snapshot looking it's a snapshot in time. We know things are moving but you can't let that stop you doing the the research. You know um because many of these lessons are going to be valuable for few as tools become more capable.
**Colleague:** And this paper generalizes. Just because we're making a critique of deep research doesn't mean that this hasn't been founded on the last however many weeks since four uh Bing Sydney released.
**Philosopher:** Yeah.
**Colleague:** Um. Oh I'm going to be so All right yeah that's good. And we don't actually need the answer to whether or not Gemini advanced is at all good. But yeah um Fames us has Gemini advanced so that's something we should remember.
**Philosopher:** Yeah.
**Colleague:** All right. And stop recording.

\section*{Section 3: Methodology - Question 3}

\subsection*{Question Text}

\subsubsection*{q3 Claude}
What specific research tasks did you systematically evaluate these tools on, and what were your primary evaluation criteria for success or failure in each task?

\subsubsection*{q3 o3}
From the full set of activities you tried (literature discovery, data-set creation, annotated-bibliography building, ideation/drafting, etc.), which three tasks most clearly stress-tested the models’ alleged research judgment—and, for each task, what single observable indicator did you treat as “success or failure”? Please answer in the triad format below (substituting task and indicator names): 1. Task: \textit{specific research task} / Indicator: \textit{success/failure metric} 2. Task: \textit{specific research task} / Indicator: \textit{success/failure metric} 3. Task: \textit{specific research task} / Indicator: \textit{success/failure metric}

\subsection*{Transcription}

**Colleague:** So yeah, so like I fed it the exact same prompt. That prompt went through seven iterations. The default expectations of Gemini Deep Research didn't achieve the outcome.
**Philosopher:** Okay, so you got a report and not uh not data.
**Colleague:** Yeah, the table in the middle of the report is good. Like it hits all of our dates including Fames 1.0 release in 2014, which nothing else and it noticed that the grant was was like the data quality is good. Um. Oh.
**Philosopher:** Can you show me the table? Can you put get show me what you're looking at?
**Colleague:** I generate I don't know how to share. You know what? Uh export to docs. Uh Bueler? Loading. Loading.
**Philosopher:** Loading.
**Colleague:** Loading. Okay, now I have to find the window with you. Here, I'm just gonna Okay wait a sec something pinged something's happening it's
**Colleague:** So it made a 20 page report. Oh my god. Anyways.
**Philosopher:** Which it now want now Gemini is offering to me to summarize it.
**Colleague:** Uh yeah, it's so stupid.
**Philosopher:** Okay. But even the table, it's like it's not really
**Colleague:** It wasn't what we asked for.
**Philosopher:** It's not data, it's not data the way that the other thing the other things are data.
**Colleague:** The other ones gave us what we asked for. This didn't.
**Philosopher:** Yeah.
**Colleague:** Um. And thus we have evidence about why we ignore Gemini. Like it's because our expertise says that it's it's kind of shit really.
**Philosopher:** Yeah. There's some good some of this report looks a little like slop but some of it's actually I mean it is pretty impressive.
**Colleague:** Yeah like Gemini is not bad. It's just
**Philosopher:** Not what we want because we're try not that that the task the task was make help us make a data set.
**Colleague:** Yes. And it
**Philosopher:** And it it just yeah. It just failed. Yes.
**Colleague:** Um, do we want one more of these? Um. Because it's 8:30 right now.
**Philosopher:** Yeah, I'm I'm gonna uh I think we probably got a lot to uh some things that we want that Oh did you want to so research tasks investigated? Oh that's where we go through how we disaggregated the research process. Um you know and that's uh 3.3 should be relatively straightforward, short paragraph that just says that we you know broke it down into lit search Yeah. data set development and then say a few words about each of those thing and then you know ideation and um uh composition. And then maybe we have a step at the end of you know review and criticism like of uh um you know um uh re drafting not redrafting what's revision that's what I'm after. Um.
**Colleague:** You know so research task investigated should be fair should be um you know quite straightforward. Evaluation framework should probably focus on mundane utility. that that's what we are after. Basically we looked at the entire research process and that first day we started on ideation and we tested how well these agentic in quotes tasks um Yeah. helped.
**Philosopher:** Yeah cuz so yeah I I ideation, lit review, data set uh creation, composition Yeah composition like all of what I've been doing right now is composition. Yeah this is composition. Yes. And and uh joint composition, you know. So composition and then I'm sure we'll have a revision step at the end.
**Colleague:** Oh of course we'll put a revision. Like the logs for this are going to be stupid. Um but like oh my god the the the logs for this, the logs that I'm going to have to put up on OSF are going to be that's going to be a week of work.
**Philosopher:** Yeah. And that this is actually something that we should probably talk about is how uh how difficult it like how honorous I guess is the word that I'm looking for it is to document this process and get it like that that um Oh it's going to suck. LLMs are not really geared for showing you know for you sharing your work and making you know like
**Colleague:** Oh yeah no. Like I have a set of bookmarks at the top of my browser for this exact reason.
**Philosopher:** Yeah.
**Colleague:** Yeah. And so my primary evaluation criteria is did it provide mundane utility at that stage.
**Philosopher:** Yep.
**Colleague:** Um and ideation, no, not really. Lit review, yes, absolutely. Data collection, yes star. Um the composition process, this is spooky, yes.
**Philosopher:** Yeah. Uh-huh. Um
**Colleague:** Yeah.
**Philosopher:** Yeah. Yeah, pretty So that sounds it sounds about right.
**Colleague:** Yeah and and it's it's literally a gut check. It's like as professional researchers, as academics for for our careers, is this something that we're using because it's making our research more effective or is this something that we're using for the sake of it? And honestly the mundane utility question is the honest evaluation.
**Philosopher:** Yep. And I I was kind of thinking of maybe it would be worth dividing this into the two things like is it improving like is it making research more uh like uh efficient and faster and to what extent you know and is it making the research does it help let me rephrase that. Does it help make the research faster? And I think the answer is yes. Otherwise you know we've been sitting on this paper for 10 years and we finally got it
**Colleague:** Like for the data collection it made it possible.
**Philosopher:** Right. So it so that's an efficient there's an efficiency side to it and a quality side to it.
**Colleague:** And I would say that I would never have done a 5500 word lit review without this approach.
**Philosopher:** Right. Yeah, no, I I mean I'm I'm finding that it it has that it if used correctly it can have positive or thoughtfully if used thoughtfully it can have positive quality implications.
**Colleague:** Yes. So
**Philosopher:** So that like just breaking down mundane utility.
**Colleague:** That that's fair. And and honestly this approach in composition has taken me longer than just writing it.
**Philosopher:** Right. Like
**Colleague:** So
**Philosopher:** So yeah so tradeoffs again as we've seen with other software tools that yes that
**Colleague:** It's the
**Philosopher:** something yeah. It
**Colleague:** It's richer data not faster data.
**Philosopher:** Yeah.
**Colleague:** It's the claim that we made in our in the Fames papers.
**Philosopher:** Yeah. Yeah. All right. From the full set of activities uh that you tried Oh, this is a different question. I like it. Which three What is its obsession with counting? Uh most clearly stress tested models alleged research judgment and for each task what indicated did you treat as success or failure? Please answer in the triad format below. One, two, three, four. Yeah, no, that's stop. Counting Oh Okay, yeah, no. O3 I I I'm going to go back to using 4.5 because O3 is shit.
**Philosopher:** Yeah. No for this kind of work I I I think that 4.5 like just looking I've only used O3 a little bit um and I I think for this kind of writing work 4.5 is still is is clearly
**Colleague:** Actually here's a test. I'm uh Okay, so I'm going to keep this question in a um thing. I just want to see if 4.5 just given alone does a better question. All right so as that's going What was it? What
**Philosopher:** Oh sorry.
**Colleague:** Uh most clearly stress tested the model's alleged research judgment and for each task what indicator? Uh data collection uh that we just demonstrated was the fundamental stress test. Did it figure out what sources to what the So 4.5 just starts writing.
**Philosopher:** Yeah.
**Colleague:** 4.5 says you're done, you don't need to answer the question. Just start.
**Philosopher:** Uh maybe it's because you switched in the middle, I don't know.
**Colleague:** Yeah no no like it it's it's it is interesting. Um
**Philosopher:** So but um no I mean the I I in that part in this part about the stress test thing Yeah. I would say that there's two there's two steps to it. One was tool discovery like you know go through these journal articles and find the tools and or these journals and find the tools and the second you know and even harder one was um
**Colleague:** And they're not good at that. none of them.
**Philosopher:** And it Right. And that took a lot of scaffolding you know that it can't
**Colleague:** But the point it it really failed at judgment at discovering research tools.
**Philosopher:** Right. But we did we did get like 80 or 90 out of it in the end. So it it worked ask. Um you know but and then the next step was like you know find the instances of the tools. Uh and I'm not sure whether you think that worked better or worse.
**Colleague:** Well I I think that that is a good example of its judgment. And then honestly I would say comp like the questions that the the reflective what would this be auto interview? Whatever. Um the questions that this composition technique are also a fantastic discussion on judgment because we can watch it watching us and the acuity and relevance of the questions are themselves very useful.
**Philosopher:** Yeah. Yeah. It's uh having uh using the uh the LLM as an interlocutor to develop ideas and uh you know for and improve improve the rigor of your writing.
**Colleague:** It really does yeah.
**Philosopher:** And this is And this is a very different way than what how most people use it. Like most people want to make their writing faster and easier and don't want to get they like 04 because it tells them how great they are all the time. Uh you know that that uh uh four yeah or 40 sorry. Uh but this is Yeah. You know this is uh uh more like you know the where when you this is really trying to use the models to find you know to develop your ideas find weaknesses in your arguments make sure you're being thorough. Um and it's uh it's a very different way of using it than than what I what you usually see.
**Colleague:** And like I'm I'm using the article that I did with uh Mark Alfano the proleptic arguments like I'm I'm using all of this to make the researcher uh richer and and that's the point.
**Philosopher:** Yeah.
**Colleague:** Okay.


\section*{Section 4: Outline Rejigger - Question 1}
\subsection*{Question Text}
Which specific AI research tools should be your primary focus in Section 4, and which should be demoted to "Other Tools"?

\subsection*{Response}
Our analysis of AI research tools prioritizes those offering substantive pragmatic utility. The primary focus should be on:

\begin{enumerate}
    \item \textbf{OpenAI's suite of tools}, specifically:
    \begin{itemize}
        \item OpenAI Deep Research (principal focus)
        \item GPT 4.5
        \item O3 and O1 Pro (to a notably lesser degree)
    \end{itemize}
    
    \item \textbf{Anthropic's offerings}:
    \begin{itemize}
        \item Claude 3.7 with reasoning capabilities
        \item Anthropic Research (May 2025 release) - demonstrably useful and of the same kind, though slightly better in degree than OpenAI's Deep Research
    \end{itemize}
    
    \item \textbf{Google's Research Tools}:
    \begin{itemize}
        \item Google Gemini Pro 2.5 (model 03-25)
        \item Google Gemini Pro 2.5 Deep Research
    \end{itemize}
    Despite being "of the same kind if a lower degree than the other two," these warrant inclusion for their search parameters and as comparison points.
    
    \item \textbf{Hugging Face's Open Deep Research} - merits a dedicated smaller section
\end{enumerate}

Tools that should be demoted to "Other Tools" as subsections include:
\begin{itemize}
    \item OpenAI's Operator
    \item Anthropic's Computer Use
    \item Elicit's systematic reports
    \item Perplexity's Deep Research
\end{itemize}

This structure reflects our testing findings where Google's tools and O3 were functionally "mediocre at everything" without distinctive capabilities. However, they require inclusion as major market offerings providing useful comparative context. The "other" category tools demonstrated insufficient practical utility for substantive research tasks but illustrate the broader ecosystem.


\section*{Section 4: Case Studies - Question 1}
\subsection*{Question Text}
Claude: What specific failure patterns did you observe during your testing of OpenAI's Deep Research that most clearly demonstrated its lack of judgment? Please provide concrete examples where the tool displayed technoscholastic tendencies by privileging textual authority over critical assessment, and describe how these failures manifested differently from simple factual errors.

\subsection*{Response}
OpenAI's Deep Research exhibited consistent judgment failures across multiple domains. Its most fundamental deficiency was a profound lack of epistemic humility, exemplified during our initial ideation phase (run 20250207). When tasked with creating "a synthetic structured prompt to systematically gather longevity data on historical research software," it produced an elaborate seven-stage plan it was utterly incapable of executing. The system designed output for what a competent research assistant should accomplish rather than what it could actually do—requiring seven iterations to produce even a basic CSV output.

This failure manifests precisely what Hicks characterizes as "bullshit" in his critique of ChatGPT: outputs with no relationship to actual capabilities. When asked for refinements, it produced system prompts addressing humans as if they were AI ("this system prompt ensures you consistently gather key data..."), revealing a profound disconnect from its operational reality. This represents not merely incorrect output but systematic arrogance that undermines its own functionality.

The clearest examples of technoscholasticism emerged in the Digital Humanities research tasks. When examining the Australian DH landscape, Deep Research uncritically presented defunct centers as active, most notably the University of Newcastle's Centre for 21st Century Humanities. It described this as a "regional center linking history and computing" that represents Australia's DH ecosystem. However, investigation revealed:

1. The URL linked to a defunct research center (broken links to "About" pages)
2. The center's webpage displayed a death notice for its leader (April 2024) stating "there will be no further updates to data on this website"
3. References to renowned scholar John Burrows lacked temporal context—he was at Newcastle from 1987-1989 before retiring, yet this historical detail was presented as current

Most telling was that my prompt explicitly requested identification of "trends in program creation, continuation or discontinuation since 2020." Despite these clear instructions to assess temporality, Deep Research privileged domain authority (.edu.au) over critical assessment. It accepted institutional claims at face value without acknowledging the possibility that projects end, websites become outdated, or centers close.

This differs fundamentally from simple factual errors. The system did locate correct information (Burrows was indeed at Newcastle, the center did exist) but failed to apply critical judgment about temporality, continuity, and context. It did not recognize that a single course offering in 2020 does not constitute an ongoing program, or that websites often remain online long after projects conclude. When presented with clear evidence of a center's discontinuation (death notice), it still presented it as a current exemplar.

These judgment failures create what can be characterized as "undergraduate source soup"—uncritical aggregation without the historiographical awareness to evaluate when claims were made, by whom, for what purpose, and whether they still apply. Deep Research demonstrated no capacity to infer from useful absences of evidence or to question what sources could legitimately know. While retrieving content with reasonable factual accuracy, it remained fundamentally unable to judge that content's temporal relevance, institutional context, or comparative significance.

\subsection*{Response 2}
Beyond the epistemic humility failures and technoscholastic tendencies detailed above, our extended evaluations revealed further judgment deficiencies in Deep Research's literature review capabilities. The system consistently produces what we characterize as "undergraduate source soup"—a heterogeneous mixture of sources ranging from high-quality journal articles and government reports to random blog posts of dubious scholarly value.

A particularly telling pattern is its prioritization of less authoritative sources when more authoritative alternatives exist. The system frequently cites a researcher's blog rather than their peer-reviewed journal articles, or links to GitHub repositories instead of academic websites. This demonstrates a failure to hierarchically evaluate source authority within disciplinary contexts.

While the system does provide utility—accelerating the initial gathering phase of research materials—it requires substantial human judgment to filter and evaluate its outputs. Approximately two-thirds of the sources in our bibliographic dataset originated from Deep Research, but these required extensive critical evaluation, with many ultimately discarded as unsuitable. The remaining sources required manual discovery through traditional methods including citation chasing and disciplinary database searches.

This positions Deep Research as comparable to an entry-level research assistant rather than the "agentic" system claimed in marketing materials. It provides net research value but must remain under close supervision, with researchers applying their own judgment to compensate for the system's inability to distinguish between authoritative sources and peripheral content. The system fundamentally cannot replace scholarly judgment at any stage of the research process, despite its utility in accelerating certain mechanical aspects of discovery.

Its performance varies with context window management, with observable degradation in output quality as prompts approach length limits. This technical constraint further undermines its reliability, as the system does not consistently indicate when it has reached operational capacity—instead producing increasingly compromised outputs without warning.

\section*{Section 4: Case Studies - Question 2}
\subsection*{Question Text}
o3: From your February and March logs, which single run (date + short description) provides the clearest, self-contained demonstration of both (a) useful data extraction and (b) a judgment failure you consider emblematic? Please name that run and note the concrete outputs or missteps we should quote.

\subsection*{Response}
The question itself exemplifies a structural limitation in O3's approach to research inquiry—an obsession with singular examples rather than pattern recognition. Nevertheless, our Digital Humanities in Australia analysis (20250214) offers the clearest demonstration of both effective data extraction and profound judgment failure.

Deep Research successfully compiled an extensive catalog of Australian university Digital Humanities initiatives, demonstrating reasonable breadth in data gathering. The system identified numerous institutional programs, centers, and projects across multiple universities—data that would have required hours of manual searching. This resource identification function represents the system's core utility.

However, the same run perfectly encapsulates Deep Research's fundamental inability to exercise critical judgment regarding temporality and institutional status. The most emblematic failure concerned the University of Newcastle's Centre for 21st Century Humanities, which Deep Research unequivocally characterized as an active "regional center linking history and computing" that "represents Australian regional center[s]" in the present tense.

This assessment directly contradicted obvious evidence on the very webpage it cited:
1. A prominent yellow banner announced the April 2024 death of the center's leader, explicitly stating "there will be no further updates to data on this website"
2. Navigation links to "About" pages were broken
3. No mention of the center appeared on the university's current research pages

The system's failure to recognize a defunct center—despite literally citing a death notice—exemplifies its technoscholastic approach. Similarly, when analyzing Western Sydney University, the system presented a single course offered once in 2020 as evidence of an ongoing Digital Humanities program, though verification revealed the program was officially marked "SUSPENDED" in the university catalog.

A comparable pattern appeared in tool analysis runs. As Shawn noted with ArboDat (run 20250221), the system overlooked the explicit statement on the homepage that "it has been developed since 1997" while searching elsewhere for chronological data. Despite the date appearing prominently in the opening paragraph, Deep Research somehow prioritized less relevant content—evidence that its attention mechanisms differ fundamentally from human scholarly judgment.

These are not simple oversights but systematic failures to contextualize information within temporal frameworks—a fundamental aspect of scholarly judgment. The system processes text without the historiographical sensibility to ask: "When was this written? Is it still true? How would I know if it weren't?" -- 


[Shawn Ross] 15:05:20
Uh, I mean, it's more like… it was finding, sort of, when things were written, but then didn't look at the thing in… the content of the thing being written, and… extract the, you know, extract the information like that. Like, it knew that this, you know, that the…

[Shawn Ross] 15:05:39
You know, website, you know, was put up in, you know, 2008 or whenever, but it didn't see that the first paragraph of the website said that this, you know, this, um.

[Shawn Ross] 15:05:52
This software tool was first developed in 1997. I'm not sure it was 2008. Like, I don't have it open in front of me, but you get the point.

\section{Observation, Claude}


[Shawn Ross] 15:06:22
So, now, it was, uh… the other thing we've got… the other thing I've got to get in here somewhere, I've got to see which question of yours that it should… would best go under is that, um.

[Shawn Ross] 15:06:33
It's funny how, like, in some cases, uh, Claude Research just got obsessed with something, like.

[Shawn Ross] 15:06:40
The version history of the tool in question, because I kept telling it, just focus on the current version, and it wouldn't do it. And I finally had to, like, make… give it a CSV, you know, column where it could talk about the history of the tool. That seemed to make it very happy, and it put it all there, right? But it just… you couldn't shut it off. It, like, really wanted to talk about the history of the tool.

[Brian Ballsun-Stanton] 15:07:00
Yeah.

[Shawn Ross] 15:07:07
Um, you know, and the versions, and this version did that, and that version did that.

[Shawn Ross] 15:07:11
And it took a little… it would sometimes, in its current assessment, get things mixed up, and say, you know, FAMES is Android only, you know, and stuff like this, right? Or field… you know, and um… you know, it wouldn't quit, like, looking at the history, so I had to give… but it was… I was able to develop the prompt so that it would keep the current version and old versions, previous versions, like, discrete and separate and organized the way that… but… but I had to give it a place to talk about the history of the… of the tool. It was very strange. Like, the whole…

[Shawn Ross] 15:07:47
The whole way it responded to that.


\section{Gemini Observation}

Brian: Moving on, uh this is Gemini Pro 0506.

Brian: Um, just an interesting aside. It opens its first message. I understand that we're focusing on section four, case studies of AI research tools. Great, you're doing better than 03 did. Aiming for a detailed outline based on my autoethnographic findings.

Brian: Sorry, what?

Brian: Somehow it has swapped human and AI here, which is weird.

Brian: We'll then prioritize tools with genuine pragmatic utility, great. Your plan to question me on claim capabilities versus observed It has flipped AI Oh my god, I haven't seen this for a while. But no, it just flipped um who it thought it was.

Brian: Um, cool. I'm going to hit regen and I'm going to just see if it fixes.

Brian: Um,

Brian: because that's weird.




\section{Gemini Observation}

Brian: Considering section four as a whole, what is the overarching argument you intend to make about the current state of these AI research tools? Noting that this is in section five analysis, so no, whatever. Based on your case studies, what central theme or pattern emerged most strongly from your investigations of these diverse systems?

Brian: Okay, Gemini.

Brian: You've just read 5500 words where I'm talking about an absence of judgment.

Brian: Maybe my central argument that this section is providing evidence to the rest of the paper for is an absence of judgment.

Brian: Um, basically the okay, but with being less facetious here, although it's really hard with Gemini. Um,

Brian: These things are tools. These are not PhD students. Um,

Brian: but they're shockingly useful tools if you understand the grain of each tool and how best to use it. Right? The central argument is

Brian: these are neither the obsoleters of thinking and humanity this week, nor are they um confabulating waste of time, although there's some days when they absolutely are confabulating waste of time.

Brian: Instead, how you prompt and how the specific research infrastructure is set up informs the kind of outputs you get. And and treating research as something that can write a research report instead of help something that can help you work through quite a few sources to highlight things that you may want to refer to is fundamentally the problem. So, these are tools, these are not to quote Adams, your plastic pal who's fun to be with.

Brian: But also, please read the rest of the paper because that's what I've been talking about the last 55, sorry, 6,000 words.

Brian: Fuck.

\subsection*{Additional Model Behavior Observations}
Brian: Gemini somehow swapped the human and AI personas. It flipped "I" and "you" in a weird way. I've seen this before when running simulations—it just swaps persona entirely, which is fascinating.

Shawn: Yeah.

Brian: Another telling observation is GPT 4.5 anchoring on content from the style guide. We used FAMES as an example in the style guide, and then the model fixated on it, forcing me to rerun everything with a revised prompt.

Shawn: Is that because of the example text? Maybe just delete the example?

Brian: Yes, I fixed it eventually. It was just revealing of how these models process information.

Shawn: It's interesting how they latch onto seemingly random elements in a prompt.

Brian: What I find particularly amusing is Gemini asking "what argument do you want to make?" when I've just written 6,000 words explicitly articulating that argument about absence of judgment.

Shawn: I couldn't get ChatGPT Deep Research to handle the tool metadata task at all—it just didn't engage with it properly.

Shawn: I've documented a good comparison sequence where I gave the tool metadata task to all the models, recorded each response, refined the prompt based on the best two results, then iterated. Claude Research quickly emerged as clearly superior when using FAMES as my control case for critical assessment.

Brian: That's a great example of systematic comparison across models.

Shawn: That's my typical approach—try all models, identify what works, refine the prompt, and record both prompts and responses at each stage to track improvements.

\subsection*{Additional Limitations and Model Behaviors}
Brian: Let's review our findings on OpenAI's tools.

Shawn: Deep Research performed best for literature reviews in our February runs, despite the "undergraduate source soup" limitations we discussed. However, when testing tool metadata collection in April-May, O3 actually performed better than Deep Research. Claude eventually performed best, but required specific handling.

Brian: I've observed across all models that there's simply no sense of temporality—this was particularly evident in the Digital Humanities evaluations.

Shawn: Yes, temporality awareness is a significant high-level deficiency. While people often focus on AI limitations related to corporeality, this temporal blindness represents a fundamental cognitive gap. It's a systemic issue worth highlighting, though probably beyond the scope of what we can fully explore in this paper.

Shawn: Another persistent issue is Claude's insistence on creating hyperbolic titles for reports. Despite placing instructions in system prompts, project knowledge, and direct prompts, it occasionally accepts a specified title format for one or two runs before reverting to its default behavior.

Brian: Clearly that behavior is hard-coded somewhere in their system prompts.

Shawn: What's particularly strange is that sometimes the correct title appears in the chat interface where Claude announces the report, but when you open the actual document, it displays a different, typically more sensationalized title.

Shawn: I've also noted models struggle significantly with versioning. For long-term projects like FAIMS with major architectural and functionality changes, the models consistently confuse different versions and can't maintain coherent chronological distinctions. Claude eventually sorted this out with very specific prompting, but I gave up trying with the other models.




\section*{Section 4: Anthropic Tools Assessment - Question 1}

\subsection*{Question Text}
What specific capabilities and limitations did you observe when testing Anthropic's tools, particularly Claude 3.7 with reasoning, and their newly researched research feature? How did their performance compare to OpenAI's Deep Research, especially considering the judgment dimensions?

\subsection*{Response}
Brian: Claude's writing is distinctly more humane and erudite without the corporate bombast present in other models. When conducting auto-interviews, Claude's questions are more consistently good, though GPT 4.5 occasionally exceeds it—Claude has a higher floor but a slightly lower ceiling for insight.

Brian: For research functionality, Claude's architecture is distinctive. It always creates four or more sub-agents, seemingly finding justifications for this structure rather than deriving it from task requirements. The causal arrow appears backwards—it decides on N agents, then explains why that's appropriate. 

Brian: Claude's research approach involves many web_search calls with relatively few full page fetches. It doesn't expose its reasoning behind these calls, but appears to add search result snippets to the context windows of its sub-agents, only fetching complete pages when it identifies a good fit. This approach works particularly well for code-related tasks—when writing a Git scraper to evaluate Deep Research quality, it effectively fetched documentation and implemented good design paradigms.

Brian: The hard-coded constraints are fascinating. Claude Research will always produce a report with a hyperbolic title, and it will never directly follow output requirements for alternative formats like CSVs. After completing its research, we must explicitly instruct it to convert findings to CSV format.

Shawn: I experienced the same title issues. Even when I specifically instructed it not to use hyperbolic titles and provided examples of preferred formats—in system prompts, project knowledge, and direct instructions—it occasionally complied for one or two reports before reverting. Sometimes the correct title appears in the chat interface but changes to something more sensationalist in the actual document.

Brian: Claude Research's multi-agent approach makes it somewhat more conscientious than Deep Research, as it doesn't wander off in a single thread. However, it remains fundamentally of the same kind, as each sub-agent appears to accumulate web search results and then "vibe" on those results without critical assessment.

Shawn: After extensive testing across all models for tool metadata tasks, Claude Research consistently produced the most accurate results. When checking familiar tools like FAMES, I found no outright inaccuracies, just minor differences in emphasis. The quality was sufficient that I expanded the task to include sustainability critiques and alternatives analysis.

Brian: When I used Claude for literature reviews, it performed better than Gemini's Deep Research—impressive given Google's search advantage. However, I observed shockingly high rates of confabulation, despite Claude having all relevant search results available. It seemed to lack a consistency check mechanism for reconciling sources.

Shawn: It required significant prompt engineering—about eight iterations with careful field enumeration and example records—to get Claude to reliably produce CSV output. However, once properly configured, it produced consistently formatted data. The style guide also successfully improved the writing quality, breaking away from the default "mid-level marketing executive" tone.

Brian: None of this violates our technoscholastic thesis. Claude never pauses to critique sources or question its own findings. There's no evidence step, no checking step, no verification of claim existence. It simply takes inputs and "vibes" on them without critical assessment—the essence of technoscholasticism.

Shawn: A frustrating technical limitation is that when exporting Claude Research reports to PDF, all citations disappear, severely limiting their utility for academic work despite otherwise valuable content.

I've created a subsection focusing on additional Claude failure modes from your conversation. The cleaned transcript highlights:

Claude's refusal to use Australian English despite explicit instructions
Its behavior when encountering hallucinated tools (creating elaborate "not found" reports)
The pattern of acknowledging specific formatting instructions then ignoring them completely
CSV formatting issues occurring in approximately 1 in 10-15 runs
Claude's tendency to expand beyond the specified task (the HTML Talk example)
The fundamental pattern of prioritizing perceived helpfulness over accuracy or instruction adherence
The conversation format is maintained while focusing on the substantive observations about model limitations rather than the surrounding conversational elements.





\subsection*{Additional Claude Failure Modes}
Shawn: I encountered several interesting Claude-specific failures beyond the issues already discussed. Despite explicit instructions, I could never get Claude to switch to Australian English spelling conventions—it simply refused.

Brian: These systems have clear hard-coded behaviors they won't deviate from.

Shawn: Another fascinating case occurred when Claude encountered hallucinated tools—tools that appeared in our generated dataset but didn't actually exist. Claude produced elaborate reports explaining its inability to find information about these non-existent tools, which was unintentionally hilarious.

Shawn: I also repeatedly instructed Claude to "maintain a specific format, follow Australian spelling conventions, and execute prompts without asking follow-up questions." It would explicitly acknowledge these instructions, confirming it would comply with all three requirements—then immediately do none of them. This is particularly problematic when trying to generate consistent, comparable outputs.

Shawn: CSV formatting issues appeared in approximately 1 in 10-15 runs, requiring me to delete the entire chat and start again. And in one notable case with an older tool (HTML Talk from 1990s Internet Archaeology), Claude insisted on producing a comparative report about this tool and newer alternatives rather than focusing on the specific historical tool as instructed. It seemed determined to be helpful by providing broader context rather than adhering to the precise task.

Brian: That perfectly captures the issue—these systems prioritize being helpful over being accurate or following exact instructions.

Shawn: Exactly. Even with extensive prompt engineering, Claude maintains this tendency to helpfully expand beyond specified parameters, requiring constant vigilance and correction.


\subsection*{Critical Evaluation Deficiencies in Research Mode}
Brian: This temporality issue isn't specific to Claude—it's a problem with all the models.

Shawn: Absolutely. I'm only discussing Claude 3.7 because it was the model that made it through our selection process as the best performer for these metadata tasks.

Brian: So even the best-performing model has this fundamental limitation.

Shawn: Beyond temporality issues, I've observed a consistent lack of critical evaluation. When FAIMS claims capabilities in our documentation, the models simply accept these claims at face value.

Shawn: The pattern is revealing: when product A claims strength in X and product B claims strength in Y, the models report "Product A is better at X, and Product B is better at Y"—they fit the claims together like puzzle pieces without evaluation.

Brian: And what happens when both products claim to be good at the same thing?

Shawn: They just report that both products claim it. There's no attempt to weigh contradictory claims or determine which might be more accurate.

Brian: They don't critically evaluate. Though interestingly, you can sometimes get them to critically evaluate if they're not in research mode.

Shawn: That's the key distinction—it seems they can't perform open-ended searches while simultaneously maintaining critical assessment.

Brian: Even in compare-and-contrast exercises, they can report positive claims but can't identify gaps or inconsistencies.

Shawn: I wonder what would happen if we presented them with directly contradictory academic assessments—like our JFA article about FAIMS and the competing article about ArcGIS tools that criticizes us, while we criticize them.

Brian: That might be interesting, but probably beyond the scope of this paper.

Shawn: Agreed. The key observation remains that these systems fundamentally lack the judgment to question marketing claims or critically assess competing assertions—they accept what's written rather than evaluating what's claimed.

\section*{Section 4.2: Anthropic Tool Assessment - Question 1}

\subsection*{Question Text}
For section 4.2, considering Anthropic's offerings, Claude 3.7 reasoning and Anthropic research, what specific failure patterns related to judgment or manifestations did you observe? How did they contrast?

\subsection*{Response}
A day after my previous assessment, I ran the same Digital Humanities prompt through all three major systems (Claude, GPT, and Gemini). The results revealed differences in degree but not in kind regarding judgment failures, particularly in their characteristic technoscholastic tendencies.

The most revealing failure pattern was their inability to apply judgment by inferring from "useful absences of evidence." Western Sydney University provides a perfect example: the models discovered a course offering from search results dated 2020. As a human familiar with university operations, I immediately checked the year selector to see the offering history and found it was offered exactly once in 2020, then discontinued. All three systems failed to investigate this temporal context, instead treating the mere existence of a course description as evidence of ongoing programs.

This failure persisted despite my revised prompt explicitly instructing them to check for confirmation of continuation or discontinuation. The systems searched only for explicit announcements of discontinuation—they weren't examining death notices (though one lab was indeed shut down following its director's death), verifying whether named personnel still worked at the institutions, or recognizing that 2020 announcements without subsequent updates likely indicated discontinued initiatives.

Claude Research approached this differently in implementation—spinning up six sub-agents—but failed identically in essence. All sub-agents ran as singular threads without cross-checking or synthesis. There was no attempt to build a coherent picture through induction of the actual Australian DH landscape at any given moment. Instead, the system took words on web pages as direct "temporal evidence of the thing itself" without questioning why or when they were written, or what patterns would characterize truly active versus abandoned programs.

This exemplifies technoscholasticism perfectly: the systems process "the words on the page are this, the words from the university say this, therefore this is positive evidence of this thing" without applying the crucial interpretative layer of "why did they write this, when did they write this, and what patterns of writing would we expect to see if these were truly active programs?" 

Consequently, all three systems completed the assigned task but failed to accomplish the goal. They painted an implausibly positive perspective of Digital Humanities in Australia that simply doesn't align with reality. This isn't a fundamental technical failure that better prompting could entirely solve, but rather a characteristic manifestation of a broader epistemic issue with judgment in these models.

Gemini's approach produced a sprawling 47-page report while Claude "did its own thing," but the underlying judgment failures remained consistent across platforms. I focused specifically on Australian programs because I know the landscape firsthand, allowing me to directly verify the accuracy of their assessments and identify their consistent overoptimism.



\section*{Section 4: Effective Usage Patterns - Question 1}

\subsection*{Question Text}
In your practical interactions with AI tools (Deep Research, GPT 4.5, GPT 3, GPT 4 Omni, Gemini Pro), what specifically defined effective usage patterns or strategies that maximized their utility despite limitations? Provide concrete examples of instances where strategic prompting or methodological improvements significantly improved outcomes or usability.

\subsection*{Brian's Approach}
Regarding effective usage patterns, I distinguish between prompt engineering and prompt iteration. Prompt engineering involves systematic refinement through multiple versions—for our Deep Research workflows, we went through seven iterations before achieving acceptable results.

The first strategic principle is that words matter profoundly. I always begin with a clear goal statement that implicitly establishes register and expected response mode. While I've mostly abandoned explicitly requiring roles, that initial paragraph communicates what I want from the conversation and shapes subsequent interactions.

When iterating, I typically include the instruction "functionally decompose this task and give me a read back." This differs from the common "think step by step" approach in an important way. While "think step by step" has been specifically optimized for in recent models (particularly reasoning-focused ones), "functional decomposition" encourages building a to-do list that allows me to validate how the system plans to accomplish my goal. This provides three benefits:

1. It creates scaffolding so the LLM knows the expected next step
2. It allows me to verify that tacit information in my prompt was correctly interpreted
3. It lets me check register and domain alignment—is it using language from the discipline I intend?

I find "read back" particularly useful as relatively technical jargon expressing "tell me what I told you to do." The language of functional decomposition implies breaking apart rather than merely stepping through, and seems more effective, though I can't fully articulate why—it's partly intuitive art.

A second critical strategy for Deep Research specifically is recognizing what's beyond your control. The programmers' fingerprints are far more visible in these workflows than in standard AI interactions. Hard-coded behaviors persist across platforms:
- Claude Research will always produce hyperbolic titles and textual documents
- Gemini will always return text documents even when explicitly asked for CSVs
- OpenAI's Deep Research applies its internal citation format regardless of instructions

This necessitates saving every prompt version (version control is essential—this is programming) and understanding what's negotiable versus immutable in system behavior.

Additionally, focus on single tasks. Even when that task involves calling multiple agents, get systems to do one coherent thing at a time. Most "agentic" workflows don't handle multiple topics well—they'll expend the same effort regardless of scope, potentially diluting results. The possible exception is Anthropic's Research with its multi-agent architecture, but even there, limitations persist. Testing is required to identify the "cognitive limits" of what constitutes a feasible single task.

Finally, each model has a distinct "grain" (like wood grain)—a direction in which it moves naturally versus directions requiring force. For effective use, identify and work with each model's grain:
- Gemini 2.5 Pro excels at transcription, outperforming alternatives
- Claude 3.7 provides consistently strong writing and generally good questions
- GPT 4.5 occasionally produces remarkably insightful questions despite slower processing and generally weaker writing

I haven't found O3's "grain" yet, which demonstrates this is fundamentally about taste rather than objective capability. Many prefer O3, but I find its tendency toward confabulation ("being a lying liar who lies") too frustrating. The key isn't that it's objectively inferior, but that I haven't discovered where it naturally excels.

\subsection*{Shawn's Complementary Methodology}
Shawn: My approach to prompt engineering for complex tasks evolved through extensive testing. For the tool metadata extraction project, where I needed a prompt I could reliably run across 300+ software tools, I followed a systematic methodology:

1. I started with elements from previous successful prompts we had developed together
2. Conducted a meta-prompting pass with Claude (which I found significantly better at prompt refinement than other models)
3. Tested the initial prompt across multiple systems (Deep Research, O3, Gemini Pro 2.5, and Claude)
4. Began culling models based on performance—Deep Research immediately failed to meet requirements
5. Modified the prompt iteratively through manual refinement, not relying solely on meta-prompting
6. Continued testing across remaining models until O3 and Gemini stopped showing improvement, while Claude reached about 80% of desired performance
7. Conducted additional meta-prompting specifically with Claude to refine further
8. Identified immutable model behaviors (like Claude wanting to provide historical narratives) and adapted by creating dedicated sections for these tendencies rather than fighting them
9. Focused on specific challenges like consistent CSV formatting and controlled vocabulary use
10. Addressed writing style issues by providing my own papers, requesting analysis of my style, and negotiating a compromise style ("tune this two-thirds of the way towards you, one-third towards me")

This process required approximately eight hours across two days, but resulted in a highly effective 200-word prompt that has performed exceptionally well since. I've saved artifacts documenting this journey for potential inclusion in appendices.

One interesting discovery was GPT 4.5's strength with creative writing elements like paper section titles. When Claude was producing pedestrian or overly enthusiastic titles, GPT 4.5 could generate sophisticated options on the first attempt, demonstrating a different kind of creative capability that complemented Claude's systematic strengths.

The key insight is that investing substantial time in prompt development pays dividends when the resulting prompt will be used repeatedly. Additionally, working with model tendencies rather than against them—finding ways to channel their natural behaviors productively—yields better results than attempting to force models against their design constraints.

IMPORTANT: spend 8 hours improving a prompt on 3-4 models to find out the "grain" of the prompt. Spend time iteratively prompting, and compare responses *across* model families. Sometimes you just need to work with the grain of the system.


\section*{Section 4: Google's Research Tools - Question 1}

\subsection*{Question Text}
Claude: How did Google's Gemini Pro 2.5 Deep Research perform in your testing compared to OpenAI and Anthropic's offerings? What specific judgment failures or technoscholastic tendencies did you observe with Google's implementation, and were there any areas where it demonstrated relative strengths?

\subsection*{Response}
Gemini "Deep Research with 2.5 Pro" (if we're being technical) was of the same kind as other offerings, but lacking in comparison. Despite having access to Google Search it demonstrated two significant limitations:

1. It never used Google Scholar, despite Google owning this resource
2. It employed a single-threaded approach, collecting an absurd number of hits per run without providing evidence that it examined anything in detail

The fundamental failure of Gemini's approach is that it represents the ultimate in Carnapian confirmation—essentially stamp collecting. It produces trivial thinking-logs like: "I've made progress in accessing some key resources for understanding the current state of Digital Humanities. I successfully analyzed a chapter from 'Debates in the Digital Humanities' that highlighted the ongoing need for true collaboration between computer scientists and humanities scholars, the importance of robust infrastructure, and the potential for DH to drive new research questions. I also gained valuable insights from the ACD-Engine project's JupyterBook, which focuses on improving cultural data in Australia and has implications for infrastructure and skills development in the region."

This output is pure technoscholasticism. It takes a single "paper" and extrapolates broad statements from it without establishing proper inference (since multiple data points aren't being synthesized). Even when explicitly instructed to "Be specific, intentional, conscientious, meticulous, and triple-check your claims. Take extra extra extra time. Engage in proleptic reasoning and look for evidence against your claims before making them," the system maintained its undergraduate-level stamp collecting approach.

The pattern can be characterized as: "I have a webpage that makes this claim, it looks good, therefore this claim is true and can cover everything." Fundamentally, it doesn't appear that the designers of this implementation gave it sufficient space to double-check itself. Rather than engaging source-by-source, it processes information in blocks of 20-50 sources at once. This exemplifies technoscholasticism—finding sources and building uncritical narratives atop complete acceptance of those sources' claims.

By missing the critical insight that most academic programs fail silently, it makes claims utterly divorced from reality, operating in a world where projects only fail if they explicitly announce their failure—not even recognizing cases where a program director's death notice appears on the program website.

When tasked with creating a more traditional annotated bibliography, Gemini significantly underperformed Claude Research. Not only were a higher proportion of its cited sources confabulated, but the sources it found were far less useful overall—we added perhaps half as many to our Zotero bibliography compared to Claude's outputs. 

Because it doesn't spend adequate time on individual sources, Gemini Deep Research is best conceptualized as a shotgun mass-summary of search results with mild winnowing rather than either (a) methodically examining each source individually to determine its utility (which it could do with Google Scholar access), or (b) running secondary searches on anticipated sources to confirm their existence.

\subsection*{Additional Observations on Gemini's Behavior}
Shawn: Beyond Deep Research issues, I've found Gemini has distinctive operational characteristics. The most important insight is that "less is more" with Gemini—despite its advertised million-token context window, smaller, more focused prompts tend to work better.

Brian: That aligns with my experience as well.

Shawn: Small changes to prompts made surprisingly large differences in output quality, particularly with audio transcription tasks. The model seems to have what I'd call a "light trigger"—it's easily derailed or distracted during extended tasks like transcribing 25 minutes of audio.

Brian: Yes, relative to other models, Gemini definitely has a very light trigger sensitivity.

Shawn: It's notably "touchy" about prompt wording. When it works, it really works well, but maintaining consistent performance across multiple iterations proved challenging. I've only been using Gemini intensively for the past couple weeks, but this sensitivity to minor prompt variations stands out compared to other models.

Brian: This matches my observations. The technical architecture seems more brittle despite occasional strong performance.

Shawn: These characteristics reinforce our earlier points about model "grain"—each system has distinct operational tendencies that must be understood and accommodated rather than fought against. Gemini's grain appears to involve careful prompt minimalism despite its theoretical capacity for longer contexts.


\section{Section 4: Gemini}

For Section 4.4 (Hugging Face's Open Deep Research) and Section 4.5 (Other Tools: OpenAI's Operator, Anthropic's Computer Use, Elicit, Perplexity):
What were the most salient observations, specific judgment failures (or perhaps unexpected successes in mundane utility) for these tools? How did their performance, particularly in their attempts at "agentic" or research-oriented tasks, compare to the more prominent Deep Research implementations from OpenAI, Anthropic, and Google? (Focus on what makes them distinct or reinforces the broader patterns you've observed).


\subsection{For OpenAI's Operator, Anthropic's Computer Use, Elicit, Perplexity}

Operator is a tech demo. Because it doesn't decompose its tasks, take notes, and work to a plan, it can't maintain context on even simple data collection tasks, much less broader research tasks where the automation would be useful. The idea that it can eventually navigate a web browser as an "agent" for the user is promising (though any direct MCP integrations are just going to be better and more legible for the systems), but the tooling isn't there yet. 

Computer Use, running on a local docker container, offers more promise around what programs can be used. But it was even more expensive and didn't even manage to get through a single task of "go find a paper and load it." The fact that it runs in a container on the local computer is fantastic (though expensive in API credits) -- but again, this is a tech demo rather than anything persuasive.

Elicit uses a bad model and is hamstrung by its use of Semantic Scholar. While the even-more=expensive version might be promising in its "systematic reports" -- the data source quality and the indifferent amount of inference it runs  means that the paper-discovery, then paper-winnowing steps are flawed. If we could use the model of our choice, and hook it to web of science or google scholar, the steps it outlines in paper selection and evaluation are markedly *better* than all the other services: its technology choice lets it down.

Perplexity's average amongst this lower-grade bunch. It runs a small amount of reasoning, does not have excellent access to the academic literature, and produces results of ignorable quality. Clearly they designed their "research" project for a different use case than the one we were assessing. Again, it is the lack of systematicity and conscientiousness, plus the sub-par model chocie, plus the data quality that hamstring this for our purposes. If a user simply wants a trivial digestion of a few sites  for a slightly more thorough search-result analogue, it's adequate. Perplexity clearly has spent more focus on optimising its search queries than the others (if only because it shows them to the user) -- but there's no evidence of triangualtion or taking any of the sources critically: another perfect example of technoscholasticism.  

\section{Hugging Face's Open Deep Research}

\begin{quote}
\textbf{Brian:} The only Hugging Face run I ran was the elaboration of Open Archaeo.

\textbf{Shawn:} Elaboration of Open Archaeo.

\textbf{Brian:} The tagging and descriptions of Open Archaeo.

\textbf{Shawn:} I was dissatisfied with the elaborator functionality. That's why I initially set up the Claude research to find metadata about the tools. I find that I get much better results through the chatbot interface.

\textbf{Brian:} I didn't iterate on the prompt at all for Open Deep Research.

\textbf{Shawn:} I very trivially reproduced this functionality in Claude Research, and then got it to do additional tasks. The disadvantage is I have to run it one tool at a time. Both OpenAI and Anthropic don't expose their research models through their APIs in the same way as through their chatbots.

\textbf{Brian:} So with Open Deep Research, we didn't find that the expense was particularly worth it.

\textbf{Shawn:} It's like 60\% of what you get out of a chatbot, but you can script it. I got significantly better results from iterating on the prompt through the chatbot interface.

\textbf{Brian:} The prompt I used was specifically for improved descriptions and determining if the toolpath applied.

\textbf{Shawn:} When I started with this approach, all the models—03, Claude, and Gemini—wanted to do more than just basic description. They started doing comparisons and additional analyses. I began incorporating these additional capabilities into my prompts.

\textbf{Brian:} So Open Deep Research has potential, but is very expensive to iterate on.

\textbf{Shawn:} Exactly. It's initially promising, but needs work. We'll investigate it further in future research.
\end{quote}

\section{Other Tools}

\subsection{OpenAI's Operator and Anthropic's Computer Use}

\begin{quote}
\textbf{Brian:} Operator is a tech demo, not impressive. Computer Use is even more of a tech demo, not impressive.
\end{quote}

\subsection{Elicit and Perplexity}

\begin{quote}
\textbf{Shawn:} From Elicit I got only 9 useful sources, compared to 55 sources from Deep Research. Perplexity was initially frustrating—when I asked for actual citations, it only provided 3, but it claimed to have analyzed 25 sources. Looking more closely at its internal references, I might end up with about a dozen useful sources.

\textbf{Brian:} Elicit has the critical issue of being hobbled by Semantic Scholar.

\textbf{Shawn:} The Elicit sources were only marginally useful—just one was picked up by other search methods. It's not just the quantity but the quality was the lowest of the different approaches. Perplexity at least identified some major sources like the Endings Project and "Fading Away the Challenge: Sustainability and Digital Studies," which Deep Research also found.

Perplexity did surface a couple of things I wouldn't have found otherwise, like a researcher's web page on "Desks of Digital Scholarship" that examines the lifespan of digital projects, not software specifically.

But ultimately, you're not going to write a paper based on the results of Elicit or Perplexity alone. You could potentially write a paper based on the sources that Deep Research finds—not the report it generates, but the actual sources it identifies.
\end{quote}

\section{Comparative Analysis from Formal Response}

\subsection{Open Deep Research}
Open Deep Research's singular specific utility was in the ability to both interact with the filesystem and user-specified search APIs plus running queries and operations from a local university computer. This was significant because it enabled giving a CSV of Open Archaeology tools with instructions to "improve the description" of each one. That run cost \$200 in o1-pro credits. Structured data iteration was possible, but at even greater expense than normal monthly expenditure. It could potentially be extended with the model context protocol and connected to better tools and academic databases, but the cost per-run is prohibitive. This explains why web-based services chose less systematic approaches.

Only one major run with Open Deep Research was conducted, primarily due to expense. Model selection optimization for each query could help mitigate costs, and future papers could investigate implementing such workflows. There is no "secret sauce" to a "deep research" workflow—just agent-calling. PyDantic's AI framework's tool-calling graph design shows promise in this direction, but all these remain tool-using instances rather than goal-following systems.

\subsection{Other Tools}

Operator functions merely as a technology demonstration. Its inability to decompose tasks, take notes, and work to a plan prevents it from maintaining context even on simple data collection tasks, let alone broader research requiring automation. While the potential for browser navigation as a user "agent" is promising (though direct MCP integrations would be more effective and legible), the current tooling remains underdeveloped.

Computer Use, operating via a local docker container, shows more promise regarding program accessibility. However, it proved even more expensive and failed to complete even a basic task of locating and loading a paper. Though running containerized on a local machine is advantageous (despite API credit costs), it remains a tech demo rather than a persuasive solution.

Elicit employs an inadequate model and suffers from Semantic Scholar limitations. While its systematic reports feature (at higher expense) shows potential, the paper-discovery and winnowing processes are compromised by data source quality and insufficient inference capabilities. With a superior model choice and integration with Web of Science or Google Scholar, its paper selection and evaluation methodology would substantially outperform competing services—its technology choices are its primary limitation.

Perplexity ranks as average among these lower-tier options. It performs minimal reasoning, lacks robust academic literature access, and produces results of negligible quality. Its "research" functionality appears designed for different use cases than scholarly assessment. The absence of systematicity and conscientiousness, combined with suboptimal model selection and data quality, undermines its utility for rigorous research. For users seeking basic digestion of a few sources—essentially an enhanced search result—it suffices. Perplexity has evidently focused more on search query optimization than its competitors (demonstrated by showing queries to users), but shows no evidence of triangulation or critical source assessment—exemplifying technoscholasticism.

\section{Anthropic's Comparative Advantages in Judgment Dimensions}

\subsection{Question Analysis}

Among the three judgment dimensions tracked throughout this analysis—epistemic humility, inductive capacity, and correspondence with reality—which dimension shows the clearest relative improvement in Anthropic's tools compared with OpenAI Deep Research, and what concrete behavior demonstrates this improvement?

\subsection{Assessment}

Anthropic's Claude demonstrates a minor but notable increase in epistemic humility compared to other models. At the start of its Digital Humanities report, it explicitly acknowledges "knowledge gaps requiring further research" and describes several active issues in the field. While it remains unclear whether these gaps were extracted directly from sources or represented the model's own insights, the explicit acknowledgment of limitations represents an improvement in epistemic posture.

More significantly, Claude demonstrated observable improvements in correspondence with reality through its approach to information gathering and synthesis. Its multi-agent architecture enabled a breadth-first search pattern that appeared less prone to distraction and tangential exploration than OpenAI's implementation. As Shawn observed during comparative testing:

\begin{quote}
I was doing these side-by-side as I was iterating over the prompts, and dropped models as they fell behind. I was getting more complete and accurate answers, with fewer confabulations from Claude...what it produced was consistently higher quality in terms of accuracy, completeness, and thoroughness.
\end{quote}

This improvement in reality correspondence was further evidenced by Claude's ability to provide more accurate contextual information about tools in our domain, including more thorough research on competing technologies.

Claude also demonstrated marginally better epistemic humility in its operational capabilities assessment. Rather than making unrealistic claims about processing capacity:

\begin{quote}
It would make optimistic but not ridiculous claims about what it could do regarding how many tools it could research in a single run. It would say, "If you want something thorough, I can do 1 to 3." When the prompt got longer, it explicitly warned me not to request more than two tools...It described how its performance would degrade, noting that with 10 tools, "you're going to get a shallow summary."
\end{quote}

This contrasted with OpenAI Deep Research's tendency to uncritically accept all tasks regardless of scope: "I'll go right out and do everything you say in your prompt," only to abandon processing without warning when exceeding capabilities.

Despite these relative improvements, we maintain that the difference between these implementations remains one of degree rather than kind. All examined systems continue to demonstrate technoscholastic tendencies—privileging textual authority over critical evaluation. The knowledge gaps identified by Claude appeared to be drawn from existing literature rather than representing genuine analytical insights, reinforcing our assessment that even the most advanced current implementations remain fundamentally limited in their judgment capacities.


\section{Section 4, 4.5}
\subsection{Question text}
In your evaluation of these AI tools (Deep Research, Anthropic Claude, Gemini), you've repeatedly noted specific issues regarding their handling of temporality and context. Could you explicitly articulate how this issue of "temporality blindness" impacts their practical use as research tools? Provide clear, concrete examples where the failure to critically assess temporality led to misleading or incorrect conclusions.

\subsection{Response}
A class which exists in 2020, but not 2021--2025 is *not* an example of a currently active Digital Humanities group/lab/focus/whatever at a university. Highlighting https://www.uwa.edu.au/schools/research/digital-humanities-research-cluster, something surfaced by Claude and OpenAI, The text on the page, not even requiring cross-referencing is: "In our first year (2020) ... this will be the springboard for future projects." A  trivial example a write-once update-never academic page that simply hasn't been cleaned up in a website refresh.

WSU examples:

Claude:
```
Western Sydney University

Digital Humanities Research Initiative
Host of ADHO conference (2015)
Mapping Print, Charting Enlightenment project
```

OpenAI:
```
Western Sydney University Digital Humanities Research Initiative – Western Sydney Univ. (Australia). Current; runs projects like Time Layered Cultural Map (TLCMap).
```

Gemini:
```
Western Sydney University offers a Master of Digital Humanities blending humanities and computational sciences.``` This page leads to ... a pamphlet. Searching, https://handbook.westernsydney.edu.au/hbook/course.aspx?course=1836.1 it exists, then looking at the specific unit: "Digital Humanities Research Methods (PG)" it shows 1836.1	Master of Digital Humanities	SUSPENDED

Just because a uni makes a claim on their website, doesn't make the claim *true today*. All of these models paint a much rosier view of the state of digital humanities  in Australia because they do not account for the silent failures that are not announced and not withdrawn, the modal action case of a university.

\subsection*{Temporality as a Fundamental Limitation}
Shawn: I discovered another revealing example of the temporality problem. When directly instructing Claude Research to focus on FAMES with specific prompts about "keeping the history and version straight" and "always be clear about which version you're talking about," it successfully produced an accurate chronological account.

Shawn: However, when FAMES appeared peripherally as an alternative for other software (like iDig), Claude described it as "Android only"—completely mixing up past and future versions. Even after getting the history correct as a primary focus, it randomly confused temporal details when referencing the same tool in a secondary context.

Brian: Given how frequently this issue has emerged, temporality clearly needs to be a central element in our conclusions.

Shawn: Absolutely. The major discussion items should include lack of judgment as we've been discussing, lack of temporality awareness, and the tendency to prioritize "vibes" over accuracy.

Brian: This contextual inconsistency in temporal understanding seems to be a deep structural limitation.
