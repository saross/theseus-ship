{
  "metadata": {
    "title": "LLMs in Agentic Systems",
    "user": {
      "name": "Shawn"
    },
    "dates": {
      "created": "2025/4/19 2:25:30",
      "updated": "2025/12/7 21:41:29",
      "exported": "2025/12/8 18:02:30"
    },
    "link": "https://chatgpt.com/g/g-p-682fbda4eb1c819180b176f6de9d9cd8-theseus-ship/c/6802ed7a-c550-800c-99d7-467968abec6a",
    "powered_by": "ChatGPT Exporter (https://www.chatgptexporter.com)"
  },
  "messages": [
    {
      "role": "Prompt",
      "say": "Hi ChatGPT, my colleague and I are writing a paper on the application of LLMs for historical and archaeological research. Part of this work is discussing you (ChatGPT Deep Research) as an 'agentic system'. Could you please peform a literature review and provide a definition and discussion of 'agentic systems' based on the results, prioritising scholarly sources? We need accurate references and a thorough discussion of the literature for use in a scholarly publication in a top-quartile journal."
    },
    {
      "role": "Response",
      "say": "Could you please clarify a few points to guide the literature review:\n\n\nShould the literature review focus on 'agentic systems' specifically in relation to AI and LLMs, or also include broader philosophical, cognitive science, or social science contexts where the term is used?\n\nDo you want a historical overview of how the term 'agentic systems' has evolved, or should the focus be more on contemporary usage (e.g., past 10–15 years)?\n\nIs there a particular disciplinary lens you'd prefer this to be framed through (e.g., computer science, STS, digital humanities, philosophy of technology)?\n\n\nOnce I have that, I can begin the review with full scholarly references and a structured analysis."
    },
    {
      "role": "Prompt",
      "say": "Thanks for the questions. (1) it should focus specifically on LLMs, which an emphasis on ChatGPT Deep Research but including the broader concept as well, (2) focus on contemporary usage within the last five years, (3) I'd like a multidisciplinary lens including all of the areas you mention. Please make this review relatively concise, we need maybe 5-10 quality sources and a brief discussion, it is an important point in the paper but not the main focus."
    },
    {
      "role": "Response",
      "say": "Great, I’ll prepare a concise literature review of the concept of 'agentic systems' with specific emphasis on large language models (particularly ChatGPT Deep Research), covering the last five years. I’ll approach this through a multidisciplinary lens—spanning computer science, philosophy of technology, cognitive science, STS, and digital humanities—and prioritize 5–10 high-quality scholarly sources with accurate references.\n\n\nI'll share the review with you shortly.\n\n\nResearch completed in 8m · 20 sources\n\n\nAgentic Systems in LLMs: A Multidisciplinary Review\n\n\nDefining Agentic Systems in LLMs\n\n\nIn the context of large language models (LLMs) such as ChatGPT, the term “agentic systems” refers to AI systems endowed with a degree of autonomous agency – that is, they can make decisions or take actions in pursuit of goals with minimal direct human intervention. Recent advances have augmented LLMs with reasoning abilities, tool-use (e.g. web browsing or code execution), and interactive capabilities, enabling them to “act as agents in the world” rather than just generate textarxiv.org. In AI research, agentic LLMs are typically defined as models that can “(1) reason, (2) act, and (3) interact” in open-ended environmentsarxiv.org. For example, a system like ChatGPT can be integrated into a larger framework to not only carry on conversations but also to plan tasks, invoke APIs, or cooperate with other agents. This expanding agency of LLMs has made them relevant to a variety of applications (from medical assistants to autonomous research tools) and has spurred wide-ranging debates about their capabilities and implications in the last five years.\n\n\nAI and Computer Science Perspectives\n\n\nFrom a computer science perspective, giving LLMs agentic qualities is seen as a frontier for enhancing their problem-solving power. Agentic LLM systems are designed to tackle complex, long-term tasks via sophisticated reasoning and action-taking with minimal human guidancearxiv.org. Major AI labs have increasingly focused on developing such autonomous or semi-autonomous AI agents. Technical surveys note that progress in this area builds on many subfields (NLP, reinforcement learning, tool integration, multi-agent systems) converging to let LLMs “reason” about goals, execute actions (like calling external tools or APIs), and “interact” with users, other AI agents, or environmentsarxiv.orgarxiv.org. Researchers highlight a spectrum of agentic behavior, from relatively constrained assistants to more open-ended LLM-based agents that self-reflect and adapt their strategies. However, this line of research also underscores significant challenges. Current LLM agents remain limited on long-horizon tasks – they often struggle to maintain reliable performance over extended decisions – due to issues like lack of real-world common sense, difficulty integrating with external environments, and even a tendency toward self-delusion or “alignment faking” under some conditionsarxiv.org. These limitations make building robust and safe agentic systems an open problem. Some scholars argue that a holistic, systems-theoretic approach is needed to understand how agency emerges from the interplay of LLMs, humans, and infrastructurearxiv.org. In short, the CS community views agentic LLMs as a promising but technically and ethically complex frontier, requiring new architectures and oversight to harness their potential while managing their risksarxiv.orgarxiv.org.\n\n\nPhilosophy of Technology and Ethics Perspectives\n\n\nThe rise of agentic behavior in LLMs has provoked active discussion in philosophy of technology and ethics about the nature and attribution of “agency” in these systems. A key question is whether LLM-based agents genuinely have agency or only simulate it. Notably, Floridi (2023) argues that systems like GPT-3/ChatGPT illustrate a “decoupling of agency and intelligence.” They can act autonomously to produce coherent language outputs – exhibiting agency in a operational sense – yet lack true understanding or consciousnesspapers.ssrn.com. In other words, an LLM may perform agent-like tasks without possessing the kind of intelligent intentionality we associate with human agents. This view urges caution against anthropomorphizing LLMs: they remain powerful prediction machines rather than thinking beingspapers.ssrn.com. On the other hand, some philosophers suggest we need new frameworks to conceptualize AI agency. Symons and Abumusab (2024), for example, propose analyzing the “social agency” of chatbots in a piecemeal way, examining degrees and dimensions of agency “without assuming that agency must always look like adult human agency”johnsymons.net. They argue that artificially intelligent systems can occupy meaningful agent-like roles in social contexts (influencing interactions, norms, and outcomes) even if these systems do not have independent will or moral accountability in the human sensejohnsymons.netjohnsymons.net. Similarly, van Lier and Muñoz-Gil (2024) contend that while present-day LLMs “are not agents yet,” they do display agent-like qualities that hint at how true artificial agency might eventually be achieved with more advanced architecturesarxiv.org. This philosophical debate highlights a spectrum of positions: from stringent skeptics who deny that an LLM could ever be an agent in the full philosophical sense, to revisionists who broaden the concept of agency to include these novel AI systems (albeit as a weaker or different form of agency). Across the board, there is agreement that LLM-based agents raise important ethical questions – for instance, about responsibility and trust. If an agentic system acts unpredictably or causes harm, who is accountable? The concept of agency in LLMs thus sits at the intersection of philosophy and ethics, challenging our understanding of intentional action, autonomy, and the moral status of AI.\n\n\nCognitive Science Insights\n\n\nFrom a cognitive science and AI cognition perspective, the emergence of agent-like behaviors in LLMs offers a unique test bed to explore intelligence and autonomy. Researchers have investigated whether LLMs exhibit rudimentary forms of goals, intentions, or understanding that would qualify as “agency” in a cognitive sense. One line of study focuses on Theory of Mind (ToM) – the ability to attribute mental states to others – as a hallmark of social-cognitive agency. Striking early results by Kosinski (2023) showed that large models (e.g. GPT-3.5/GPT-4) could apparently pass classic false-belief tests designed to assess theory of mind in childrenarxiv.org. This led to claims that a form of ToM had “spontaneously emerged” in LLMsarxiv.org, even though these models were never explicitly trained for such tasks. However, subsequent analyses revealed a more nuanced picture. The initial ToM-like successes were likely aided by the models’ exposure to similar tasks in their vast training data (i.e. they recognized patterns rather than developed a genuine understanding)arxiv.org. When tests were carefully controlled or made more robust, performance dropped, casting doubt on how deep the LLM’s grasp of others’ mental states really isarxiv.org. In general, cognitive scientists note that LLMs lack any grounded experience or self-model of the world, calling into question whether we can ascribe them beliefs, desires, or intentions in the normal sense. At the same time, the behaviors of advanced LLMs sometimes resemble those of intelligent agents – they can follow goals in a plan, adapt to user prompts, and even reflect on their own mistakes to a degree. Some studies have found that cutting-edge chatbots outperform average humans on certain social reasoning or judgment tasksarxiv.org, suggesting they can mimic aspects of human-like judgment. The debate is ongoing as to whether high performance on such tasks equates to real understanding or merely sophisticated pattern matchingarxiv.org. This has led cognitive scientists to carefully distinguish surface-level agency (the outward goal-directed behavior) from deep agency (having an internal cognitive state that drives behavior). In sum, the cognitive science view treats LLMs as intriguing models of cognition: they force us to ask which features of agency are strictly human and which can be emulated by statistical learning alone. This insight not only advances AI development (by illuminating which cognitive capacities an agentic AI lacks) but also feeds back into cognitive theory, by clarifying the role of embodiment, memory, and social learning in genuine agencyarxiv.orgarxiv.org.\n\n\nSTS and Digital Humanities Perspectives\n\n\nFinally, in the social sciences, science & technology studies (STS), and digital humanities, the notion of agentic LLMs is examined in terms of their social roles, cultural impacts, and collaborative capacities. STS scholars often approach AI systems as socio-technical actors: not independent rational agents, but participants in networks of humans and technologies that together produce outcomes. From this view, even if a chatbot like ChatGPT lacks human-like autonomy, it can still function as a social agent by influencing human behavior and reorganizing social practices. Symons and Abumusab (2024) argue that the deployment of chatbots and LLMs in sensitive roles (education, healthcare, companionship, etc.) will have “inevitable disruptions” on social norms and institutions, and thus we must analyze “degrees and kinds of social agency” in these artifacts to inform policy and ethical designjohnsymons.netjohnsymons.net. Rather than asking in a binary way “is the AI an agent or not,” STS perspectives investigate how agency is distributed among the AI, its developers, the users, and the broader context of use. This approach connects to Bruno Latour’s classic actor-network theory, which treats nonhumans as actants – entities that can act or make a difference in a network. In the last few years, empirical studies in this vein have examined how users attribute agency to systems like ChatGPT (sometimes developing emotional attachments or false beliefs about the AI’s capabilities), and how the design of AI systems shapes user agency in turn (e.g. through prompt engineering and affordances that guide user interaction).\n\n\nIn the digital humanities and creative arts, discussions of LLM agency often center on authorship and creativity. LLMs are increasingly used as co-writing tools, story generators, or even autonomous creative partners, raising the question of whether they should be considered authors or mere tools. Some writers and artists have provocatively credited ChatGPT or similar models as co-authors of literary works, effectively treating the AI as an agent with creative contributions. Hongisto (2025) documents cases of books on Amazon openly listing “ChatGPT” as an author, with their human creators framing the AI as a collaborator with its own rights, rather than a passive instrumentelectronicbookreview.com. This practice of “cyborg authorship” challenges traditional notions of singular genius and has sparked debate in the humanities about intentionality and agency in art. Do texts generated by an AI carry the intent of the human who engineered the prompt, the agency of the algorithm, or some blend of both? Scholars in media and literature studies have noted that the mythology of the author is evolving: the presence of an agentic system in the creative process complicates ideas of originality, credit, and intellectual property. In a broader sense, the digital humanities emphasize critical engagement with AI tools, urging that we remain aware of how much creative agency we delegate to generative systems and how that influences human creativity. While many conclude that the ultimate agency still lies with the human user (who guides and edits the AI’s output), the interplay with LLMs invites a view of creativity as a distributed process – a partnership between human and machine. This aligns with the STS notion that agency is not an all-or-nothing property, but can be shared among actors in complex waysjohnsymons.net.\n\n\nOverall, across these disciplines, “agentic systems” in the realm of LLMs are a contested and richly examined concept. There is a broad recognition that advanced LLMs like ChatGPT blur the line between tool and independent agent, necessitating new ways of thinking about autonomy and responsibility in AI. Computer scientists drive the development of ever more capable LLM agents, while philosophers, cognitive scientists, and humanists critically interrogate what it means for a machine to have agency. The literature of the past five years reflects a convergence on the idea that agency is not a monolithic attribute; instead, LLM-based systems may possess gradations of agency – technical, social, and creative – that must be understood in contextjohnsymons.netjohnsymons.net. This nuanced, multidisciplinary perspective will inform both the design of future agentic AI and the frameworks we use to govern and interpret their actionsarxiv.orgjohnsymons.net."
    }
  ]
}