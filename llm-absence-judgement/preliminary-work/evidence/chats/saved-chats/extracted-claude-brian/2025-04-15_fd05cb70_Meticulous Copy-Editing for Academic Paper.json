{
  "uuid": "fd05cb70-9074-492f-8882-0bcd4ac03607",
  "name": "Meticulous Copy-Editing for Academic Paper",
  "summary": "",
  "created_at": "2025-04-15T06:03:46.011227Z",
  "updated_at": "2025-04-15T06:19:50.011209Z",
  "account": {
    "uuid": "17538ba7-7aa5-450c-9d3f-69ab346ad799"
  },
  "chat_messages": [
    {
      "uuid": "ae0a6e99-897e-44da-89ff-20f2480acede",
      "text": "Hi Claude. Please take the role of a highly meticulous copy-editor. You will take *extra* time and be extremely thoughtful in your discussions and considerations. The user, above all else, requires thoughtful and reasoned editing and discussion. We need to ensure that everything is in a single, consistent, and clear register. We also need to remove some words, where possible.\n\nI would like you to highlight issues rather than making corrections. Then, for each issue, please thoughtfully discuss the issue. Make sure any suggested contributions (rare) are in bold so that we may quickly assess them.\n\nAttached is a long paper. My objective is for us to highlight issues with flow, clarity, and consistency, and for you to function as a highly attentive copy editor.\n\nTo begin, please provide a functional decomposition of these tasks and give me a readback of the headers of the document.",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:03:27.090512Z",
          "stop_timestamp": "2025-04-15T06:03:27.090512Z",
          "flags": null,
          "type": "text",
          "text": "Hi Claude. Please take the role of a highly meticulous copy-editor. You will take *extra* time and be extremely thoughtful in your discussions and considerations. The user, above all else, requires thoughtful and reasoned editing and discussion. We need to ensure that everything is in a single, consistent, and clear register. We also need to remove some words, where possible.\n\nI would like you to highlight issues rather than making corrections. Then, for each issue, please thoughtfully discuss the issue. Make sure any suggested contributions (rare) are in bold so that we may quickly assess them.\n\nAttached is a long paper. My objective is for us to highlight issues with flow, clarity, and consistency, and for you to function as a highly attentive copy editor.\n\nTo begin, please provide a functional decomposition of these tasks and give me a readback of the headers of the document.",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:03:47.257858Z",
      "updated_at": "2025-04-15T06:03:47.257858Z",
      "attachments": [
        {
          "file_name": "main.tex",
          "file_size": 61248,
          "file_type": "",
          "extracted_content": "\\documentclass[]{interact}\n\\usepackage{fix-cm}\n\n\\usepackage{setspace}\n% \\doublespacing\n\\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.\n\\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables\n%\\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required\n%\\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required\n%\\setlength\\parindent{24pt}% To increase paragraph indentation when line spacing is doubled\n\\setlength{\\parskip}{\\baselineskip}\n\n\\usepackage[english]{babel}\n\\usepackage{csquotes}\n\\usepackage[style=apa]{biblatex}\n\\addbibresource{references-biblatex.bib}\n% \\usepackage[longnamesfirst,sort]{natbib}% Citation support using natbib.sty\n% \\bibpunct[, ]{(}{)}{;}{a}{,}{,}% Citation support using natbib.sty\n% \\renewcommand\\bibfont{\\fontsize{10}{12}\\selectfont}% To set the list of references in 10 point font using natbib.sty\n\n\n\n% \\documentclass[sn-vancouver, pdflatex, referee]{sn-jnl}% Style for submissions to Nature Portfolio journals\n\n%%%% Standard Packages\n%%<additional latex packages if required can be included here>\n% \\usepackage{natbib}\n\\usepackage{graphicx}%\n\\usepackage{url}\n% \\usepackage{multirow}%\n% \\usepackage{amsmath,amssymb,amsfonts}%\n% \\usepackage{amsthm}%\n% \\usepackage{mathrsfs}%\n% \\usepackage[title]{appendix}%\n% \\usepackage{xcolor}%\n% \\usepackage{textcomp}%\n% \\usepackage{manyfoot}%\n% \\usepackage{booktabs}%\n% \\usepackage{algorithm}%\n% \\usepackage{algorithmicx}%\n% \\usepackage{algpseudocode}%\n% \\usepackage{listings}%\n% \\usepackage{tabularx} % in the preamble\n\n\\usepackage{microtype}\n\n\\usepackage{ragged2e}\n\\RaggedRight\n\\usepackage[none]{hyphenat}\n\n\n% % Use double line spacing\n% % \\usepackage{setspace}\n\n% \\usepackage[running]{lineno}\n\n% \\usepackage{enumitem}\n\n\n%%%%\n\n%%%%%=============================================================================%%%%\n%%%%  Remarks: This template is provided to aid authors with the preparation\n%%%%  of original research articles intended for submission to journals published \n%%%%  by Springer Nature. The guidance has been prepared in partnership with \n%%%%  production teams to conform to Springer Nature technical requirements. \n%%%%  Editorial and presentation requirements differ among journal portfolios and \n%%%%  research disciplines. You may find sections in this template are irrelevant \n%%%%  to your work and are empowered to omit any such section if allowed by the \n%%%%  journal you intend to submit to. The submission guidelines and policies \n%%%%  of the journal take precedence. A detailed User Manual is available in the \n%%%%  template package for technical guidance.\n%%%%%=============================================================================%%%%\n\n%\\jyear{2021}%\n\n%% as per the requirement new theorem styles can be included as shown below\n\n% \\raggedbottom\n%%\\unnumbered% uncomment this for unnumbered level heads\n\n\\begin{document}\n\n\\title{A University Framework for the Responsible use of Generative AI in Research}\n\n% \\author[1]{\\fnm{Shannon Michelle} \\sur{Smith}}%\\email{shannon.smith@mq.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[2]{\\fnm{Melissa} \\sur{Tate}}%\\email{m1.tate@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[3]{\\fnm{Keri} \\sur{Freeman}}%\\email{keri.freeman@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[2]{\\fnm{Anne} \\sur{Walsh}}%\\email{anne.walsh@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author*[4]{\\fnm{Brian} \\sur{Ballsun-Stanton}}\\email{brian.ballsun-stanton@mq.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors  }\n\n% \\author[2]{\\fnm{Mark} \\sur{Hooper}}%\\email{m5.hooper@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[3]{\\fnm{Murray} \\sur{Lane}}%\\email{mc.lane@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n\n\n% \\affil[1]{\\orgdiv{Research Services}, \\orgname{Macquarie University}, \\orgaddress{\\street{16 Wally's Walk}, \\city{North Ryde}, \\postcode{2109}, \\state{New South Wales}, \\country{Australia}}}\n\n% \\affil[2]{\\orgname{Queensland University of Technology}, \\orgdiv{Office of Research Ethics and Integrity}, \\orgaddress{\\street{2 George St}, \\city{Brisbane}, \\postcode{4000}, \\state{Queensland}, \\country{Australia}}}\n\n% \\affil[3]{\\orgname{Queensland University of Technology}, \\orgdiv{Graduate Research Education and Development}, \\orgaddress{\\street{2 George St}, \\city{Brisbane}, \\postcode{4000}, \\state{Queensland}, \\country{Australia}}}\n\n% \\affil[4]{\\orgdiv{Faculty of Arts}, \\orgname{Macquarie University}, \\orgaddress{\\street{25 Wally's Walk}, \\city{North Ryde}, \\postcode{2109}, \\state{New South Wales}, \\country{Australia}}}\n\n\n% \\author{\n% \\name{Shannon Michelle Smith\\textsuperscript{a}, \n% Melissa Tate\\textsuperscript{b}, \n% Keri Freeman\\textsuperscript{c}, \n% Anne Walsh\\textsuperscript{b}, \n% Brian Ballsun-Stanton\\textsuperscript{d}\\thanks{CONTACT Brian Ballsun-Stanton. Email: brian.ballsun-stanton@mq.edu.au}, and \n% Murray Lane\\textsuperscript{c}}\n% \\affil{\\textsuperscript{a}Research Services, Macquarie University, North Ryde, New South Wales, Australia; \n% \\textsuperscript{b}Office of Research Ethics and Integrity, Queensland University of Technology, Brisbane, Queensland, Australia; \n% \\textsuperscript{c}Graduate Research Education and Development, Queensland University of Technology, Brisbane, Queensland, Australia; \n% \\textsuperscript{d}Faculty of Arts, Macquarie University, North Ryde, New South Wales, Australia}\n% }\n\n\n\\maketitle\n\n\n%%==================================%%\n%% sample for unstructured abstract %%\n%%==================================%%\n\n\\begin{abstract}\n    AI poses both opportunities and risks for the integrity of research. Universities must guide researchers in using AI responsibly and in navigating a complex regulatory landscape subject to rapid change. Drawing on the consultative experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of AI. We provide guidance to help distil the diverse regulatory environment into a principles-based position statement through the core values of honesty, transparency and accountability. Further, we explain how a position statement can serve as a foundation for initiatives in training, communications, infrastructure and process change. This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework to do so.\n\\end{abstract} \n\n\\begin{keywords}AI, ethics framework, research integrity, university policy, research institution\n\\end{keywords}\n\n\n% \\maketitle\n\n\\newpage\n\n\n\\section*{Introduction}\\label{sec1}\n\nGenerative Artificial Intelligence (AI) poses both opportunities and risks for the integrity and quality of research. To support researchers in using AI responsibly, institutions must adapt their research governance and management practices to mitigate the risks of AI while remaining flexible to allow for its innovative use in research. This is a significant challenge due to the uncertainty of how AI will ultimately impact the research sector, which is driven by a complex research regulatory environment, rapid innovation, and differing norms between research disciplines, universities and countries.\n\nThis paper draws on insights from two Australian universities to provide a strategic framework for navigating this evolving space and promoting the responsible use of AI in research. We provide worked examples of how Macquarie University (Macquarie) and Queensland University of Technology (QUT) have followed this framework to develop a principles-based position statement and guide the delivery of communications, infrastructure, process improvements and training to support researchers in the responsible use of AI. \nOur framework, based on current literature and practical implementation plans, serves as a foundational and adaptable model offering clarity and direction for research institutions.\n\nWhile most recent studies have focused on the impact of AI on undergraduate teaching and learning and assessment  \\parencite{sullivan_chatgpt_2023,crompton_artificial_2023}, discussions of institutional responses to the responsible use of AI for \\textit{research} are much less common \\parencite{popenici_critique_2023}. Universities are uniquely placed to instigate proactive measures to mitigate serious risks posed by AI, such as creating fake or unreproducible research \\parencite{gibney_could_2022, liverpool_ai_2023}, which threaten research credibility and public trust. Our paper emphasises developing an institutional position statement that applies research integrity principles to AI challenges, serving as \\textit{common ground} for mobilising training, communication and infrastructure initiatives to meet specific institutional and research needs.\n\nThe aim of this paper is to provide a structure to develop guidance in this evolving context. The literature and discussion in this paper should help inform institutional decision-making related to governance and policy implementation. This paper is a case study, allowing readers to apply our lessons learned and models in the development of their own institutional policies, using the framework we extrapolated from our efforts.\nWe offer this strategic framework to allow institutions to identify critical issues and practices that they and relevant stakeholders must address. In doing so, institutions can guide appropriate and effective responses to opportunities and challenges related to the responsible use of AI in research. \n\nOur framework is structured in four layers that represent the order in which the elements should generally be considered and implemented. Figure \\ref{fig:framework} provides a diagrammatic representation of our framework. \n\n\\begin{enumerate}\n\\item\n  \\textbf{Context} describes the external and internal policy environment that governs research integrity, research conduct and research student supervision and assessment.\n\\item\n  \\textbf{Development} of a university position includes the position statement and describes worked examples by our two universities on how we consulted across disciplines and engaged AI researchers.\n\\item\n  \\textbf{Implementation} describes a plan to put the position statement into practice with appropriate support, processes and infrastructure.\n\\item\n  \\textbf{Review} describes a plan to iteratively evaluate the framework to test its effectiveness and undertake revisions or updates to ensure currency.\n\\end{enumerate}\n\n\n\n\\begin{figure}[ht]\n\\centering\n% \\includegraphics[width=\\textwidth]{240415_Framework_A4.pdf}\n\\caption{A framework for institutions to support the responsible use of AI in research (available here under a Creative Commons CC-BY 4.0 Licence: \\url{https://osf.io/9b5an/}). Image by Mark Hooper (mark@trickygoose.training). }\\label{fig:framework}\n\\end{figure}\n\n\nThe structure of this paper reflects the four levels of the framework, which are numbered accordingly from one to four: context, position, implementation and review. We begin by explaining the institutional and external context for the responsible use of AI in research. This section essentially forms a literature review underpinning our topic. We then outline the processes involved in developing a position statement or guidance note which can be viewed as a methodology. The primary result of our study is the framework itself. However, our descriptions of the framework's implementation and review can also be viewed as forms of results. Lastly, discussions around the various issues raised by the development of the framework are included throughout the paper. We took this unconventional approach to the structure of the paper for conceptual legibility to guide the reader through each concept in the framework chronologically.\n\nIt is worth noting that Macquarie University called its document a Guidance Note for two reasons. First, pragmatically, a Guidance Note, did not require full senate approval for updates, allowing for the note to be a responsive living document. Second, as a document which exists only in an advisory capacity, the format of the Macquarie Guidance Note was significantly more pedagogical, containing many footnotes with links to resources and explanations that would be inappropriate in a more formal senate-confirmed Procedure or Policy. Each university governance structure is different, but having the ability to respond to models that update at a frequency of a few months is a highly desirable component of this framework.\n\nIn this paper, the term, \\textit{framework}, refers to the overarching structure guiding institutional governance, strategic direction, and adaptation to AI-related developments. \\textit{Policy}, on the other hand, refers to specific institutional rules and formal guidelines that establish compliance requirements for researchers around their use of AI.\n\n\\section{Context}\\label{sec3}\n\nPolicy development by research institutions is shaped by external and internal contextual environments, and in the case of AI, the relationships between these environments are complex: the increasing use of AI is creating new ways of conducting research; introducing new techniques, methods and complex datasets, and challenges around authorship, integrity and judgement. This poses challenges to accepted research practices and requires institutions to reassess institutional policy and infrastructure to support researchers and university staff in research integrity, human research ethics, research degree requirement, data management, sector policies and terms of service. \n\n\n\\subsection{Research integrity}\\label{research-ethics-and-integrity} \n\nIn Australia, the NHMRC \\textit{Code for the Responsible Conduct of Research} \\parencite{nhmrc_australian_2018} establishes principles that, extended, also guide responsible AI use in research, notably rigour, transparency, and accountability. Researchers must document and transparently disclose AI integration into their research, detailing AI tools, methodologies, and potential biases \\parencite[Principles 2, 3]{nhmrc_australian_2018}. Comprehensive records, including logs of AI interactions, prompts, and post-processing methods, should be securely maintained and made accessible to promote reproducibility.\n\nDisclosure practices vary widely across disciplines and institutions. Many journals currently require minimal disclosure following COPE guidelines \\parencite{committee_on_publication_ethics_cope_2024}. However, some have started mandating detailed disclosures. For instance, APA requires authors to explicitly document AI use in their methods sections and to provide full AI outputs as supplemental materials \\parencite*{american_psychological_association_apa_2023}. Practical disciplinary trends include attaching AI-generated logs and prompts as supplemental material \\parencite[e.g.][]{butson_ai_2024, shiferaw_spatial_2024}.\n\n\\subsubsection{Acknowledgement, disclosure, and documentation}\n\nAcknowledging AI use separately from general acknowledgements, such as under its own heading akin to funding or conflicts of interest, can clarify the extent and role of AI contributions. Simply acknowledging AI use is insufficient for rigorous peer assessment; detailed disclosure, proportional to the AI involvement, should include specific prompts, model versions, and outputs \\parencite*{ballsun-stanton_briefing_2024}. While no consistent standard exists yet across journals or disciplines, Macquarie and QUT have initiated comprehensive requirements for postgraduate students. Macquarie mandates documentation of AI use in each student's thesis, including interactions and supervisory approval, while QUT requires detailed written disclosure of AI use within theses \\parencite{queensland_university_of_technology_ai_2025}.\n\nHowever, pragmatic concerns about the practicality of these standards highlight implementation challenges. The required `active supervision and written consent' may be burdensome; thus, institutions must balance rigorous expectations with realistic supervisory practices. Both Macquarie and QUT acknowledge this tension, placing accountability largely on supervisors while recognizing a need for clearer norms and ongoing education. \n\nTherefore, effective, ethical, and responsible AI integration in research necessitates explicit institutional guardrails and ongoing training aligned with evolving ethical standards. Institutions should support researchers in applying FAIR principles \\parencite[see][]{wilkinson_fair_2016}, making AI-related data and methods transparent to strengthen research integrity and reproducibility. Rather than banning generative AI\u2014which risks unintended shadow use, institutions should proactively equip researchers with the skills to critically evaluate and transparently disclose AI involvement, addressing ethical challenges while leveraging AI\u2019s potential for scholarly innovation \\parencite{sadasivan_can_2023, ben_jabeur_artificial_2023}. An adaptable disclosure checklist supporting discipline-specific guidelines is provided in Appendix 1 for practical guidance across diverse research contexts.\n\n\n\\subsection{Human research ethics}\\label{human-research}\n\nResearch involving human participants is governed by region-specific\nlaws, regulations and guidelines. Human research ethics is guided by\nthe NHMRC code, which outlines the ethical aspects of human research\ndesign, review and conduct. When AI is used in research with\nhuman participation or data, ethical implications require close\nconsideration by Human Research Ethics Committees (HRECs), also known as\nInstitutional Review Boards \\parencite[][]{zhou_ai_2023, herington_ethical_2023}.\nAustralian universities must have frameworks and training to help researchers apply\nthe principles of the NHMRC code to AI use and equip HRECs with\nthe necessary knowledge for project review.\n\nRelevant ethical issues that fall under the scope of the NHMRC code\nrelate to bias and committee membership. While AI may not be\ninherently biased, the data upon which they are trained and the\ninterpretation of their results can lead to bias. For example, \\textcite{obermeyer_dissecting_2019} revealed bias in an AI tool used to identify\npatients for follow-up care that reduced the number of black patients\nidentified for additional care by more than half. The images generated by\nAI could also promote bias due to their training data, system\nprompts, or responses to previous scandals \\parencite[see][]{robertson_google_2024, yue_chatbodies_2024}. On the other hand, \\textcite{salah_may_2023} suggest\nthat AI could potentially help avoid human bias such as in\ncoding and categorisation of qualitative studies. Another issue in\naccord with the NHMRC code involves technical expertise on HRECs.\nCurrent committee membership is diverse, but may not necessarily include\nthe expertise to ethically evaluate research with AI.\nResearch institutions may therefore need to reconsider the core\nmembership of their HRECs and animal ethics committees to ensure\nsufficient knowledge and experience is available. Moreover, these technologies are not like traditional IT services. They require specialised research support around prompting, ethical issues, methodological implications and quality assurance.\n\n\\subsection{Research degree requirements}\\label{research-degree-requirements}\n\n\nStandards for postgraduate degrees are set by regulatory bodies external to the university. In Australia, the standard for doctoral degrees requires researchers' full responsibility and accountability for personal\noutputs, regardless of source and sufficient cognitive skills for\nintellectual independence and critical thinking\\parencite{australian_qualifications_framework_council_australian_2013}. Given that some scholars question the use of AI as infringing independent thought, there are additional questions needed to ensure the reputation of postgraduate work in this instance, as norms shift \\parencite{butson_ai_2024}. Therefore, implementation of\npostgraduate degree policy at an institutional level requires more\ndetailed guidelines than those offered by external regulatory bodies,\nand university protocols on the ethical use of AI may be one\ncomponent of these guidelines. This can be challenging, since the novel\nposition of postgraduate students, acting as researchers in training,\ncan sometimes be unclear. On the one hand, their work must be subject to\nassessment and degree standards, but as researchers, they can be treated\nas academics and even equals \\parencite{tanggaard_survival_2017}. If research staff are taking advantage of cutting-edge opportunities using\nAI, then it is natural for an institution to encourage\npostgraduate students to do the same. This duality of identity presents\na unique challenge for internal institutional policies and requires\nuniversities to take action to adequately support research students'\nunderstanding of their ethical responsibilities when using AI.\n\nMoreover, postgraduate student use represents two novel failure modes which should be accounted for. The first is over-reliance on AI without the student engaging in \\textit{any} critical thinking, where `the AI did my thesis for me' is the worst outcome. The second is the opposite problem: extremely problematic first drafts and under-reliance on AI for editing and polishing. In recognition of these challenges, both universities placed the responsibility for monitoring appropriate AI use onto supervisors, as this aligns with existing research training responsibilities. The Macquarie Guidance note (Section 2.3) splits this Gordian knot by requiring the `active supervision and written consent' of the supervisor, allowing the supervisor to actively and narrowly govern the AI use of their student. QUT requires that postgraduate students `discuss the use of any AI in writing your thesis with your principal supervisor prior to use'(Internal QUT HDR guidance 2025).\n\nChanges to postgraduate student research processes and assessment are a\nlikely consequence of increasing use of AI across\nuniversities. Students may use \nAI for planning research design, conducting research and assisting in\nthe preparation of research outputs and presentations, all of which\ncould require revisions to existing training and a clearer understanding\nof the responsibilities. Likewise, university processes around thesis\nassessment must also undergo review. Any effective policy for assessing\npostgraduate students' work must be enforceable, and therefore universities\nmust address the feasibility of detection \\parencite{lodge_assessment_2023}. However,\nquantitative measures of AI use seem currently\nunreliable \\parencite[see][]{liang_gpt_2023, openai_how_2023, sadasivan_can_2023}. One possible response to this challenge should be for a greater\nreliance on in-person, verbal defences of student and research work,\nwhich are currently uncommon in Australia \\parencite{bending_safeguarding_2023, pearce_rethinking_2023}. As \\textcite{vinge_rainbows_2006} predicted, `{[}T{]}he beginning of\ntrust has to be an in-person contact.' For universities to acknowledge\nstudent capabilities, in-person milestones are needed to help validate a\nstudent's intrinsic knowledge and subject mastery outside the context of\nany tool-assisted capabilities.\n\n\\subsection{Research data management}\\label{research-data-management}\n\n\nAI platforms process user data offshore, including\npotentially sensitive research data, which serves as another risk in the\nuse of AI. Depending on the security supporting the\nAI platform, data may be at risk of privacy or copyright\nbreaches due to user error or cyber threats. Furthermore, a user's\nprompts may compromise ownership of subsequent innovations, as the legal\nand moral aspects of copyright related to AI are presently\nunclear \\parencite{wipo_ip_and_frontier_technologies_division_generative_2024}. According to the\nNHMRC code, institutions must provide supporting infrastructure for\nresearch data, including guidance and facilities, to protect data\nownership, security and confidentiality \\parencite[Section 2]{nhmrc_management_2019}.\nInstitutions must therefore exercise prudence when employing\nAI to support research data, particularly where the AI\nplatform may not apply adequate data privacy, security and Intellectual\nProperty (IP) standards. Universities choosing what AI to use\nmust trade off between model capability, speed of response, building\nversus buying and internal capabilities when trying to thread the IP\nneedle.\n\nCopyright is another area of concern in the context of research data\nmanagement. The legal aspects of copyright related to the outputs of\nAI in Australia have yet to be updated to address the use of\nAI and are inadequately regulated \\parencite[see][]{gaffar_copyright_2024, samuelson_generative_2023}. It is\nnot currently known whether works created by AI platforms\nwill be protected given that the nature of copyright in Australia relies\non human authorship. Changing copyright standards for AI involvement may cause a published \nwork to not meet the human author threshold, and thus may not be eligible for protection under the current law. Research data management is\nanother example of the need for researchers to be supported while\nspecific regulations addressing the use of AI are resolved.\n\n\n\\subsection{Sector policies}\\label{sector-policies}\n\nEnsuring accountability in research practices necessitates robust sector\npolicies for academic publishers, research funding and institutional\nstrategies to manage the impact of AI. Academic publishers,\nparticularly those adhering to the Committee on Publication Ethics\n(COPE) guidelines, emphasise the importance of ethical standards in\npublication \\citeyear{committee_on_publication_ethics_cope_2024}. They generally agree that AI cannot be\nheld responsible as authors; it can be used for subsidiary research\npublishing tasks \\parencite{nature_portfolio_artificial_2023}, provided acknowledgement is\nincluded. Given the rapidly evolving landscape of AI,\npublishers and institutions must be careful not to harm authors\nwith false claims of inappropriate use of AI \\parencite[see][]{wolkovich_obviously_2024}. This acknowledgement-permissive mode ensures that content\nresponsibility always remains with human authors.\n\n\nLarge research funding bodies external to universities may also\ninfluence institutional research policies and practices and may override\nuniversity policies for responsible use of research funds. Mitigating\nrisks is a primary priority, exemplified by the European Union's strict\nlegislation on transparency, bias, privacy and copyright principles \n\\parencite{gibney_what_2024}. In Australia, several funding bodies have implemented\npolicies on the use of AI in peer review grant\napplications \\parencite{australian_research_council_research_policy_branch_policy_2023, national_institute_for_health_use_2023}. The Australian Research Council\nacknowledges that while AI presents opportunities for\nresearch and grant proposal writing, the responsibility for errors falls\non researchers and their institutions. This stance,\nalthough not new, emphasises the need for thorough due diligence and\nappropriate training and guidance due to the complexity and uncertainty\nintroduced by AI.\n\n\nGrant-makers' concerns include the ability of AI to make\neffective judgments, the potential sharing of confidential\ninformation and protected intellectual property if documents are\nuploaded to publicly available AI services. Although these issues might be\nresolved, human oversight remains crucial to maintaining\nconfidence in the peer review process and the responsible allocation of\nresearch funds. Thereby underscoring the principle of accountability in\ninstitutional AI position statements.\n\nAnother aspect of sector policy impacts on AI research frameworks involves institutional strategies and\noperational plans. A 2024 Australian government review of higher\neducation quality, accessibility and sustainability highlighted the\npotential of AI to boost research productivity, urging\nuniversities to lead in building new research infrastructure \\parencite{australian_universities_accord_australian_2024}. AI may contribute to digital\ntransformation efforts, similar to the push for paperless offices in the\n1990s \\parencite{meyendorf_digitization_2021}, and universities might deploy AI to reduce\nstaff loads, an efficiency expected to extend to research integrity and\nethics committees. However, institutions must carefully assess the\nimportance of human judgment in all tasks, especially in research\nsupport and governance roles. The rapid evolution of AI also\ncomplicates the enterprise software purchasing process, as products may\nchange significantly before they are fully implemented. Therefore,\nproactive strategies can ensure that institutional frameworks\nincorporate responsible research conduct principles and provide adequate\nsupport for researchers amidst the evolving AI landscape.\n\n\n\\subsection{Terms of service}\n\n\nInstitutions should regularly monitor the terms of service of\nAI platforms to maintain control over research data and\ncomply with applicable privacy legislation. Typically, an institution\ncan negotiate specific terms of service when\npurchasing enterprise research software and can\nreasonably expect that it will remain the same throughout the course of\nthe agreement. However, the current rate of change means that\nAI platforms release frequent terms of service updates which presents a significant legal risk if subsequent updates changeintellectual property or privacy provisions that compromise security and ownership.\n\nAI platforms also mandate service use disclosure that puts the onus of\nresponsibility for any text generated by AI onto the human\nauthor \\parencite{openai_sharing_2022} and specific agreements must inform university policies. Therefore,\nposition statements should clearly allocate the responsibilities of\nmonitoring AI company terms and to\nmaintain a list of services, models and modes of interaction that are\nrisk-acceptable and privacy-acceptable to the university.  While an institution cannot forbid\nan entire technology category, it can point to specific ways\nof using services and guide researchers to more effective and less risky\noptions. The continually changing technology makes maintaining these position statements an ongoing task.\n\n\n\\section{Developing a University Position}\n\nConsidering the diverse contextual factors relevant to the use of\nAI in research, an institutional position statement can serve\nthe important function of distilling and applying that context in one\nplace. In general, position statements provide a foundation for the\ndevelopment of institutional policies, guidelines and training in a\nwhole of enterprise strategy. Applied to AI, they enable\ninstitutions to take a proactive stance to promote the responsible use\nof technology to foster research quality and innovation, while\nprohibiting practices inconsistent with research integrity. \n\nDeveloping a policy (controlling document), position statement (preparatory document), or guidance note (advisory document) --- as institutional politics and knowledge dictate --- informs research managers and research leaders, empowering them to contribute to institutional initiatives. By providing\na common ground within the institution, a clear position statement can\nreduce confusion caused by the saturation of literature and opinions,\nthe proliferation of new tools and the rapid pace of disruption. A guidance note can serve as a first introduction, allowing conversations to occur in the absence of formal and enforced policy.\n\nThe\nformat of a position statement can be simple, with clear declarative\nstatements about responsible use of AI, forming the basis for\nmore complex considerations and scholarly debates on nuanced issues. The format of a guidance note requires more explanation and justification of decisions, in an attempt to persuade and inform, with enforcement power coming from extant policy.\n\nThis section of the paper describes the processes involved in the\ndevelopment of a university document on AI (we will say Position Statement here on out), serving as both a\nmethodological outlining and a case study of how our two institutions developed their own position statements.\n\n\n\\subsection{Consult across disciplines and research services}\n\nDeveloping a position statement requires broad consultation across\nresearch disciplines and research support services to capture the\ndiverse ways of using AI. Development requires targeted\nconsultation with academics who have expertise in research ethics and\nresponsible research practices. This consultation could take multiple\nforms. We describe the process here for two universities: QUT and\nMacquarie.\n\nQUT began\nconsultation in January 2023 by drafting a discussion paper:\n\\emph{Emerging AI tools and challenges for research integrity}, which\nwas then circulated broadly to provoke discussion and feedback. This\ndiscussion paper recognised the potential opportunities of emerging\nAI in research. It outlined the inherent risks AI\ncould pose for responsible research practices, such as authorship,\noriginality of publications and research outputs. It provided a\nbalanced overview of the use of AI in research to help focus\nthe consultation on the key issue of how AI intersects with\nresearch integrity. We solicited feedback from across the university, including from Research Integrity and Ethics Advisers.\n\n\\textbf{Macquarie University} established a committee of\nstakeholders and consulted experts from each faculty and professional\nservices. They considered the risks and opportunities posed by\nAI to research integrity, processes and operations. The\ncommittee was tasked with considering whether a new policy or targeted\npolicy updates were required or whether changes to processes were\ndeemed appropriate. Anticipating the fast pace of change and the\nforecast need for regular updates, a Guidance Note outlining the\nUniversity's position and recommendations regarding the use of\nAI in research was drafted. To create the Guidance Note, the\ncommittee evaluated the current literature and engaged in cross-faculty\ndiscussions. Key aspects of the Guidance Note included identification of\nkey resources to provide an initial foundation for researchers,\npostgraduates and support staff and a summary of potential risks and\njustifications for the intended approach. The Guidance Note was\nsubmitted to the relevant committees for comment or endorsement. This\nstep was considered critical to allow diverse areas of the University\nan opportunity to build understanding and consider implications for\ntheir areas or alignment with other projects or initiatives.\n\nFor both cases, consultation was coordinated by each\nuniversity's Office of Research Ethics and Integrity (OREI). The\nconsultation process provided the opportunity for OREI at both\nuniversities to engage with their research communities to understand the\nvaried uses of AI in research and the potential impacts of\nAI on responsible research practices. Our examples illustrate\nthat while the approach to developing a university position can vary\naccording to each institution's needs and internal structure,\ncoordinated consultation, discussion and feedback across university\nresearch communities is crucial to ensure all stakeholders are provided\nthe opportunity to participate in the process.\n\nIn 2023, when both universities were first writing their documents, no informed student voice was available for formal consultation. During development at Macquarie, in the three months of development of our Guidance Note, the research working group was instructed to consult with their HDR students and appropriate staff in their respective faculties. QUT also asked staff to reach out to well-informed students.\n\nOne crucial aspect of the development of our frameworks was involving researchers and practitioners with actual, thorough, use of the tools when developing our advice. In 2023, this pool was quite narrow, leading to most consultation through faculty experts and those people tasked with investigating these services. In 2025, a broader and more consultative approach is warranted.\n\n\\subsection{Engage AI Researchers}\n\nApart from the formal consultation process across research disciplines\nand services, it is vital to recognise that some researchers have long been experts in this space and already engage both\ninternally and publicly with emerging opportunities and challenges.\nWhere possible, these researchers should be invited to participate in the consultation process to incorporate their input and gain their endorsement. According to the Technology Acceptance Model \\parencite{shahzad_factors_2024}, the advocacy for institutional policy by trusted academics has generally been shown to be a successful strategy for the adoption of technology by staff and students \\parencite{faden_importance_2002}. Engaging AI researchers within the university forms a vital part of an institution's strategic framework in shaping policies related to research integrity and providing essential AI literacy training. As \\textcite{butson_ai_2024} have highlighted, it is also essential that institutions consider contrasting and diverse perspectives related to the integration of AI in research tasks, because `this is not merely a technological transition but also a significant ethical and methodological turning point' (p. 567). Combined broad and specific consultation ensures that institutions not only address the challenges but also explore the opportunities AI provides. Consequently, we consulted innovators and early adopters in our universities who had already published their research \\parencite[e.g.][]{bell_rapid_2023, doherty_upskilling_2024, kowalkiewicz_economy_2024, mcdonald_apostles_2024}.\n\n\n\n\\subsection{Position Statement}\n\nA strong position statement should address key research integrity\nprinciples, especially honesty, transparency and accountability, and\nreinforce the importance of these principles in the responsible use of\nAI in research practices. Our university position statements\nhave been developed based on broad stakeholder feedback. They provide a\nconsolidated institutional position and clear guidance for diverse\nfaculties and research groups. We provide copies of our position\nstatements as Attachments 1 and 2, and make them available to adapt\nunder a Creative Commons 4.0 CC-BY Licence.\n\nBroadly, our two position statements have much in common:\n\\begin{itemize}\n\\item\n  They \\emph{support} other existing responsible conduct of research policies.\n\\item\n  They \\emph{encourage} the use of AI where it promotes responsible research quality and impact.\n\\item\n  They are primarily \\emph{principles-based} so that the readers can interpret their implementation within their own specific context.\n\\item\n  They have minimum prescribed rules, such as: \\begin{itemize}\n  \\item\n    An AI tool must not be listed as an author, as this violates the principle of accountability for research outputs.\n  \\item\n    Explanations on how AI was used should be included in the\n    Methods and/or Acknowledgements sections of a research output to be\n    consistent with the policies of publishers and the principle of\n    transparency.\n  \\end{itemize}\n\\end{itemize}\n\nThere are some notable differences between the two statements in terms\nof style and delivery. While QUT named its document a `position\nstatement' to emphasise that it provides definitive university-level\nadvice, Macquarie University named its statement a `Guidance Note',\nemphasising its role in highlighting extant policies and indicating how they apply to the responsible conduct of research. Both the guidance note and the position statement proved politically useful for the subsequent development of AI policy in 2025, as well as texts to align university thinking around consistent norms of the responsible use of AI in research.\n\n\n\\section{Implementation}\n\nA position statement serves as a foundational guide, articulating the university's principles and position on the responsible use of AI in research. However, it is not a policy itself, but rather informs the development of specific institutional policies and procedures that establish enforceable guidelines. To ensure effective implementation, these policies must be embedded within a broader strategic framework. This strategic framework then supports the development of a governance structure that guides institutional responses and adaptations over time. The framework should encompass initiatives in training and education, procedural updates, equity in access across staff and students, impacts on infrastructure and a communication and engagement strategy.\n\nThis section of the paper outlines each of these key areas and reflects\nthe third layer of our framework after Context and Development.\n\n\\subsection{Communication and engagement strategy} \n\n\nRapid AI developments and new AI services can overwhelm researchers tasked with staying informed. To support informed decision-making, any position statement must include a well-designed communication and engagement strategy.\n\nIntroducing a new policy naturally requires clear communication. However, policy alone rarely changes culture; thus, it must be accompanied by active \\textit{engagement} initiatives. QUT's position statement `recognises that the use of, and implications of using, AI tools may differ widely by discipline,' encouraging ongoing scholarly debate and research. The unique risks and benefits of AI particularly necessitate proactive change management \\parencite{singh_22nd_2023,kotter_leading_1995}.\n\nRetraction Watch maintains a list of papers flagged as `ChatGPT Writing,' indicating low-effort or uncritical AI usage \\parencite{marcus_papers_2024}. Such publications can negatively impact researchers and institutions. To mitigate these risks, institutions must clearly communicate expectations around responsible AI use. Classroom attempts to detect unauthorised AI use have sometimes failed dramatically, typically due to inadequate guidance about AI's capabilities and limitations \\parencite{klee_professor_2023}.\n\nAs AI capabilities, services and models continually emerge, institutions must not only communicate broader policy implications to staff and students, but also provide safe institutional access to mitigate privacy risks and promote equity of use. This communication should be a coordinated change management strategy that allows researchers to understand both the benefits and risks of AI, especially when available models change. For example, policy authors might visit department meetings to explain critical clauses and their technical or practical rationale, ensuring that most researchers are exposed to the new guidelines.\n\nBeyond announcements and informational sessions, a detailed communication plan can support further university-wide discussions on the nuances and implementation of AI. Encouraging and acting on feedback allows faculty and researchers to voice concerns, particularly those critical of emerging technologies. It is critical to foster a collaborative culture that focuses on policies related to responsible AI use and also capitalises on researchers' experiences of the benefits that using AI tools can provide \\parencite{balalle_reassessing_2025}.Through such collaboration and conversations, universities can refine norms accepted by various faculties and reach consensus on ethical AI use. Often, such norms can help mitigate institutional risk by clarifying which training resources and AI literacy initiatives are needed to support ongoing engagement. \n\n\n\\subsection{Training and education strategy}\n\n\nTraining and education are fundamental and ongoing elements of any\ninstitution's response to ensuring its researchers fully understand\ntheir ethical responsibilities around the use of AI in their\nresearch practices. In Australia, institutions have a responsibility to\n`\\emph{Provide ongoing training and education that promotes and supports\nresponsible research conduct for all researchers}' \\parencite{nhmrc_australian_2018}.\nMoreover, AI literacy is fast becoming an essential part of research\nprocesses and a valuable industry skill \\parencite[see][]{van_noorden_ai_2023, hutson_hypotheses_2023, prillaman_is_2024}. As reported by \\textcite[3]{thompson_ai_2023}, `there is still sensemaking work to be done, as misconceptions about what AI is and how it works remain common.' \nTherefore, there is an urgency for institutions to provide education that\nequips research graduates for their careers and empowers academic staff\nto pursue industry research partnerships.\n\n\nThe diversity of opportunities provided by AI heightens the\nimportance of AI literacy training and initiatives which emphasise and\nintegrate research integrity. However, supervisors are best positioned to promote the appropriate upsides of AI use. The developed position statement should, instead, focus on downside risks and issues. Some issues to consider, therefore, include data management\npolicies, authorship guidelines, copyright law and research\nreproducibility. AI literacy training must also focus on the development\nof skills to understand, identify and address the trustworthiness and\nreliability of AI. Researchers should be able to assess whether\nusing AI is appropriate for a specific challenge and therefore be able to defend their choice of AI selection or not. Necessary researcher skills include the ability to tackle\nchallenges such as privacy and the risk of disclosing IP, the potential\nfor bias and prejudice in the data that AI uses to formulate\nresponses and the ability to critically evaluate outputs. Training\nshould focus on explicit scaffolding and self-regulated learning that\nencourages critical engagement and agency in ethical decision-making\nwhen using AI \\parencite{markauskaite_rethinking_2022, mcknight_electric_2021, unesco_chatgpt_2023}.\n\nAt our two universities, initial researcher training on the responsible\nuse of AI has included workshops, online modules and panel\ndiscussions allowing postgraduate students and supervisors to ask\nquestions of experts with diverse specialities in AI research, research\nethics and student policies. Training has also emphasised the importance of effective and thoughtful prompting. Likewise, online modules challenge\nparticipants to make decisions for themselves on the appropriate use of AI, using the key principles of honesty, transparency,\naccountability and fairness, through scenarios and case studies. In each\nof these examples, rather than providing purely didactic instruction,\nparticipants are prompted to consider their own unique circumstance and\nmake decisions accordingly. As such, training strategies on \nAI usage must be regularly updated and adaptable to keep pace with the\nfrequent updates and advancements in the field.\n\n\\subsubsection{Acknowledgement and documentation of use by postgraduate students}\n\\label{subsection:ack}\n\nAs an example of training and education strategy, we may examine how both universities have set high expectations of their postgraduate students. These requirements have been written into guidance for postgraduate students and communicated by presentation and webpage to supervisors and students. \n\nMacquarie University has chosen to require documentation of use: \n\\begin{quote}\nGenerative AI, LLMs, and tools integrating these models will likely become part of the writing, research,\nand composition process for many researchers and students. However, Higher Degree Research (HDR) and\nHonours students must be responsible, from start to finish, for their research. ... At Macquarie University, it is recommended that \\textit{all} stages of a students\u2019 research journey be well documented\u2014both for data integrity purposes and to serve as evidence of originality and intellectual input if challenged.\n ... Supervisors must closely monitor the use of Generative AI by their research students. Generative AI should not be used without supervisory approval, and where these tools are used in any\ncapacity by research students or supervisors, they must be acknowledged' (Macquarie University Guidance Note, Section 2.3). \n\\end{quote} \n\nAt QUT, the position statement focuses on the university's commitment to research integrity. Additionally, research students are instructed to seek approval from their Principal Supervisor prior to AI use. Any use of AI, including text editing for readability and grammar, must be disclosed in the thesis acknowledgement and a detailed written description documenting the extent of AI use and how it has been used is also required  \\parencite{queensland_university_of_technology_ai_2025}. \n\nWe acknowledge that both universities could do more in this regard---as there is a tension between the high standards of our guidance and the lived realities of supervisors. The pragmatic reality of both of these institution's requirements is that of offloading the onus of responsibility of the student's use of AI onto the supervisor. The Macquarie Guidance Note chose to explicitly empower supervisors to make the guidelines specific \\textit{per student} as students and supervisors have different needs and expectations when it comes to writing and AI use. This phrasing is an acknowledgement that as researchers in training, the consequences of AI use will fall on the supervisor. The choice of terms of `active supervision' is to emphasise the supervisor's responsibility of this component of research training. The level of active attention given by the supervisor is up to the supervisor's discretion. \nAt Macquarie, Ballsun-Stanton has mediated supervisor and postgraduate student meetings setting out the allowable scope of AI use---while these meetings are the exception to the norm of benign neglect, some students and supervisors have been open to these discussions when educated about the guidance note. More work is necessary around norm setting, provision, and sufficient education of the appropriate uses of these technologies, however.\n\n\n\\subsection{Infrastructure and procedural considerations}\n\nUniversities face several common barriers when implementing AI policies: equity of access, budget constraints, maintaining integrity, and promoting appropriate use amid uneven distribution of expertise. For institutions with limited resources, the rapid evolution of AI creates additional challenges.\n\nTo address these barriers effectively while maximizing limited resources:\n\n\\begin{enumerate}\n\\item Make transparent disclosure mandatory through existing processes. Data management plans and ethics applications should require researchers to declare AI use, creating accountability without new administrative structures.\n\n\\item Prioritize cost-effective solutions. The price of AI inference is dropping rapidly, \"about 10x every 12 months\" \\parencite{altman_three_2025}, making thoughtful commercial AI adoption increasingly affordable compared to custom solutions that promise to reduce expertise requirements. Paying per token with a low daily budget allocated per student and researcher is \\textit{significantly} cheaper than paying a fixed fee per month per user, regardless of their use. By targeting a cost-efficient frontier model with a pay-per-token approach, universities pay only for what they use, and cap expenses of power-users.\n\n\\item Leverage established research norms. Existing integrity frameworks provide a foundation for \\textit{post hoc} judgment as technology norms evolve, requiring only targeted adaptations rather than complete policy overhauls.\n\n\\item Ensure auditability through service selection. Choose AI platforms that maintain researcher-accessible logs and comply with institutional research policies, avoiding costly compliance issues later.\n\\end{enumerate}\n\nMost universities already have secure research data infrastructure that can be leveraged for AI implementation. By communicating clear trade-offs between hosted solutions and institutional infrastructure, universities can help researchers make appropriate choices for their specific needs while controlling costs and maintaining security standards.\n\n\\section{Review}\n\nThe evaluation process is an important component in reviewing the design,\nimplementation and refinement of university policy and is therefore integrated\ninto the framework at all levels. At each stage of development, implementation and communication, feedback from stakeholders is needed to highlight mistakes, inform future policy development and respond to the changing AI landscape. The evaluation process, therefore, provides a review\nmechanism to inform stakeholders of policy effectiveness, \nensures the currency of the position statement and informs new directions and updates to the framework. Essentially, the evaluation phase of a\nstrategic framework involves asking stakeholders for feedback on\nprogress and adjusting the framework and/or implementation strategies\naccordingly. This is a cyclical process where implementation can have impacts on the institutional context, and therefore, feedback is essential to inform the next stage of policy development. \n\nStrategic planning, governance and policy development for the responsible use of AI will continue to evolve and continuous adaptation is essential to `harness its full potential while safeguarding academic integrity and promoting equitable access' \\parencite[79]{mcdonald_apostles_2024}. We suggest reviews should consistently examine gaps in\nresearch governance that will inevitably be created by ongoing research\ninnovation and digital disruption. Given the fast pace of AI change, it is inevitable that\nfuture changes will necessitate equivalent policy changes to maintain\nrelevance. For example, as AI developers and providers continue to address limitations and challenges related to model capabilities, inquiry and rigour to understand the challenges and benefits of AI updates to models and updates will remain fundamental and ongoing issues that are `central to academic endeavour and require rigorous scrutiny' by institutions and researchers \\parencite[566]{butson_ai_2024}. We also propose a more direct form of review; to regulalry survey\nstakeholders at regular intervals.\n\nTo evaluate effectiveness amid rapid AI evolution, institutions should track these trends: awareness and engagement, whether researchers understand policies; compliance trends, whether AI misuse is decreasing; and policy relevance, whether guidelines remain current. As \\textcite{chen_research_2024} note, effective mechanisms are required to `detect and prevent unethical behaviours in research, such as data fabrication, plagiarism, and improper use of AI technology.\u2019 Regular reviews should prioritize maintaining currency rather than reactive overhauls, tracking new model capabilities that impact different university stakeholders in uneven ways. \n\nFor example, after \\textcite{mollick_what_2025} described using AI to detect statistical errors in journal articles, Macquarie updated its Peer Review policy to permit reviewers to use AI tools that support their judgment, provided this doesn't violate confidentiality or commissioning guidelines. Similarly, QUT is implementing additional training based on stakeholder feedback. These cases demonstrate how institutions can adapt policies to reflect emerging scholarly practices through regular review. \n\nThe Macquarie Guidance note levies an additional onus on supervisors: `Supervisors should have iterative exposure to their students\u2019 work across their entire research degree and ensure their research progression (including critical judgment or originality) is observed and well documented. Supervisors must closely monitor the use of Generative AI by their research students' (Macquarie Guidance Note, section 2.3). Macquarie is tracking these conversations through checking compliance at Data Management Plan submission stage, along with \\textit{ad hoc} conversations with supervisors during graduate training. QUT is tracking the effectiveness of its training through training evaluation surveys.\n\n\\section*{Conclusion}\n\nAs universities grapple with the challenges and opportunities presented\nby AI, our paper introduces a strategic framework: which identifies context to investigate, stakeholders to consult in development, key implementation steps and review mechanisms to allow for iterated development of position statements and policies. The developed texts can then be used to promote\nresponsible and ethical practices in academic research. The evolving\nAI landscape necessitates comprehensive, flexible and\nuniversity-wide policies and strategies to ensure research integrity and\nsupport researchers in responsibly using AI.\n\nThe framework we present provides a structured approach with insights\ninto the elements institutions should consider for practical\nimplementation. Central to this process is developing a position\nstatement on AI, reinforcing key principles of research\nintegrity. This statement guides the operationalisation of training\nprograms, communication strategies and infrastructure initiatives. The\nframework outlines an iterative process starting with examining relevant\ncontextual factors, developing the position statement, implementing it,\nand recommends ongoing review and improvements. \n\nWhile other practitioners have produced frameworks for universities\ndealing with AI broadly \\parencite[for undergraduate policy, see][]{luo_critical_2024}, this paper specifically\naddresses the research policy context. The framework serves as a road map for\nresearch institutions to adapt, considering their unique contexts to\ndevelop appropriate strategies and plans. Examples from QUT and Macquarie\nillustrate potential outcomes of a strategic framework process, without\npresenting them as definitive end-points.\n\n\nTesting various strategies within this framework could inform future\nresearch related to identifying the most effective position statements\nand strategies to promote responsible practices and research integrity\nin AI use across institutions. Each organisation must drive\nits own AI agenda, but our framework can initiate and guide\nthat process.\n\n% \\nolinenumbers\n\n\n% \\backmatter\n\n%\\bmhead{List of Abbreviations}\n\n%\\begin{itemize}\n%    \\item[AI] Artificial Intelligence\n%    \\item[API] Application Programming Interface\n%    \\item[COPE] Committee on Publication Ethics\n%    \\item[Generative AI] Generative Artificial Intelligence\n%    \\item[HREC] Human Research Ethics Committee\n%    \\item[IP] Intellectual Property\n%    \\item[IRB] Institutional Review Boards\n%    \\item[KPI] Key Performance Indicators\n%    \\item[OREI] Office of Research Ethics and Integrity\n%    \\item[QUT] Queensland University of Technology\n%    \\item[TEQSA] Tertiary Education Quality Standards Agency\n    \n%\\end{itemize}\n\n% \\bmhead{Supplemental materials}\n\\section*{Supplemental materials}\n\nAttachment 1: Macquarie University's Position statement, available CC-BY at \\url{https://zenodo.org/records/10851623}\n\nAttachment 2: Queensland University of Technology's Position Statement, available CC-BY at\n\\url{https://zenodo.org/records/10971910}\n\nAppendix 1: Macquarie Peer Review Checklist <URL TODO>\n\nWorks Consulted: All papers consulted in preparing this manuscript are available on Zenodo: \\url{https://www.zotero.org/groups/5224649/2023qutmq-a_framework_for_the_responsible_use_of_generative_ai_tools_by_researchers/items}\n\n \n\n\n\\section*{Declarations}\n\n\n\\begin{itemize}\n\n\\item Ethics approval \n\nNot applicable.\n\n\n\\item Availability of data and materials\n\nAll prompts and full chat-logs of sessions with large language models used as part of editing this work are available at https://osf.io/8dxj6/\n\n\\item Competing interests \n\nThe authors have declared that they have no competing interests.\n\n\\item Authors' contributions\n\nAll authors contributed conceptualisation, writing, reviewing and editing.\n\n\n\\item Acknowledgements\n\nThe authors would like to acknowledge the significant contributions of Mark Hooper (mark@trickygoose.training) in developing the graphical representation of the framework and in assisting with earlier revisions of this manuscript. At his request, in this version of the manuscript, we have moved him from an author to acknowledged contributor. We warmly thank him for his time and support on this paper.\n\nSpecial thanks to the following early readers for their feedback and encouragement: Jane Thogersen, Stephanie Bradbury, Matt Bower, Ross McLennan, Sylvie Saab and Jacqueline Phillips. \n\n\\end{itemize}\n\n\\noindent\n% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. \n\n\\printbibliography\n\n% \\bibliography{references}% common bib file\n%% if required, the content of .bbl file can be included here once bbl is generated\n%%\\input sn-article.bbl\n\n\n\\end{document}\n"
        }
      ],
      "files": [
        {
          "file_name": "main.tex"
        }
      ]
    },
    {
      "uuid": "ffedeb72-8857-4203-a9b4-64d421a8492c",
      "text": " I understand you'd like me to serve as a meticulous copy-editor for your academic paper, focusing on highlighting issues rather than making direct corrections. My approach will be to carefully consider flow, clarity, and consistency throughout the document while suggesting ways to tighten the language.\n\n## Functional Decomposition of Tasks\n\n1. **Document Structure Analysis** - Identify the paper's organization by examining headers and flow\n2. **Consistency Check** - Evaluate voice, tense, terminology, and formatting for consistency\n3. **Clarity Assessment** - Identify unclear phrasing, complex sentences, or confusing passages\n4. **Flow Analysis** - Examine transitions between sections, paragraphs, and ideas\n5. **Language Efficiency** - Highlight opportunities for more concise expression\n6. **Academic Style Review** - Ensure adherence to academic writing conventions\n7. **Citation Checking** - Note any apparent issues with citations\n\n## Document Headers\n\nHere are the main sections of your document:\n\n- **Title**: A University Framework for the Responsible use of Generative AI in Research\n- **Abstract**\n- **Introduction** (Section 1)\n- **Context** (Section 3)\n  - Research integrity\n  - Human research ethics\n  - Research degree requirements\n  - Research data management\n  - Sector policies\n  - Terms of service\n- **Developing a University Position**\n  - Consult across disciplines and research services\n  - Engage AI Researchers\n  - Position Statement\n- **Implementation**\n  - Communication and engagement strategy\n  - Training and education strategy\n    - Acknowledgement and documentation of use by postgraduate students\n  - Infrastructure and procedural considerations\n- **Review**\n- **Conclusion**\n- **Supplemental materials**\n- **Declarations**\n\nI notice that there appears to be a numbering inconsistency - the Introduction is labeled as Section 1, but then we jump to Context as Section 3, with no Section 2 visible in the document. This is something I'll address in my review.\n\nWould you like me to proceed with a detailed analysis section by section, or is there a particular area of the paper you'd like me to focus on first?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:03:49.164706Z",
          "stop_timestamp": "2025-04-15T06:03:57.118254Z",
          "flags": null,
          "type": "text",
          "text": " I understand you'd like me to serve as a meticulous copy-editor for your academic paper, focusing on highlighting issues rather than making direct corrections. My approach will be to carefully consider flow, clarity, and consistency throughout the document while suggesting ways to tighten the language.\n\n## Functional Decomposition of Tasks\n\n1. **Document Structure Analysis** - Identify the paper's organization by examining headers and flow\n2. **Consistency Check** - Evaluate voice, tense, terminology, and formatting for consistency\n3. **Clarity Assessment** - Identify unclear phrasing, complex sentences, or confusing passages\n4. **Flow Analysis** - Examine transitions between sections, paragraphs, and ideas\n5. **Language Efficiency** - Highlight opportunities for more concise expression\n6. **Academic Style Review** - Ensure adherence to academic writing conventions\n7. **Citation Checking** - Note any apparent issues with citations\n\n## Document Headers\n\nHere are the main sections of your document:\n\n- **Title**: A University Framework for the Responsible use of Generative AI in Research\n- **Abstract**\n- **Introduction** (Section 1)\n- **Context** (Section 3)\n  - Research integrity\n  - Human research ethics\n  - Research degree requirements\n  - Research data management\n  - Sector policies\n  - Terms of service\n- **Developing a University Position**\n  - Consult across disciplines and research services\n  - Engage AI Researchers\n  - Position Statement\n- **Implementation**\n  - Communication and engagement strategy\n  - Training and education strategy\n    - Acknowledgement and documentation of use by postgraduate students\n  - Infrastructure and procedural considerations\n- **Review**\n- **Conclusion**\n- **Supplemental materials**\n- **Declarations**\n\nI notice that there appears to be a numbering inconsistency - the Introduction is labeled as Section 1, but then we jump to Context as Section 3, with no Section 2 visible in the document. This is something I'll address in my review.\n\nWould you like me to proceed with a detailed analysis section by section, or is there a particular area of the paper you'd like me to focus on first?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:03:58.683863Z",
      "updated_at": "2025-04-15T06:03:58.683863Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "c7fc430b-cd2a-496c-98df-9d15e31a2141",
      "text": "Hi Claude. Please take the role of a highly meticulous copy-editor. You will take *extra* time and be extremely thoughtful in your discussions and considerations. The user, above all else, requires thoughtful and reasoned editing and discussion. We need to ensure that everything is in a single, consistent, and clear register. We also need to remove some words, where possible.\n\nI would like you to highlight issues rather than making corrections. Then, for each issue, please thoughtfully discuss the issue. Make sure any suggested contributions (rare) are in bold so that we may quickly assess them.\n\nAttached is a long paper. My objective is for us to highlight issues with flow, clarity, and consistency, and for you to function as a highly attentive copy editor.\n\nTo begin, please provide a functional decomposition of these tasks and give me a readback of the headers of the document.\n\nDisregard the labels of section headers.",
      "content": [
        {
          "start_timestamp": "2025-04-15T05:54:13.273901Z",
          "stop_timestamp": "2025-04-15T05:54:13.273901Z",
          "flags": null,
          "type": "text",
          "text": "Hi Claude. Please take the role of a highly meticulous copy-editor. You will take *extra* time and be extremely thoughtful in your discussions and considerations. The user, above all else, requires thoughtful and reasoned editing and discussion. We need to ensure that everything is in a single, consistent, and clear register. We also need to remove some words, where possible.\n\nI would like you to highlight issues rather than making corrections. Then, for each issue, please thoughtfully discuss the issue. Make sure any suggested contributions (rare) are in bold so that we may quickly assess them.\n\nAttached is a long paper. My objective is for us to highlight issues with flow, clarity, and consistency, and for you to function as a highly attentive copy editor.\n\nTo begin, please provide a functional decomposition of these tasks and give me a readback of the headers of the document.\n\nDisregard the labels of section headers.",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:05:08.816320Z",
      "updated_at": "2025-04-15T06:05:08.816320Z",
      "attachments": [
        {
          "file_name": "main.tex",
          "file_size": 61248,
          "file_type": "",
          "extracted_content": "\\documentclass[]{interact}\n\\usepackage{fix-cm}\n\n\\usepackage{setspace}\n% \\doublespacing\n\\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.\n\\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables\n%\\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required\n%\\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required\n%\\setlength\\parindent{24pt}% To increase paragraph indentation when line spacing is doubled\n\\setlength{\\parskip}{\\baselineskip}\n\n\\usepackage[english]{babel}\n\\usepackage{csquotes}\n\\usepackage[style=apa]{biblatex}\n\\addbibresource{references-biblatex.bib}\n% \\usepackage[longnamesfirst,sort]{natbib}% Citation support using natbib.sty\n% \\bibpunct[, ]{(}{)}{;}{a}{,}{,}% Citation support using natbib.sty\n% \\renewcommand\\bibfont{\\fontsize{10}{12}\\selectfont}% To set the list of references in 10 point font using natbib.sty\n\n\n\n% \\documentclass[sn-vancouver, pdflatex, referee]{sn-jnl}% Style for submissions to Nature Portfolio journals\n\n%%%% Standard Packages\n%%<additional latex packages if required can be included here>\n% \\usepackage{natbib}\n\\usepackage{graphicx}%\n\\usepackage{url}\n% \\usepackage{multirow}%\n% \\usepackage{amsmath,amssymb,amsfonts}%\n% \\usepackage{amsthm}%\n% \\usepackage{mathrsfs}%\n% \\usepackage[title]{appendix}%\n% \\usepackage{xcolor}%\n% \\usepackage{textcomp}%\n% \\usepackage{manyfoot}%\n% \\usepackage{booktabs}%\n% \\usepackage{algorithm}%\n% \\usepackage{algorithmicx}%\n% \\usepackage{algpseudocode}%\n% \\usepackage{listings}%\n% \\usepackage{tabularx} % in the preamble\n\n\\usepackage{microtype}\n\n\\usepackage{ragged2e}\n\\RaggedRight\n\\usepackage[none]{hyphenat}\n\n\n% % Use double line spacing\n% % \\usepackage{setspace}\n\n% \\usepackage[running]{lineno}\n\n% \\usepackage{enumitem}\n\n\n%%%%\n\n%%%%%=============================================================================%%%%\n%%%%  Remarks: This template is provided to aid authors with the preparation\n%%%%  of original research articles intended for submission to journals published \n%%%%  by Springer Nature. The guidance has been prepared in partnership with \n%%%%  production teams to conform to Springer Nature technical requirements. \n%%%%  Editorial and presentation requirements differ among journal portfolios and \n%%%%  research disciplines. You may find sections in this template are irrelevant \n%%%%  to your work and are empowered to omit any such section if allowed by the \n%%%%  journal you intend to submit to. The submission guidelines and policies \n%%%%  of the journal take precedence. A detailed User Manual is available in the \n%%%%  template package for technical guidance.\n%%%%%=============================================================================%%%%\n\n%\\jyear{2021}%\n\n%% as per the requirement new theorem styles can be included as shown below\n\n% \\raggedbottom\n%%\\unnumbered% uncomment this for unnumbered level heads\n\n\\begin{document}\n\n\\title{A University Framework for the Responsible use of Generative AI in Research}\n\n% \\author[1]{\\fnm{Shannon Michelle} \\sur{Smith}}%\\email{shannon.smith@mq.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[2]{\\fnm{Melissa} \\sur{Tate}}%\\email{m1.tate@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[3]{\\fnm{Keri} \\sur{Freeman}}%\\email{keri.freeman@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[2]{\\fnm{Anne} \\sur{Walsh}}%\\email{anne.walsh@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author*[4]{\\fnm{Brian} \\sur{Ballsun-Stanton}}\\email{brian.ballsun-stanton@mq.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors  }\n\n% \\author[2]{\\fnm{Mark} \\sur{Hooper}}%\\email{m5.hooper@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n% \\author[3]{\\fnm{Murray} \\sur{Lane}}%\\email{mc.lane@qut.edu.au}\\equalcont{Author order was randomised by ChatGPT with the consent of all authors }\n\n\n\n% \\affil[1]{\\orgdiv{Research Services}, \\orgname{Macquarie University}, \\orgaddress{\\street{16 Wally's Walk}, \\city{North Ryde}, \\postcode{2109}, \\state{New South Wales}, \\country{Australia}}}\n\n% \\affil[2]{\\orgname{Queensland University of Technology}, \\orgdiv{Office of Research Ethics and Integrity}, \\orgaddress{\\street{2 George St}, \\city{Brisbane}, \\postcode{4000}, \\state{Queensland}, \\country{Australia}}}\n\n% \\affil[3]{\\orgname{Queensland University of Technology}, \\orgdiv{Graduate Research Education and Development}, \\orgaddress{\\street{2 George St}, \\city{Brisbane}, \\postcode{4000}, \\state{Queensland}, \\country{Australia}}}\n\n% \\affil[4]{\\orgdiv{Faculty of Arts}, \\orgname{Macquarie University}, \\orgaddress{\\street{25 Wally's Walk}, \\city{North Ryde}, \\postcode{2109}, \\state{New South Wales}, \\country{Australia}}}\n\n\n% \\author{\n% \\name{Shannon Michelle Smith\\textsuperscript{a}, \n% Melissa Tate\\textsuperscript{b}, \n% Keri Freeman\\textsuperscript{c}, \n% Anne Walsh\\textsuperscript{b}, \n% Brian Ballsun-Stanton\\textsuperscript{d}\\thanks{CONTACT Brian Ballsun-Stanton. Email: brian.ballsun-stanton@mq.edu.au}, and \n% Murray Lane\\textsuperscript{c}}\n% \\affil{\\textsuperscript{a}Research Services, Macquarie University, North Ryde, New South Wales, Australia; \n% \\textsuperscript{b}Office of Research Ethics and Integrity, Queensland University of Technology, Brisbane, Queensland, Australia; \n% \\textsuperscript{c}Graduate Research Education and Development, Queensland University of Technology, Brisbane, Queensland, Australia; \n% \\textsuperscript{d}Faculty of Arts, Macquarie University, North Ryde, New South Wales, Australia}\n% }\n\n\n\\maketitle\n\n\n%%==================================%%\n%% sample for unstructured abstract %%\n%%==================================%%\n\n\\begin{abstract}\n    AI poses both opportunities and risks for the integrity of research. Universities must guide researchers in using AI responsibly and in navigating a complex regulatory landscape subject to rapid change. Drawing on the consultative experiences of two Australian universities, we propose a framework to help institutions promote and facilitate the responsible use of AI. We provide guidance to help distil the diverse regulatory environment into a principles-based position statement through the core values of honesty, transparency and accountability. Further, we explain how a position statement can serve as a foundation for initiatives in training, communications, infrastructure and process change. This paper underscores the urgency for research institutions to take action in this area and suggests a practical and adaptable framework to do so.\n\\end{abstract} \n\n\\begin{keywords}AI, ethics framework, research integrity, university policy, research institution\n\\end{keywords}\n\n\n% \\maketitle\n\n\\newpage\n\n\n\\section*{Introduction}\\label{sec1}\n\nGenerative Artificial Intelligence (AI) poses both opportunities and risks for the integrity and quality of research. To support researchers in using AI responsibly, institutions must adapt their research governance and management practices to mitigate the risks of AI while remaining flexible to allow for its innovative use in research. This is a significant challenge due to the uncertainty of how AI will ultimately impact the research sector, which is driven by a complex research regulatory environment, rapid innovation, and differing norms between research disciplines, universities and countries.\n\nThis paper draws on insights from two Australian universities to provide a strategic framework for navigating this evolving space and promoting the responsible use of AI in research. We provide worked examples of how Macquarie University (Macquarie) and Queensland University of Technology (QUT) have followed this framework to develop a principles-based position statement and guide the delivery of communications, infrastructure, process improvements and training to support researchers in the responsible use of AI. \nOur framework, based on current literature and practical implementation plans, serves as a foundational and adaptable model offering clarity and direction for research institutions.\n\nWhile most recent studies have focused on the impact of AI on undergraduate teaching and learning and assessment  \\parencite{sullivan_chatgpt_2023,crompton_artificial_2023}, discussions of institutional responses to the responsible use of AI for \\textit{research} are much less common \\parencite{popenici_critique_2023}. Universities are uniquely placed to instigate proactive measures to mitigate serious risks posed by AI, such as creating fake or unreproducible research \\parencite{gibney_could_2022, liverpool_ai_2023}, which threaten research credibility and public trust. Our paper emphasises developing an institutional position statement that applies research integrity principles to AI challenges, serving as \\textit{common ground} for mobilising training, communication and infrastructure initiatives to meet specific institutional and research needs.\n\nThe aim of this paper is to provide a structure to develop guidance in this evolving context. The literature and discussion in this paper should help inform institutional decision-making related to governance and policy implementation. This paper is a case study, allowing readers to apply our lessons learned and models in the development of their own institutional policies, using the framework we extrapolated from our efforts.\nWe offer this strategic framework to allow institutions to identify critical issues and practices that they and relevant stakeholders must address. In doing so, institutions can guide appropriate and effective responses to opportunities and challenges related to the responsible use of AI in research. \n\nOur framework is structured in four layers that represent the order in which the elements should generally be considered and implemented. Figure \\ref{fig:framework} provides a diagrammatic representation of our framework. \n\n\\begin{enumerate}\n\\item\n  \\textbf{Context} describes the external and internal policy environment that governs research integrity, research conduct and research student supervision and assessment.\n\\item\n  \\textbf{Development} of a university position includes the position statement and describes worked examples by our two universities on how we consulted across disciplines and engaged AI researchers.\n\\item\n  \\textbf{Implementation} describes a plan to put the position statement into practice with appropriate support, processes and infrastructure.\n\\item\n  \\textbf{Review} describes a plan to iteratively evaluate the framework to test its effectiveness and undertake revisions or updates to ensure currency.\n\\end{enumerate}\n\n\n\n\\begin{figure}[ht]\n\\centering\n% \\includegraphics[width=\\textwidth]{240415_Framework_A4.pdf}\n\\caption{A framework for institutions to support the responsible use of AI in research (available here under a Creative Commons CC-BY 4.0 Licence: \\url{https://osf.io/9b5an/}). Image by Mark Hooper (mark@trickygoose.training). }\\label{fig:framework}\n\\end{figure}\n\n\nThe structure of this paper reflects the four levels of the framework, which are numbered accordingly from one to four: context, position, implementation and review. We begin by explaining the institutional and external context for the responsible use of AI in research. This section essentially forms a literature review underpinning our topic. We then outline the processes involved in developing a position statement or guidance note which can be viewed as a methodology. The primary result of our study is the framework itself. However, our descriptions of the framework's implementation and review can also be viewed as forms of results. Lastly, discussions around the various issues raised by the development of the framework are included throughout the paper. We took this unconventional approach to the structure of the paper for conceptual legibility to guide the reader through each concept in the framework chronologically.\n\nIt is worth noting that Macquarie University called its document a Guidance Note for two reasons. First, pragmatically, a Guidance Note, did not require full senate approval for updates, allowing for the note to be a responsive living document. Second, as a document which exists only in an advisory capacity, the format of the Macquarie Guidance Note was significantly more pedagogical, containing many footnotes with links to resources and explanations that would be inappropriate in a more formal senate-confirmed Procedure or Policy. Each university governance structure is different, but having the ability to respond to models that update at a frequency of a few months is a highly desirable component of this framework.\n\nIn this paper, the term, \\textit{framework}, refers to the overarching structure guiding institutional governance, strategic direction, and adaptation to AI-related developments. \\textit{Policy}, on the other hand, refers to specific institutional rules and formal guidelines that establish compliance requirements for researchers around their use of AI.\n\n\\section{Context}\\label{sec3}\n\nPolicy development by research institutions is shaped by external and internal contextual environments, and in the case of AI, the relationships between these environments are complex: the increasing use of AI is creating new ways of conducting research; introducing new techniques, methods and complex datasets, and challenges around authorship, integrity and judgement. This poses challenges to accepted research practices and requires institutions to reassess institutional policy and infrastructure to support researchers and university staff in research integrity, human research ethics, research degree requirement, data management, sector policies and terms of service. \n\n\n\\subsection{Research integrity}\\label{research-ethics-and-integrity} \n\nIn Australia, the NHMRC \\textit{Code for the Responsible Conduct of Research} \\parencite{nhmrc_australian_2018} establishes principles that, extended, also guide responsible AI use in research, notably rigour, transparency, and accountability. Researchers must document and transparently disclose AI integration into their research, detailing AI tools, methodologies, and potential biases \\parencite[Principles 2, 3]{nhmrc_australian_2018}. Comprehensive records, including logs of AI interactions, prompts, and post-processing methods, should be securely maintained and made accessible to promote reproducibility.\n\nDisclosure practices vary widely across disciplines and institutions. Many journals currently require minimal disclosure following COPE guidelines \\parencite{committee_on_publication_ethics_cope_2024}. However, some have started mandating detailed disclosures. For instance, APA requires authors to explicitly document AI use in their methods sections and to provide full AI outputs as supplemental materials \\parencite*{american_psychological_association_apa_2023}. Practical disciplinary trends include attaching AI-generated logs and prompts as supplemental material \\parencite[e.g.][]{butson_ai_2024, shiferaw_spatial_2024}.\n\n\\subsubsection{Acknowledgement, disclosure, and documentation}\n\nAcknowledging AI use separately from general acknowledgements, such as under its own heading akin to funding or conflicts of interest, can clarify the extent and role of AI contributions. Simply acknowledging AI use is insufficient for rigorous peer assessment; detailed disclosure, proportional to the AI involvement, should include specific prompts, model versions, and outputs \\parencite*{ballsun-stanton_briefing_2024}. While no consistent standard exists yet across journals or disciplines, Macquarie and QUT have initiated comprehensive requirements for postgraduate students. Macquarie mandates documentation of AI use in each student's thesis, including interactions and supervisory approval, while QUT requires detailed written disclosure of AI use within theses \\parencite{queensland_university_of_technology_ai_2025}.\n\nHowever, pragmatic concerns about the practicality of these standards highlight implementation challenges. The required `active supervision and written consent' may be burdensome; thus, institutions must balance rigorous expectations with realistic supervisory practices. Both Macquarie and QUT acknowledge this tension, placing accountability largely on supervisors while recognizing a need for clearer norms and ongoing education. \n\nTherefore, effective, ethical, and responsible AI integration in research necessitates explicit institutional guardrails and ongoing training aligned with evolving ethical standards. Institutions should support researchers in applying FAIR principles \\parencite[see][]{wilkinson_fair_2016}, making AI-related data and methods transparent to strengthen research integrity and reproducibility. Rather than banning generative AI\u2014which risks unintended shadow use, institutions should proactively equip researchers with the skills to critically evaluate and transparently disclose AI involvement, addressing ethical challenges while leveraging AI\u2019s potential for scholarly innovation \\parencite{sadasivan_can_2023, ben_jabeur_artificial_2023}. An adaptable disclosure checklist supporting discipline-specific guidelines is provided in Appendix 1 for practical guidance across diverse research contexts.\n\n\n\\subsection{Human research ethics}\\label{human-research}\n\nResearch involving human participants is governed by region-specific\nlaws, regulations and guidelines. Human research ethics is guided by\nthe NHMRC code, which outlines the ethical aspects of human research\ndesign, review and conduct. When AI is used in research with\nhuman participation or data, ethical implications require close\nconsideration by Human Research Ethics Committees (HRECs), also known as\nInstitutional Review Boards \\parencite[][]{zhou_ai_2023, herington_ethical_2023}.\nAustralian universities must have frameworks and training to help researchers apply\nthe principles of the NHMRC code to AI use and equip HRECs with\nthe necessary knowledge for project review.\n\nRelevant ethical issues that fall under the scope of the NHMRC code\nrelate to bias and committee membership. While AI may not be\ninherently biased, the data upon which they are trained and the\ninterpretation of their results can lead to bias. For example, \\textcite{obermeyer_dissecting_2019} revealed bias in an AI tool used to identify\npatients for follow-up care that reduced the number of black patients\nidentified for additional care by more than half. The images generated by\nAI could also promote bias due to their training data, system\nprompts, or responses to previous scandals \\parencite[see][]{robertson_google_2024, yue_chatbodies_2024}. On the other hand, \\textcite{salah_may_2023} suggest\nthat AI could potentially help avoid human bias such as in\ncoding and categorisation of qualitative studies. Another issue in\naccord with the NHMRC code involves technical expertise on HRECs.\nCurrent committee membership is diverse, but may not necessarily include\nthe expertise to ethically evaluate research with AI.\nResearch institutions may therefore need to reconsider the core\nmembership of their HRECs and animal ethics committees to ensure\nsufficient knowledge and experience is available. Moreover, these technologies are not like traditional IT services. They require specialised research support around prompting, ethical issues, methodological implications and quality assurance.\n\n\\subsection{Research degree requirements}\\label{research-degree-requirements}\n\n\nStandards for postgraduate degrees are set by regulatory bodies external to the university. In Australia, the standard for doctoral degrees requires researchers' full responsibility and accountability for personal\noutputs, regardless of source and sufficient cognitive skills for\nintellectual independence and critical thinking\\parencite{australian_qualifications_framework_council_australian_2013}. Given that some scholars question the use of AI as infringing independent thought, there are additional questions needed to ensure the reputation of postgraduate work in this instance, as norms shift \\parencite{butson_ai_2024}. Therefore, implementation of\npostgraduate degree policy at an institutional level requires more\ndetailed guidelines than those offered by external regulatory bodies,\nand university protocols on the ethical use of AI may be one\ncomponent of these guidelines. This can be challenging, since the novel\nposition of postgraduate students, acting as researchers in training,\ncan sometimes be unclear. On the one hand, their work must be subject to\nassessment and degree standards, but as researchers, they can be treated\nas academics and even equals \\parencite{tanggaard_survival_2017}. If research staff are taking advantage of cutting-edge opportunities using\nAI, then it is natural for an institution to encourage\npostgraduate students to do the same. This duality of identity presents\na unique challenge for internal institutional policies and requires\nuniversities to take action to adequately support research students'\nunderstanding of their ethical responsibilities when using AI.\n\nMoreover, postgraduate student use represents two novel failure modes which should be accounted for. The first is over-reliance on AI without the student engaging in \\textit{any} critical thinking, where `the AI did my thesis for me' is the worst outcome. The second is the opposite problem: extremely problematic first drafts and under-reliance on AI for editing and polishing. In recognition of these challenges, both universities placed the responsibility for monitoring appropriate AI use onto supervisors, as this aligns with existing research training responsibilities. The Macquarie Guidance note (Section 2.3) splits this Gordian knot by requiring the `active supervision and written consent' of the supervisor, allowing the supervisor to actively and narrowly govern the AI use of their student. QUT requires that postgraduate students `discuss the use of any AI in writing your thesis with your principal supervisor prior to use'(Internal QUT HDR guidance 2025).\n\nChanges to postgraduate student research processes and assessment are a\nlikely consequence of increasing use of AI across\nuniversities. Students may use \nAI for planning research design, conducting research and assisting in\nthe preparation of research outputs and presentations, all of which\ncould require revisions to existing training and a clearer understanding\nof the responsibilities. Likewise, university processes around thesis\nassessment must also undergo review. Any effective policy for assessing\npostgraduate students' work must be enforceable, and therefore universities\nmust address the feasibility of detection \\parencite{lodge_assessment_2023}. However,\nquantitative measures of AI use seem currently\nunreliable \\parencite[see][]{liang_gpt_2023, openai_how_2023, sadasivan_can_2023}. One possible response to this challenge should be for a greater\nreliance on in-person, verbal defences of student and research work,\nwhich are currently uncommon in Australia \\parencite{bending_safeguarding_2023, pearce_rethinking_2023}. As \\textcite{vinge_rainbows_2006} predicted, `{[}T{]}he beginning of\ntrust has to be an in-person contact.' For universities to acknowledge\nstudent capabilities, in-person milestones are needed to help validate a\nstudent's intrinsic knowledge and subject mastery outside the context of\nany tool-assisted capabilities.\n\n\\subsection{Research data management}\\label{research-data-management}\n\n\nAI platforms process user data offshore, including\npotentially sensitive research data, which serves as another risk in the\nuse of AI. Depending on the security supporting the\nAI platform, data may be at risk of privacy or copyright\nbreaches due to user error or cyber threats. Furthermore, a user's\nprompts may compromise ownership of subsequent innovations, as the legal\nand moral aspects of copyright related to AI are presently\nunclear \\parencite{wipo_ip_and_frontier_technologies_division_generative_2024}. According to the\nNHMRC code, institutions must provide supporting infrastructure for\nresearch data, including guidance and facilities, to protect data\nownership, security and confidentiality \\parencite[Section 2]{nhmrc_management_2019}.\nInstitutions must therefore exercise prudence when employing\nAI to support research data, particularly where the AI\nplatform may not apply adequate data privacy, security and Intellectual\nProperty (IP) standards. Universities choosing what AI to use\nmust trade off between model capability, speed of response, building\nversus buying and internal capabilities when trying to thread the IP\nneedle.\n\nCopyright is another area of concern in the context of research data\nmanagement. The legal aspects of copyright related to the outputs of\nAI in Australia have yet to be updated to address the use of\nAI and are inadequately regulated \\parencite[see][]{gaffar_copyright_2024, samuelson_generative_2023}. It is\nnot currently known whether works created by AI platforms\nwill be protected given that the nature of copyright in Australia relies\non human authorship. Changing copyright standards for AI involvement may cause a published \nwork to not meet the human author threshold, and thus may not be eligible for protection under the current law. Research data management is\nanother example of the need for researchers to be supported while\nspecific regulations addressing the use of AI are resolved.\n\n\n\\subsection{Sector policies}\\label{sector-policies}\n\nEnsuring accountability in research practices necessitates robust sector\npolicies for academic publishers, research funding and institutional\nstrategies to manage the impact of AI. Academic publishers,\nparticularly those adhering to the Committee on Publication Ethics\n(COPE) guidelines, emphasise the importance of ethical standards in\npublication \\citeyear{committee_on_publication_ethics_cope_2024}. They generally agree that AI cannot be\nheld responsible as authors; it can be used for subsidiary research\npublishing tasks \\parencite{nature_portfolio_artificial_2023}, provided acknowledgement is\nincluded. Given the rapidly evolving landscape of AI,\npublishers and institutions must be careful not to harm authors\nwith false claims of inappropriate use of AI \\parencite[see][]{wolkovich_obviously_2024}. This acknowledgement-permissive mode ensures that content\nresponsibility always remains with human authors.\n\n\nLarge research funding bodies external to universities may also\ninfluence institutional research policies and practices and may override\nuniversity policies for responsible use of research funds. Mitigating\nrisks is a primary priority, exemplified by the European Union's strict\nlegislation on transparency, bias, privacy and copyright principles \n\\parencite{gibney_what_2024}. In Australia, several funding bodies have implemented\npolicies on the use of AI in peer review grant\napplications \\parencite{australian_research_council_research_policy_branch_policy_2023, national_institute_for_health_use_2023}. The Australian Research Council\nacknowledges that while AI presents opportunities for\nresearch and grant proposal writing, the responsibility for errors falls\non researchers and their institutions. This stance,\nalthough not new, emphasises the need for thorough due diligence and\nappropriate training and guidance due to the complexity and uncertainty\nintroduced by AI.\n\n\nGrant-makers' concerns include the ability of AI to make\neffective judgments, the potential sharing of confidential\ninformation and protected intellectual property if documents are\nuploaded to publicly available AI services. Although these issues might be\nresolved, human oversight remains crucial to maintaining\nconfidence in the peer review process and the responsible allocation of\nresearch funds. Thereby underscoring the principle of accountability in\ninstitutional AI position statements.\n\nAnother aspect of sector policy impacts on AI research frameworks involves institutional strategies and\noperational plans. A 2024 Australian government review of higher\neducation quality, accessibility and sustainability highlighted the\npotential of AI to boost research productivity, urging\nuniversities to lead in building new research infrastructure \\parencite{australian_universities_accord_australian_2024}. AI may contribute to digital\ntransformation efforts, similar to the push for paperless offices in the\n1990s \\parencite{meyendorf_digitization_2021}, and universities might deploy AI to reduce\nstaff loads, an efficiency expected to extend to research integrity and\nethics committees. However, institutions must carefully assess the\nimportance of human judgment in all tasks, especially in research\nsupport and governance roles. The rapid evolution of AI also\ncomplicates the enterprise software purchasing process, as products may\nchange significantly before they are fully implemented. Therefore,\nproactive strategies can ensure that institutional frameworks\nincorporate responsible research conduct principles and provide adequate\nsupport for researchers amidst the evolving AI landscape.\n\n\n\\subsection{Terms of service}\n\n\nInstitutions should regularly monitor the terms of service of\nAI platforms to maintain control over research data and\ncomply with applicable privacy legislation. Typically, an institution\ncan negotiate specific terms of service when\npurchasing enterprise research software and can\nreasonably expect that it will remain the same throughout the course of\nthe agreement. However, the current rate of change means that\nAI platforms release frequent terms of service updates which presents a significant legal risk if subsequent updates changeintellectual property or privacy provisions that compromise security and ownership.\n\nAI platforms also mandate service use disclosure that puts the onus of\nresponsibility for any text generated by AI onto the human\nauthor \\parencite{openai_sharing_2022} and specific agreements must inform university policies. Therefore,\nposition statements should clearly allocate the responsibilities of\nmonitoring AI company terms and to\nmaintain a list of services, models and modes of interaction that are\nrisk-acceptable and privacy-acceptable to the university.  While an institution cannot forbid\nan entire technology category, it can point to specific ways\nof using services and guide researchers to more effective and less risky\noptions. The continually changing technology makes maintaining these position statements an ongoing task.\n\n\n\\section{Developing a University Position}\n\nConsidering the diverse contextual factors relevant to the use of\nAI in research, an institutional position statement can serve\nthe important function of distilling and applying that context in one\nplace. In general, position statements provide a foundation for the\ndevelopment of institutional policies, guidelines and training in a\nwhole of enterprise strategy. Applied to AI, they enable\ninstitutions to take a proactive stance to promote the responsible use\nof technology to foster research quality and innovation, while\nprohibiting practices inconsistent with research integrity. \n\nDeveloping a policy (controlling document), position statement (preparatory document), or guidance note (advisory document) --- as institutional politics and knowledge dictate --- informs research managers and research leaders, empowering them to contribute to institutional initiatives. By providing\na common ground within the institution, a clear position statement can\nreduce confusion caused by the saturation of literature and opinions,\nthe proliferation of new tools and the rapid pace of disruption. A guidance note can serve as a first introduction, allowing conversations to occur in the absence of formal and enforced policy.\n\nThe\nformat of a position statement can be simple, with clear declarative\nstatements about responsible use of AI, forming the basis for\nmore complex considerations and scholarly debates on nuanced issues. The format of a guidance note requires more explanation and justification of decisions, in an attempt to persuade and inform, with enforcement power coming from extant policy.\n\nThis section of the paper describes the processes involved in the\ndevelopment of a university document on AI (we will say Position Statement here on out), serving as both a\nmethodological outlining and a case study of how our two institutions developed their own position statements.\n\n\n\\subsection{Consult across disciplines and research services}\n\nDeveloping a position statement requires broad consultation across\nresearch disciplines and research support services to capture the\ndiverse ways of using AI. Development requires targeted\nconsultation with academics who have expertise in research ethics and\nresponsible research practices. This consultation could take multiple\nforms. We describe the process here for two universities: QUT and\nMacquarie.\n\nQUT began\nconsultation in January 2023 by drafting a discussion paper:\n\\emph{Emerging AI tools and challenges for research integrity}, which\nwas then circulated broadly to provoke discussion and feedback. This\ndiscussion paper recognised the potential opportunities of emerging\nAI in research. It outlined the inherent risks AI\ncould pose for responsible research practices, such as authorship,\noriginality of publications and research outputs. It provided a\nbalanced overview of the use of AI in research to help focus\nthe consultation on the key issue of how AI intersects with\nresearch integrity. We solicited feedback from across the university, including from Research Integrity and Ethics Advisers.\n\n\\textbf{Macquarie University} established a committee of\nstakeholders and consulted experts from each faculty and professional\nservices. They considered the risks and opportunities posed by\nAI to research integrity, processes and operations. The\ncommittee was tasked with considering whether a new policy or targeted\npolicy updates were required or whether changes to processes were\ndeemed appropriate. Anticipating the fast pace of change and the\nforecast need for regular updates, a Guidance Note outlining the\nUniversity's position and recommendations regarding the use of\nAI in research was drafted. To create the Guidance Note, the\ncommittee evaluated the current literature and engaged in cross-faculty\ndiscussions. Key aspects of the Guidance Note included identification of\nkey resources to provide an initial foundation for researchers,\npostgraduates and support staff and a summary of potential risks and\njustifications for the intended approach. The Guidance Note was\nsubmitted to the relevant committees for comment or endorsement. This\nstep was considered critical to allow diverse areas of the University\nan opportunity to build understanding and consider implications for\ntheir areas or alignment with other projects or initiatives.\n\nFor both cases, consultation was coordinated by each\nuniversity's Office of Research Ethics and Integrity (OREI). The\nconsultation process provided the opportunity for OREI at both\nuniversities to engage with their research communities to understand the\nvaried uses of AI in research and the potential impacts of\nAI on responsible research practices. Our examples illustrate\nthat while the approach to developing a university position can vary\naccording to each institution's needs and internal structure,\ncoordinated consultation, discussion and feedback across university\nresearch communities is crucial to ensure all stakeholders are provided\nthe opportunity to participate in the process.\n\nIn 2023, when both universities were first writing their documents, no informed student voice was available for formal consultation. During development at Macquarie, in the three months of development of our Guidance Note, the research working group was instructed to consult with their HDR students and appropriate staff in their respective faculties. QUT also asked staff to reach out to well-informed students.\n\nOne crucial aspect of the development of our frameworks was involving researchers and practitioners with actual, thorough, use of the tools when developing our advice. In 2023, this pool was quite narrow, leading to most consultation through faculty experts and those people tasked with investigating these services. In 2025, a broader and more consultative approach is warranted.\n\n\\subsection{Engage AI Researchers}\n\nApart from the formal consultation process across research disciplines\nand services, it is vital to recognise that some researchers have long been experts in this space and already engage both\ninternally and publicly with emerging opportunities and challenges.\nWhere possible, these researchers should be invited to participate in the consultation process to incorporate their input and gain their endorsement. According to the Technology Acceptance Model \\parencite{shahzad_factors_2024}, the advocacy for institutional policy by trusted academics has generally been shown to be a successful strategy for the adoption of technology by staff and students \\parencite{faden_importance_2002}. Engaging AI researchers within the university forms a vital part of an institution's strategic framework in shaping policies related to research integrity and providing essential AI literacy training. As \\textcite{butson_ai_2024} have highlighted, it is also essential that institutions consider contrasting and diverse perspectives related to the integration of AI in research tasks, because `this is not merely a technological transition but also a significant ethical and methodological turning point' (p. 567). Combined broad and specific consultation ensures that institutions not only address the challenges but also explore the opportunities AI provides. Consequently, we consulted innovators and early adopters in our universities who had already published their research \\parencite[e.g.][]{bell_rapid_2023, doherty_upskilling_2024, kowalkiewicz_economy_2024, mcdonald_apostles_2024}.\n\n\n\n\\subsection{Position Statement}\n\nA strong position statement should address key research integrity\nprinciples, especially honesty, transparency and accountability, and\nreinforce the importance of these principles in the responsible use of\nAI in research practices. Our university position statements\nhave been developed based on broad stakeholder feedback. They provide a\nconsolidated institutional position and clear guidance for diverse\nfaculties and research groups. We provide copies of our position\nstatements as Attachments 1 and 2, and make them available to adapt\nunder a Creative Commons 4.0 CC-BY Licence.\n\nBroadly, our two position statements have much in common:\n\\begin{itemize}\n\\item\n  They \\emph{support} other existing responsible conduct of research policies.\n\\item\n  They \\emph{encourage} the use of AI where it promotes responsible research quality and impact.\n\\item\n  They are primarily \\emph{principles-based} so that the readers can interpret their implementation within their own specific context.\n\\item\n  They have minimum prescribed rules, such as: \\begin{itemize}\n  \\item\n    An AI tool must not be listed as an author, as this violates the principle of accountability for research outputs.\n  \\item\n    Explanations on how AI was used should be included in the\n    Methods and/or Acknowledgements sections of a research output to be\n    consistent with the policies of publishers and the principle of\n    transparency.\n  \\end{itemize}\n\\end{itemize}\n\nThere are some notable differences between the two statements in terms\nof style and delivery. While QUT named its document a `position\nstatement' to emphasise that it provides definitive university-level\nadvice, Macquarie University named its statement a `Guidance Note',\nemphasising its role in highlighting extant policies and indicating how they apply to the responsible conduct of research. Both the guidance note and the position statement proved politically useful for the subsequent development of AI policy in 2025, as well as texts to align university thinking around consistent norms of the responsible use of AI in research.\n\n\n\\section{Implementation}\n\nA position statement serves as a foundational guide, articulating the university's principles and position on the responsible use of AI in research. However, it is not a policy itself, but rather informs the development of specific institutional policies and procedures that establish enforceable guidelines. To ensure effective implementation, these policies must be embedded within a broader strategic framework. This strategic framework then supports the development of a governance structure that guides institutional responses and adaptations over time. The framework should encompass initiatives in training and education, procedural updates, equity in access across staff and students, impacts on infrastructure and a communication and engagement strategy.\n\nThis section of the paper outlines each of these key areas and reflects\nthe third layer of our framework after Context and Development.\n\n\\subsection{Communication and engagement strategy} \n\n\nRapid AI developments and new AI services can overwhelm researchers tasked with staying informed. To support informed decision-making, any position statement must include a well-designed communication and engagement strategy.\n\nIntroducing a new policy naturally requires clear communication. However, policy alone rarely changes culture; thus, it must be accompanied by active \\textit{engagement} initiatives. QUT's position statement `recognises that the use of, and implications of using, AI tools may differ widely by discipline,' encouraging ongoing scholarly debate and research. The unique risks and benefits of AI particularly necessitate proactive change management \\parencite{singh_22nd_2023,kotter_leading_1995}.\n\nRetraction Watch maintains a list of papers flagged as `ChatGPT Writing,' indicating low-effort or uncritical AI usage \\parencite{marcus_papers_2024}. Such publications can negatively impact researchers and institutions. To mitigate these risks, institutions must clearly communicate expectations around responsible AI use. Classroom attempts to detect unauthorised AI use have sometimes failed dramatically, typically due to inadequate guidance about AI's capabilities and limitations \\parencite{klee_professor_2023}.\n\nAs AI capabilities, services and models continually emerge, institutions must not only communicate broader policy implications to staff and students, but also provide safe institutional access to mitigate privacy risks and promote equity of use. This communication should be a coordinated change management strategy that allows researchers to understand both the benefits and risks of AI, especially when available models change. For example, policy authors might visit department meetings to explain critical clauses and their technical or practical rationale, ensuring that most researchers are exposed to the new guidelines.\n\nBeyond announcements and informational sessions, a detailed communication plan can support further university-wide discussions on the nuances and implementation of AI. Encouraging and acting on feedback allows faculty and researchers to voice concerns, particularly those critical of emerging technologies. It is critical to foster a collaborative culture that focuses on policies related to responsible AI use and also capitalises on researchers' experiences of the benefits that using AI tools can provide \\parencite{balalle_reassessing_2025}.Through such collaboration and conversations, universities can refine norms accepted by various faculties and reach consensus on ethical AI use. Often, such norms can help mitigate institutional risk by clarifying which training resources and AI literacy initiatives are needed to support ongoing engagement. \n\n\n\\subsection{Training and education strategy}\n\n\nTraining and education are fundamental and ongoing elements of any\ninstitution's response to ensuring its researchers fully understand\ntheir ethical responsibilities around the use of AI in their\nresearch practices. In Australia, institutions have a responsibility to\n`\\emph{Provide ongoing training and education that promotes and supports\nresponsible research conduct for all researchers}' \\parencite{nhmrc_australian_2018}.\nMoreover, AI literacy is fast becoming an essential part of research\nprocesses and a valuable industry skill \\parencite[see][]{van_noorden_ai_2023, hutson_hypotheses_2023, prillaman_is_2024}. As reported by \\textcite[3]{thompson_ai_2023}, `there is still sensemaking work to be done, as misconceptions about what AI is and how it works remain common.' \nTherefore, there is an urgency for institutions to provide education that\nequips research graduates for their careers and empowers academic staff\nto pursue industry research partnerships.\n\n\nThe diversity of opportunities provided by AI heightens the\nimportance of AI literacy training and initiatives which emphasise and\nintegrate research integrity. However, supervisors are best positioned to promote the appropriate upsides of AI use. The developed position statement should, instead, focus on downside risks and issues. Some issues to consider, therefore, include data management\npolicies, authorship guidelines, copyright law and research\nreproducibility. AI literacy training must also focus on the development\nof skills to understand, identify and address the trustworthiness and\nreliability of AI. Researchers should be able to assess whether\nusing AI is appropriate for a specific challenge and therefore be able to defend their choice of AI selection or not. Necessary researcher skills include the ability to tackle\nchallenges such as privacy and the risk of disclosing IP, the potential\nfor bias and prejudice in the data that AI uses to formulate\nresponses and the ability to critically evaluate outputs. Training\nshould focus on explicit scaffolding and self-regulated learning that\nencourages critical engagement and agency in ethical decision-making\nwhen using AI \\parencite{markauskaite_rethinking_2022, mcknight_electric_2021, unesco_chatgpt_2023}.\n\nAt our two universities, initial researcher training on the responsible\nuse of AI has included workshops, online modules and panel\ndiscussions allowing postgraduate students and supervisors to ask\nquestions of experts with diverse specialities in AI research, research\nethics and student policies. Training has also emphasised the importance of effective and thoughtful prompting. Likewise, online modules challenge\nparticipants to make decisions for themselves on the appropriate use of AI, using the key principles of honesty, transparency,\naccountability and fairness, through scenarios and case studies. In each\nof these examples, rather than providing purely didactic instruction,\nparticipants are prompted to consider their own unique circumstance and\nmake decisions accordingly. As such, training strategies on \nAI usage must be regularly updated and adaptable to keep pace with the\nfrequent updates and advancements in the field.\n\n\\subsubsection{Acknowledgement and documentation of use by postgraduate students}\n\\label{subsection:ack}\n\nAs an example of training and education strategy, we may examine how both universities have set high expectations of their postgraduate students. These requirements have been written into guidance for postgraduate students and communicated by presentation and webpage to supervisors and students. \n\nMacquarie University has chosen to require documentation of use: \n\\begin{quote}\nGenerative AI, LLMs, and tools integrating these models will likely become part of the writing, research,\nand composition process for many researchers and students. However, Higher Degree Research (HDR) and\nHonours students must be responsible, from start to finish, for their research. ... At Macquarie University, it is recommended that \\textit{all} stages of a students\u2019 research journey be well documented\u2014both for data integrity purposes and to serve as evidence of originality and intellectual input if challenged.\n ... Supervisors must closely monitor the use of Generative AI by their research students. Generative AI should not be used without supervisory approval, and where these tools are used in any\ncapacity by research students or supervisors, they must be acknowledged' (Macquarie University Guidance Note, Section 2.3). \n\\end{quote} \n\nAt QUT, the position statement focuses on the university's commitment to research integrity. Additionally, research students are instructed to seek approval from their Principal Supervisor prior to AI use. Any use of AI, including text editing for readability and grammar, must be disclosed in the thesis acknowledgement and a detailed written description documenting the extent of AI use and how it has been used is also required  \\parencite{queensland_university_of_technology_ai_2025}. \n\nWe acknowledge that both universities could do more in this regard---as there is a tension between the high standards of our guidance and the lived realities of supervisors. The pragmatic reality of both of these institution's requirements is that of offloading the onus of responsibility of the student's use of AI onto the supervisor. The Macquarie Guidance Note chose to explicitly empower supervisors to make the guidelines specific \\textit{per student} as students and supervisors have different needs and expectations when it comes to writing and AI use. This phrasing is an acknowledgement that as researchers in training, the consequences of AI use will fall on the supervisor. The choice of terms of `active supervision' is to emphasise the supervisor's responsibility of this component of research training. The level of active attention given by the supervisor is up to the supervisor's discretion. \nAt Macquarie, Ballsun-Stanton has mediated supervisor and postgraduate student meetings setting out the allowable scope of AI use---while these meetings are the exception to the norm of benign neglect, some students and supervisors have been open to these discussions when educated about the guidance note. More work is necessary around norm setting, provision, and sufficient education of the appropriate uses of these technologies, however.\n\n\n\\subsection{Infrastructure and procedural considerations}\n\nUniversities face several common barriers when implementing AI policies: equity of access, budget constraints, maintaining integrity, and promoting appropriate use amid uneven distribution of expertise. For institutions with limited resources, the rapid evolution of AI creates additional challenges.\n\nTo address these barriers effectively while maximizing limited resources:\n\n\\begin{enumerate}\n\\item Make transparent disclosure mandatory through existing processes. Data management plans and ethics applications should require researchers to declare AI use, creating accountability without new administrative structures.\n\n\\item Prioritize cost-effective solutions. The price of AI inference is dropping rapidly, \"about 10x every 12 months\" \\parencite{altman_three_2025}, making thoughtful commercial AI adoption increasingly affordable compared to custom solutions that promise to reduce expertise requirements. Paying per token with a low daily budget allocated per student and researcher is \\textit{significantly} cheaper than paying a fixed fee per month per user, regardless of their use. By targeting a cost-efficient frontier model with a pay-per-token approach, universities pay only for what they use, and cap expenses of power-users.\n\n\\item Leverage established research norms. Existing integrity frameworks provide a foundation for \\textit{post hoc} judgment as technology norms evolve, requiring only targeted adaptations rather than complete policy overhauls.\n\n\\item Ensure auditability through service selection. Choose AI platforms that maintain researcher-accessible logs and comply with institutional research policies, avoiding costly compliance issues later.\n\\end{enumerate}\n\nMost universities already have secure research data infrastructure that can be leveraged for AI implementation. By communicating clear trade-offs between hosted solutions and institutional infrastructure, universities can help researchers make appropriate choices for their specific needs while controlling costs and maintaining security standards.\n\n\\section{Review}\n\nThe evaluation process is an important component in reviewing the design,\nimplementation and refinement of university policy and is therefore integrated\ninto the framework at all levels. At each stage of development, implementation and communication, feedback from stakeholders is needed to highlight mistakes, inform future policy development and respond to the changing AI landscape. The evaluation process, therefore, provides a review\nmechanism to inform stakeholders of policy effectiveness, \nensures the currency of the position statement and informs new directions and updates to the framework. Essentially, the evaluation phase of a\nstrategic framework involves asking stakeholders for feedback on\nprogress and adjusting the framework and/or implementation strategies\naccordingly. This is a cyclical process where implementation can have impacts on the institutional context, and therefore, feedback is essential to inform the next stage of policy development. \n\nStrategic planning, governance and policy development for the responsible use of AI will continue to evolve and continuous adaptation is essential to `harness its full potential while safeguarding academic integrity and promoting equitable access' \\parencite[79]{mcdonald_apostles_2024}. We suggest reviews should consistently examine gaps in\nresearch governance that will inevitably be created by ongoing research\ninnovation and digital disruption. Given the fast pace of AI change, it is inevitable that\nfuture changes will necessitate equivalent policy changes to maintain\nrelevance. For example, as AI developers and providers continue to address limitations and challenges related to model capabilities, inquiry and rigour to understand the challenges and benefits of AI updates to models and updates will remain fundamental and ongoing issues that are `central to academic endeavour and require rigorous scrutiny' by institutions and researchers \\parencite[566]{butson_ai_2024}. We also propose a more direct form of review; to regulalry survey\nstakeholders at regular intervals.\n\nTo evaluate effectiveness amid rapid AI evolution, institutions should track these trends: awareness and engagement, whether researchers understand policies; compliance trends, whether AI misuse is decreasing; and policy relevance, whether guidelines remain current. As \\textcite{chen_research_2024} note, effective mechanisms are required to `detect and prevent unethical behaviours in research, such as data fabrication, plagiarism, and improper use of AI technology.\u2019 Regular reviews should prioritize maintaining currency rather than reactive overhauls, tracking new model capabilities that impact different university stakeholders in uneven ways. \n\nFor example, after \\textcite{mollick_what_2025} described using AI to detect statistical errors in journal articles, Macquarie updated its Peer Review policy to permit reviewers to use AI tools that support their judgment, provided this doesn't violate confidentiality or commissioning guidelines. Similarly, QUT is implementing additional training based on stakeholder feedback. These cases demonstrate how institutions can adapt policies to reflect emerging scholarly practices through regular review. \n\nThe Macquarie Guidance note levies an additional onus on supervisors: `Supervisors should have iterative exposure to their students\u2019 work across their entire research degree and ensure their research progression (including critical judgment or originality) is observed and well documented. Supervisors must closely monitor the use of Generative AI by their research students' (Macquarie Guidance Note, section 2.3). Macquarie is tracking these conversations through checking compliance at Data Management Plan submission stage, along with \\textit{ad hoc} conversations with supervisors during graduate training. QUT is tracking the effectiveness of its training through training evaluation surveys.\n\n\\section*{Conclusion}\n\nAs universities grapple with the challenges and opportunities presented\nby AI, our paper introduces a strategic framework: which identifies context to investigate, stakeholders to consult in development, key implementation steps and review mechanisms to allow for iterated development of position statements and policies. The developed texts can then be used to promote\nresponsible and ethical practices in academic research. The evolving\nAI landscape necessitates comprehensive, flexible and\nuniversity-wide policies and strategies to ensure research integrity and\nsupport researchers in responsibly using AI.\n\nThe framework we present provides a structured approach with insights\ninto the elements institutions should consider for practical\nimplementation. Central to this process is developing a position\nstatement on AI, reinforcing key principles of research\nintegrity. This statement guides the operationalisation of training\nprograms, communication strategies and infrastructure initiatives. The\nframework outlines an iterative process starting with examining relevant\ncontextual factors, developing the position statement, implementing it,\nand recommends ongoing review and improvements. \n\nWhile other practitioners have produced frameworks for universities\ndealing with AI broadly \\parencite[for undergraduate policy, see][]{luo_critical_2024}, this paper specifically\naddresses the research policy context. The framework serves as a road map for\nresearch institutions to adapt, considering their unique contexts to\ndevelop appropriate strategies and plans. Examples from QUT and Macquarie\nillustrate potential outcomes of a strategic framework process, without\npresenting them as definitive end-points.\n\n\nTesting various strategies within this framework could inform future\nresearch related to identifying the most effective position statements\nand strategies to promote responsible practices and research integrity\nin AI use across institutions. Each organisation must drive\nits own AI agenda, but our framework can initiate and guide\nthat process.\n\n% \\nolinenumbers\n\n\n% \\backmatter\n\n%\\bmhead{List of Abbreviations}\n\n%\\begin{itemize}\n%    \\item[AI] Artificial Intelligence\n%    \\item[API] Application Programming Interface\n%    \\item[COPE] Committee on Publication Ethics\n%    \\item[Generative AI] Generative Artificial Intelligence\n%    \\item[HREC] Human Research Ethics Committee\n%    \\item[IP] Intellectual Property\n%    \\item[IRB] Institutional Review Boards\n%    \\item[KPI] Key Performance Indicators\n%    \\item[OREI] Office of Research Ethics and Integrity\n%    \\item[QUT] Queensland University of Technology\n%    \\item[TEQSA] Tertiary Education Quality Standards Agency\n    \n%\\end{itemize}\n\n% \\bmhead{Supplemental materials}\n\\section*{Supplemental materials}\n\nAttachment 1: Macquarie University's Position statement, available CC-BY at \\url{https://zenodo.org/records/10851623}\n\nAttachment 2: Queensland University of Technology's Position Statement, available CC-BY at\n\\url{https://zenodo.org/records/10971910}\n\nAppendix 1: Macquarie Peer Review Checklist <URL TODO>\n\nWorks Consulted: All papers consulted in preparing this manuscript are available on Zenodo: \\url{https://www.zotero.org/groups/5224649/2023qutmq-a_framework_for_the_responsible_use_of_generative_ai_tools_by_researchers/items}\n\n \n\n\n\\section*{Declarations}\n\n\n\\begin{itemize}\n\n\\item Ethics approval \n\nNot applicable.\n\n\n\\item Availability of data and materials\n\nAll prompts and full chat-logs of sessions with large language models used as part of editing this work are available at https://osf.io/8dxj6/\n\n\\item Competing interests \n\nThe authors have declared that they have no competing interests.\n\n\\item Authors' contributions\n\nAll authors contributed conceptualisation, writing, reviewing and editing.\n\n\n\\item Acknowledgements\n\nThe authors would like to acknowledge the significant contributions of Mark Hooper (mark@trickygoose.training) in developing the graphical representation of the framework and in assisting with earlier revisions of this manuscript. At his request, in this version of the manuscript, we have moved him from an author to acknowledged contributor. We warmly thank him for his time and support on this paper.\n\nSpecial thanks to the following early readers for their feedback and encouragement: Jane Thogersen, Stephanie Bradbury, Matt Bower, Ross McLennan, Sylvie Saab and Jacqueline Phillips. \n\n\\end{itemize}\n\n\\noindent\n% If any of the sections are not relevant to your manuscript, please include the heading and write `Not applicable' for that section. \n\n\\printbibliography\n\n% \\bibliography{references}% common bib file\n%% if required, the content of .bbl file can be included here once bbl is generated\n%%\\input sn-article.bbl\n\n\n\\end{document}\n"
        }
      ],
      "files": [
        {
          "file_name": "main.tex"
        }
      ]
    },
    {
      "uuid": "378c0a85-2c6e-4c30-9e8a-42ad1751c12f",
      "text": " I understand your request for meticulous copy-editing of this academic paper. I'll approach this with careful consideration, highlighting issues rather than making direct corrections, and providing thoughtful discussion of each issue. I'll mark any suggested contributions in bold text, and will focus on flow, clarity, and consistency.\n\n## Functional Decomposition of Tasks\n\n1. **Document Structure Analysis**: Identify the paper's organization and main sections\n2. **Consistency Check**: Identify inconsistencies in terminology, style, and tone\n3. **Flow Analysis**: Assess logical progression of ideas and smooth transitions\n4. **Clarity Assessment**: Identify unclear or ambiguous phrasing\n5. **Conciseness Review**: Identify opportunities for word reduction without losing meaning\n6. **Reference Style Consistency**: Ensure consistent citation format\n7. **Register Verification**: Ensure consistent academic tone throughout\n8. **Formatting Considerations**: Note any formatting inconsistencies\n\n## Document Headers\n\nHere are the main sections of the document:\n\n- Title: \"A University Framework for the Responsible use of Generative AI in Research\"\n- Abstract\n- Introduction\n- Context\n  - Research integrity\n  - Human research ethics\n  - Research degree requirements\n  - Research data management\n  - Sector policies\n  - Terms of service\n- Developing a University Position\n  - Consult across disciplines and research services\n  - Engage AI Researchers\n  - Position Statement\n- Implementation\n  - Communication and engagement strategy\n  - Training and education strategy\n    - Acknowledgement and documentation of use by postgraduate students\n  - Infrastructure and procedural considerations\n- Review\n- Conclusion\n- Supplemental materials\n- Declarations\n\nI'll now proceed with detailed analysis of the document, focusing on the issues you've specified. Would you like me to begin with a particular section or aspect of the paper?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:05:10.352463Z",
          "stop_timestamp": "2025-04-15T06:05:17.196166Z",
          "flags": null,
          "type": "text",
          "text": " I understand your request for meticulous copy-editing of this academic paper. I'll approach this with careful consideration, highlighting issues rather than making direct corrections, and providing thoughtful discussion of each issue. I'll mark any suggested contributions in bold text, and will focus on flow, clarity, and consistency.\n\n## Functional Decomposition of Tasks\n\n1. **Document Structure Analysis**: Identify the paper's organization and main sections\n2. **Consistency Check**: Identify inconsistencies in terminology, style, and tone\n3. **Flow Analysis**: Assess logical progression of ideas and smooth transitions\n4. **Clarity Assessment**: Identify unclear or ambiguous phrasing\n5. **Conciseness Review**: Identify opportunities for word reduction without losing meaning\n6. **Reference Style Consistency**: Ensure consistent citation format\n7. **Register Verification**: Ensure consistent academic tone throughout\n8. **Formatting Considerations**: Note any formatting inconsistencies\n\n## Document Headers\n\nHere are the main sections of the document:\n\n- Title: \"A University Framework for the Responsible use of Generative AI in Research\"\n- Abstract\n- Introduction\n- Context\n  - Research integrity\n  - Human research ethics\n  - Research degree requirements\n  - Research data management\n  - Sector policies\n  - Terms of service\n- Developing a University Position\n  - Consult across disciplines and research services\n  - Engage AI Researchers\n  - Position Statement\n- Implementation\n  - Communication and engagement strategy\n  - Training and education strategy\n    - Acknowledgement and documentation of use by postgraduate students\n  - Infrastructure and procedural considerations\n- Review\n- Conclusion\n- Supplemental materials\n- Declarations\n\nI'll now proceed with detailed analysis of the document, focusing on the issues you've specified. Would you like me to begin with a particular section or aspect of the paper?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:05:18.353707Z",
      "updated_at": "2025-04-15T06:05:18.353707Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "8015e999-bdb3-44e7-8d0c-0813060a0e1d",
      "text": "Let's begin with the intro, please. Would you like me to paste in sections?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:02:38.507255Z",
          "stop_timestamp": "2025-04-15T06:02:38.507255Z",
          "flags": null,
          "type": "text",
          "text": "Let's begin with the intro, please. Would you like me to paste in sections?",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:05:42.555613Z",
      "updated_at": "2025-04-15T06:05:42.555613Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "16282ded-4b99-4d22-9753-811bba4d2ea9",
      "text": " I can work with the introduction as it appears in the document you've already provided, so there's no need for you to paste it again. I'll analyze the introduction section and identify issues with flow, clarity, and consistency. Let me begin.\n\n## Introduction Analysis\n\nThe introduction establishes the paper's purpose: to provide a strategic framework for universities to navigate the use of AI in research. Here are the issues I've identified:\n\n### Flow and Structure Issues:\n\n1. The opening paragraph effectively sets the stage, but the transition to the second paragraph feels slightly abrupt. There's a sudden shift from general context to mentioning specific Australian universities without explanation of why these two were chosen or how they represent the broader landscape.\n\n2. The fourth paragraph mentions that \"most recent studies have focused on the impact of AI on undergraduate teaching...\" but this context feels disconnected from the previous paragraphs which were describing the paper's framework. This important contextual information might be better placed earlier to establish the research gap.\n\n3. The paragraph describing the framework's four layers (numbered list) appears before the reader is given a full understanding of the framework's purpose or significance. Without a visual reference (the figure is mentioned but not yet seen), this enumeration may be difficult to follow.\n\n### Clarity Issues:\n\n1. In paragraph 2, the phrase \"drawn on insights from two Australian universities\" could be more specific about how these insights were gathered (case studies, interviews, policy analysis, etc.).\n\n2. The statement \"guide the delivery of communications, infrastructure, process improvements and training\" doesn't specify who will deliver these elements or to whom they will be delivered.\n\n3. In paragraph 3, \"foundational and adaptable model\" is somewhat vague. It would be clearer to specify what makes the model both foundational and adaptable.\n\n4. The final paragraph explaining the paper's structure uses the phrase \"unconventional approach to the structure\" but doesn't clearly explain what conventional would be or why this approach benefits readers.\n\n### Consistency Issues:\n\n1. The paper uses both \"Macquarie University\" and \"Macquarie\" in the same paragraph, which could be standardized.\n\n2. The terms \"position statement,\" \"guidance note,\" and \"policy\" are all used in the introduction, but their relationships to each other aren't clearly defined until the later paragraphs.\n\n3. The final paragraph of the introduction refers to \"our framework,\" but the ownership and development process of this framework isn't fully clarified until later sections.\n\n### Conciseness Opportunities:\n\n1. The parenthetical explanation \"(Macquarie)\" after \"Macquarie University\" could be omitted if the abbreviation is consistently used thereafter.\n\n2. In paragraph 5, \"The aim of this paper is to provide a structure to develop guidance in this evolving context\" could be more direct: \"This paper provides a structure for developing guidance in this evolving context.\"\n\n3. The final paragraph explaining the paper's organization appears somewhat defensive and could be condensed.\n\n### Suggested Focus Areas:\n\n1. Consider strengthening the introduction by more clearly articulating the research gap this paper fills regarding institutional responses to AI in research.\n\n2. The significance of choosing a principles-based approach could be emphasized earlier to provide a stronger foundation for the framework.\n\n3. The explanation of \"Guidance Note\" vs. \"Policy\" could be clarified earlier to help readers understand why this distinction matters.\n\nWould you like me to provide more specific suggestions for any of these issues, or would you prefer I continue to the next section?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:05:43.352063Z",
          "stop_timestamp": "2025-04-15T06:05:55.559672Z",
          "flags": null,
          "type": "text",
          "text": " I can work with the introduction as it appears in the document you've already provided, so there's no need for you to paste it again. I'll analyze the introduction section and identify issues with flow, clarity, and consistency. Let me begin.\n\n## Introduction Analysis\n\nThe introduction establishes the paper's purpose: to provide a strategic framework for universities to navigate the use of AI in research. Here are the issues I've identified:\n\n### Flow and Structure Issues:\n\n1. The opening paragraph effectively sets the stage, but the transition to the second paragraph feels slightly abrupt. There's a sudden shift from general context to mentioning specific Australian universities without explanation of why these two were chosen or how they represent the broader landscape.\n\n2. The fourth paragraph mentions that \"most recent studies have focused on the impact of AI on undergraduate teaching...\" but this context feels disconnected from the previous paragraphs which were describing the paper's framework. This important contextual information might be better placed earlier to establish the research gap.\n\n3. The paragraph describing the framework's four layers (numbered list) appears before the reader is given a full understanding of the framework's purpose or significance. Without a visual reference (the figure is mentioned but not yet seen), this enumeration may be difficult to follow.\n\n### Clarity Issues:\n\n1. In paragraph 2, the phrase \"drawn on insights from two Australian universities\" could be more specific about how these insights were gathered (case studies, interviews, policy analysis, etc.).\n\n2. The statement \"guide the delivery of communications, infrastructure, process improvements and training\" doesn't specify who will deliver these elements or to whom they will be delivered.\n\n3. In paragraph 3, \"foundational and adaptable model\" is somewhat vague. It would be clearer to specify what makes the model both foundational and adaptable.\n\n4. The final paragraph explaining the paper's structure uses the phrase \"unconventional approach to the structure\" but doesn't clearly explain what conventional would be or why this approach benefits readers.\n\n### Consistency Issues:\n\n1. The paper uses both \"Macquarie University\" and \"Macquarie\" in the same paragraph, which could be standardized.\n\n2. The terms \"position statement,\" \"guidance note,\" and \"policy\" are all used in the introduction, but their relationships to each other aren't clearly defined until the later paragraphs.\n\n3. The final paragraph of the introduction refers to \"our framework,\" but the ownership and development process of this framework isn't fully clarified until later sections.\n\n### Conciseness Opportunities:\n\n1. The parenthetical explanation \"(Macquarie)\" after \"Macquarie University\" could be omitted if the abbreviation is consistently used thereafter.\n\n2. In paragraph 5, \"The aim of this paper is to provide a structure to develop guidance in this evolving context\" could be more direct: \"This paper provides a structure for developing guidance in this evolving context.\"\n\n3. The final paragraph explaining the paper's organization appears somewhat defensive and could be condensed.\n\n### Suggested Focus Areas:\n\n1. Consider strengthening the introduction by more clearly articulating the research gap this paper fills regarding institutional responses to AI in research.\n\n2. The significance of choosing a principles-based approach could be emphasized earlier to provide a stronger foundation for the framework.\n\n3. The explanation of \"Guidance Note\" vs. \"Policy\" could be clarified earlier to help readers understand why this distinction matters.\n\nWould you like me to provide more specific suggestions for any of these issues, or would you prefer I continue to the next section?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:05:59.330549Z",
      "updated_at": "2025-04-15T06:05:59.330549Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "ebfa50bf-3e51-4e3f-a4ee-6fca850aec72",
      "text": "Yes, let's work through specific suggestions. And we might just move things around in the document, since we have many of the things you've asked for later on.",
      "content": [
        {
          "start_timestamp": "2025-04-15T05:54:40.881346Z",
          "stop_timestamp": "2025-04-15T05:54:40.881346Z",
          "flags": null,
          "type": "text",
          "text": "Yes, let's work through specific suggestions. And we might just move things around in the document, since we have many of the things you've asked for later on.",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:07:40.834960Z",
      "updated_at": "2025-04-15T06:07:40.834960Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "bdc49ddd-ff77-4857-8a8e-486cceea41c6",
      "text": " I understand you'd like to work through specific suggestions for the introduction, and that we might move content from later sections to address some of the issues. Let's go through each issue with more specific recommendations:\n\n### Flow and Structure Suggestions:\n\n1. **Abrupt transition to Australian universities**: \n   Consider adding a brief explanation before mentioning the specific universities:\n   \n   **\"This paper draws on insights from two Australian universities that have independently developed frameworks in response to AI challenges. We provide worked examples of how Macquarie University (Macquarie) and Queensland University of Technology (QUT) have followed this framework...\"**\n\n2. **Research gap context placement**:\n   Consider moving the research gap statement earlier, perhaps after the first paragraph:\n   \n   **\"While most recent studies have focused on the impact of AI on undergraduate teaching and learning and assessment, discussions of institutional responses to the responsible use of AI for research are much less common. This paper addresses this gap by proposing a strategic framework for navigating this evolving space and promoting the responsible use of AI in research.\"**\n\n3. **Framework explanation placement**:\n   Consider providing a brief overview of the framework's purpose before listing its layers:\n   \n   **\"Our framework offers a structured approach for institutions to develop guidance on AI in research, arranged in four layers that represent the logical sequence for consideration and implementation:\"**\n\n### Clarity Suggestions:\n\n1. **Specificity about university insights**:\n   Consider clarifying the methodology used:\n   \n   **\"This paper draws on insights from two Australian universities' experiences developing and implementing AI frameworks, including policy development processes, consultation strategies, and implementation challenges.\"**\n\n2. **Delivery clarification**:\n   Specify the audience and implementers:\n   \n   **\"...guide institutional leaders and research offices in delivering communications, infrastructure, process improvements and training to support researchers in the responsible use of AI.\"**\n\n3. **Foundational and adaptable model**:\n   Clarify these attributes:\n   \n   **\"Our framework serves as a foundational model that addresses core ethical principles while remaining adaptable to different institutional contexts, disciplinary needs, and the rapidly evolving AI landscape.\"**\n\n4. **Paper structure explanation**:\n   Consider a more straightforward explanation:\n   \n   **\"The structure of this paper follows the four layers of our framework, with each section corresponding to a layer: context, position, implementation and review. This organization allows readers to follow the framework's logic while understanding both the theoretical foundations and practical applications.\"**\n\n### Consistency Suggestions:\n\n1. **University naming**:\n   Choose either \"Macquarie University\" throughout or establish \"Macquarie\" as the abbreviation once and use it consistently.\n\n2. **Terminology clarification**:\n   Consider adding a brief explanation of these terms early in the introduction:\n   \n   **\"In this paper, the term 'framework' refers to the overarching structure guiding institutional governance, strategic direction, and adaptation to AI-related developments. 'Policy,' on the other hand, refers to specific institutional rules and formal guidelines that establish compliance requirements for researchers around their use of AI.\"**\n\n3. **Framework ownership**:\n   Clarify the development process earlier:\n   \n   **\"Our framework, developed through analysis of current literature and practical implementation at two universities, serves as a foundational and adaptable model...\"**\n\n### Conciseness Suggestions:\n\n1. **Abbreviation handling**:\n   Introduce Macquarie University (Macquarie) once, then use the abbreviation consistently.\n\n2. **More direct aim statement**:\n   Revise to: **\"This paper provides a structure for developing guidance in this evolving context.\"**\n\n3. **Streamlined structure explanation**:\n   Consider: **\"The paper's structure follows the framework's four layers, making the conceptual organization transparent and accessible.\"**\n\nThese suggestions aim to enhance the introduction while minimizing disruption to your existing content. Would you like to focus on implementing any particular suggestions first, or should we continue analyzing the next section?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:07:41.600707Z",
          "stop_timestamp": "2025-04-15T06:07:56.814224Z",
          "flags": null,
          "type": "text",
          "text": " I understand you'd like to work through specific suggestions for the introduction, and that we might move content from later sections to address some of the issues. Let's go through each issue with more specific recommendations:\n\n### Flow and Structure Suggestions:\n\n1. **Abrupt transition to Australian universities**: \n   Consider adding a brief explanation before mentioning the specific universities:\n   \n   **\"This paper draws on insights from two Australian universities that have independently developed frameworks in response to AI challenges. We provide worked examples of how Macquarie University (Macquarie) and Queensland University of Technology (QUT) have followed this framework...\"**\n\n2. **Research gap context placement**:\n   Consider moving the research gap statement earlier, perhaps after the first paragraph:\n   \n   **\"While most recent studies have focused on the impact of AI on undergraduate teaching and learning and assessment, discussions of institutional responses to the responsible use of AI for research are much less common. This paper addresses this gap by proposing a strategic framework for navigating this evolving space and promoting the responsible use of AI in research.\"**\n\n3. **Framework explanation placement**:\n   Consider providing a brief overview of the framework's purpose before listing its layers:\n   \n   **\"Our framework offers a structured approach for institutions to develop guidance on AI in research, arranged in four layers that represent the logical sequence for consideration and implementation:\"**\n\n### Clarity Suggestions:\n\n1. **Specificity about university insights**:\n   Consider clarifying the methodology used:\n   \n   **\"This paper draws on insights from two Australian universities' experiences developing and implementing AI frameworks, including policy development processes, consultation strategies, and implementation challenges.\"**\n\n2. **Delivery clarification**:\n   Specify the audience and implementers:\n   \n   **\"...guide institutional leaders and research offices in delivering communications, infrastructure, process improvements and training to support researchers in the responsible use of AI.\"**\n\n3. **Foundational and adaptable model**:\n   Clarify these attributes:\n   \n   **\"Our framework serves as a foundational model that addresses core ethical principles while remaining adaptable to different institutional contexts, disciplinary needs, and the rapidly evolving AI landscape.\"**\n\n4. **Paper structure explanation**:\n   Consider a more straightforward explanation:\n   \n   **\"The structure of this paper follows the four layers of our framework, with each section corresponding to a layer: context, position, implementation and review. This organization allows readers to follow the framework's logic while understanding both the theoretical foundations and practical applications.\"**\n\n### Consistency Suggestions:\n\n1. **University naming**:\n   Choose either \"Macquarie University\" throughout or establish \"Macquarie\" as the abbreviation once and use it consistently.\n\n2. **Terminology clarification**:\n   Consider adding a brief explanation of these terms early in the introduction:\n   \n   **\"In this paper, the term 'framework' refers to the overarching structure guiding institutional governance, strategic direction, and adaptation to AI-related developments. 'Policy,' on the other hand, refers to specific institutional rules and formal guidelines that establish compliance requirements for researchers around their use of AI.\"**\n\n3. **Framework ownership**:\n   Clarify the development process earlier:\n   \n   **\"Our framework, developed through analysis of current literature and practical implementation at two universities, serves as a foundational and adaptable model...\"**\n\n### Conciseness Suggestions:\n\n1. **Abbreviation handling**:\n   Introduce Macquarie University (Macquarie) once, then use the abbreviation consistently.\n\n2. **More direct aim statement**:\n   Revise to: **\"This paper provides a structure for developing guidance in this evolving context.\"**\n\n3. **Streamlined structure explanation**:\n   Consider: **\"The paper's structure follows the framework's four layers, making the conceptual organization transparent and accessible.\"**\n\nThese suggestions aim to enhance the introduction while minimizing disruption to your existing content. Would you like to focus on implementing any particular suggestions first, or should we continue analyzing the next section?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:07:59.327312Z",
      "updated_at": "2025-04-15T06:07:59.327312Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "f3fcf908-855a-4157-9cf8-d685795c65e1",
      "text": "Ok, but if we're adding words, we need to remove more.",
      "content": [
        {
          "start_timestamp": "2025-04-15T05:48:13.081649Z",
          "stop_timestamp": "2025-04-15T05:48:13.081649Z",
          "flags": null,
          "type": "text",
          "text": "Ok, but if we're adding words, we need to remove more.",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:08:08.268527Z",
      "updated_at": "2025-04-15T06:08:08.268527Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "1f8cc5b3-0986-46b5-80fc-17f88691d72b",
      "text": " You're absolutely right - we need to be mindful of word count. Let's refine the suggestions to ensure we're removing more words than we're adding. I'll focus on identifying redundancies and opportunities for significant compression in the introduction.\n\n### Major Compression Opportunities:\n\n1. **First paragraph redundancies**:\n   The current opening contains repetitive phrases about \"adapting\" practices. Consider condensing:\n   \n   FROM: \"Generative Artificial Intelligence (AI) poses both opportunities and risks for the integrity and quality of research. To support researchers in using AI responsibly, institutions must adapt their research governance and management practices to mitigate the risks of AI while remaining flexible to allow for its innovative use in research.\"\n   \n   TO: **\"Generative Artificial Intelligence (AI) poses both opportunities and risks for research integrity. Institutions must balance risk mitigation with flexibility to allow innovative AI use in research.\"** \n   (Reduction of ~30 words)\n\n2. **Second paragraph efficiency**:\n   FROM: \"This paper draws on insights from two Australian universities to provide a strategic framework for navigating this evolving space and promoting the responsible use of AI in research. We provide worked examples of how Macquarie University (Macquarie) and Queensland University of Technology (QUT) have followed this framework to develop a principles-based position statement and guide the delivery of communications, infrastructure, process improvements and training to support researchers in the responsible use of AI.\"\n   \n   TO: **\"Drawing on experiences from Macquarie University and Queensland University of Technology (QUT), we provide a strategic framework for responsible AI use in research. These cases demonstrate how to develop principles-based position statements that guide communications, infrastructure, processes and training.\"** \n   (Reduction of ~35 words)\n\n3. **Third paragraph compression**:\n   FROM: \"Our framework, based on current literature and practical implementation plans, serves as a foundational and adaptable model offering clarity and direction for research institutions.\"\n   \n   This sentence could be removed entirely or merged with another paragraph. (Reduction of ~24 words)\n\n4. **Research gap paragraph**:\n   FROM: \"While most recent studies have focused on the impact of AI on undergraduate teaching and learning and assessment, discussions of institutional responses to the responsible use of AI for research are much less common. Universities are uniquely placed to instigate proactive measures to mitigate serious risks posed by AI, such as creating fake or unreproducible research, which threaten research credibility and public trust. Our paper emphasises developing an institutional position statement that applies research integrity principles to AI challenges, serving as common ground for mobilising training, communication and infrastructure initiatives to meet specific institutional and research needs.\"\n   \n   TO: **\"Unlike the abundant research on AI in teaching, institutional responses to AI in research remain underdeveloped. Universities must address serious risks including fake or unreproducible research that threaten credibility. Our paper proposes institutional position statements that apply integrity principles to AI challenges, providing common ground for training, communication and infrastructure initiatives.\"** \n   (Reduction of ~45 words)\n\n5. **Fifth paragraph streamlining**:\n   FROM: \"The aim of this paper is to provide a structure to develop guidance in this evolving context. The literature and discussion in this paper should help inform institutional decision-making related to governance and policy implementation. This paper is a case study, allowing readers to apply our lessons learned and models in the development of their own institutional policies, using the framework we extrapolated from our efforts.\"\n   \n   TO: **\"This paper provides guidance for institutional decision-making on AI governance in a rapidly evolving landscape. As a case study, it allows readers to apply our framework and lessons to their own institutional policies.\"** \n   (Reduction of ~40 words)\n\n6. **Framework explanation efficiency**:\n   The numbered list explaining the framework's four layers could be made more concise by removing repetitive phrases like \"describes\" at the beginning of each item.\n\n7. **Final paragraph elimination**:\n   The final paragraph explaining the paper's unconventional structure could potentially be removed entirely, as the structure becomes evident through reading. (Reduction of ~150 words)\n\nThese compression opportunities could reduce the introduction by approximately 300+ words while preserving the essential content. Would you like me to apply this approach to any specific paragraphs in the introduction to create a more polished, concise version?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:08:09.078145Z",
          "stop_timestamp": "2025-04-15T06:08:24.159839Z",
          "flags": null,
          "type": "text",
          "text": " You're absolutely right - we need to be mindful of word count. Let's refine the suggestions to ensure we're removing more words than we're adding. I'll focus on identifying redundancies and opportunities for significant compression in the introduction.\n\n### Major Compression Opportunities:\n\n1. **First paragraph redundancies**:\n   The current opening contains repetitive phrases about \"adapting\" practices. Consider condensing:\n   \n   FROM: \"Generative Artificial Intelligence (AI) poses both opportunities and risks for the integrity and quality of research. To support researchers in using AI responsibly, institutions must adapt their research governance and management practices to mitigate the risks of AI while remaining flexible to allow for its innovative use in research.\"\n   \n   TO: **\"Generative Artificial Intelligence (AI) poses both opportunities and risks for research integrity. Institutions must balance risk mitigation with flexibility to allow innovative AI use in research.\"** \n   (Reduction of ~30 words)\n\n2. **Second paragraph efficiency**:\n   FROM: \"This paper draws on insights from two Australian universities to provide a strategic framework for navigating this evolving space and promoting the responsible use of AI in research. We provide worked examples of how Macquarie University (Macquarie) and Queensland University of Technology (QUT) have followed this framework to develop a principles-based position statement and guide the delivery of communications, infrastructure, process improvements and training to support researchers in the responsible use of AI.\"\n   \n   TO: **\"Drawing on experiences from Macquarie University and Queensland University of Technology (QUT), we provide a strategic framework for responsible AI use in research. These cases demonstrate how to develop principles-based position statements that guide communications, infrastructure, processes and training.\"** \n   (Reduction of ~35 words)\n\n3. **Third paragraph compression**:\n   FROM: \"Our framework, based on current literature and practical implementation plans, serves as a foundational and adaptable model offering clarity and direction for research institutions.\"\n   \n   This sentence could be removed entirely or merged with another paragraph. (Reduction of ~24 words)\n\n4. **Research gap paragraph**:\n   FROM: \"While most recent studies have focused on the impact of AI on undergraduate teaching and learning and assessment, discussions of institutional responses to the responsible use of AI for research are much less common. Universities are uniquely placed to instigate proactive measures to mitigate serious risks posed by AI, such as creating fake or unreproducible research, which threaten research credibility and public trust. Our paper emphasises developing an institutional position statement that applies research integrity principles to AI challenges, serving as common ground for mobilising training, communication and infrastructure initiatives to meet specific institutional and research needs.\"\n   \n   TO: **\"Unlike the abundant research on AI in teaching, institutional responses to AI in research remain underdeveloped. Universities must address serious risks including fake or unreproducible research that threaten credibility. Our paper proposes institutional position statements that apply integrity principles to AI challenges, providing common ground for training, communication and infrastructure initiatives.\"** \n   (Reduction of ~45 words)\n\n5. **Fifth paragraph streamlining**:\n   FROM: \"The aim of this paper is to provide a structure to develop guidance in this evolving context. The literature and discussion in this paper should help inform institutional decision-making related to governance and policy implementation. This paper is a case study, allowing readers to apply our lessons learned and models in the development of their own institutional policies, using the framework we extrapolated from our efforts.\"\n   \n   TO: **\"This paper provides guidance for institutional decision-making on AI governance in a rapidly evolving landscape. As a case study, it allows readers to apply our framework and lessons to their own institutional policies.\"** \n   (Reduction of ~40 words)\n\n6. **Framework explanation efficiency**:\n   The numbered list explaining the framework's four layers could be made more concise by removing repetitive phrases like \"describes\" at the beginning of each item.\n\n7. **Final paragraph elimination**:\n   The final paragraph explaining the paper's unconventional structure could potentially be removed entirely, as the structure becomes evident through reading. (Reduction of ~150 words)\n\nThese compression opportunities could reduce the introduction by approximately 300+ words while preserving the essential content. Would you like me to apply this approach to any specific paragraphs in the introduction to create a more polished, concise version?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:08:27.277295Z",
      "updated_at": "2025-04-15T06:08:27.277295Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "d03153c5-fab8-4fa7-9505-80c13f3d7e8e",
      "text": "Ok. Those comments were effective. \n\nOnto the next section and subsection\n\n\n```\n\n\\section{Context}\\label{sec2}\n\nPolicy development by research institutions is shaped by external and internal contextual environments, and in the case of AI, the relationships between these environments are complex: the increasing use of AI is creating new ways of conducting research; introducing new techniques, methods and complex datasets, and challenges around authorship, integrity and judgement. This poses challenges to accepted research practices and requires institutions to reassess institutional policy and infrastructure to support researchers and university staff in research integrity, human research ethics, research degree requirement, data management, sector policies and terms of service. \n\n\n\\subsection{Research integrity}\\label{research-ethics-and-integrity} \n\nIn Australia, the NHMRC \\textit{Code for the Responsible Conduct of Research} \\parencite{nhmrc_australian_2018} establishes principles that, extended, also guide responsible AI use in research, notably rigour, transparency, and accountability. Researchers must document and transparently disclose AI integration into their research, detailing AI tools, methodologies, and potential biases \\parencite[Principles 2, 3]{nhmrc_australian_2018}. Comprehensive records, including logs of AI interactions, prompts, and post-processing methods, should be securely maintained and made accessible to promote reproducibility.\n\nDisclosure practices vary widely across disciplines and institutions. Many journals currently require minimal disclosure following COPE guidelines \\parencite{committee_on_publication_ethics_cope_2024}. However, some have started mandating detailed disclosures. For instance, APA requires authors to explicitly document AI use in their methods sections and to provide full AI outputs as supplemental materials \\parencite*{american_psychological_association_apa_2023}. Practical disciplinary trends include attaching AI-generated logs and prompts as supplemental material \\parencite[e.g.][]{butson_ai_2024, shiferaw_spatial_2024}.\n```",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:03:39.894485Z",
          "stop_timestamp": "2025-04-15T06:03:39.894485Z",
          "flags": null,
          "type": "text",
          "text": "Ok. Those comments were effective. \n\nOnto the next section and subsection\n\n\n```\n\n\\section{Context}\\label{sec2}\n\nPolicy development by research institutions is shaped by external and internal contextual environments, and in the case of AI, the relationships between these environments are complex: the increasing use of AI is creating new ways of conducting research; introducing new techniques, methods and complex datasets, and challenges around authorship, integrity and judgement. This poses challenges to accepted research practices and requires institutions to reassess institutional policy and infrastructure to support researchers and university staff in research integrity, human research ethics, research degree requirement, data management, sector policies and terms of service. \n\n\n\\subsection{Research integrity}\\label{research-ethics-and-integrity} \n\nIn Australia, the NHMRC \\textit{Code for the Responsible Conduct of Research} \\parencite{nhmrc_australian_2018} establishes principles that, extended, also guide responsible AI use in research, notably rigour, transparency, and accountability. Researchers must document and transparently disclose AI integration into their research, detailing AI tools, methodologies, and potential biases \\parencite[Principles 2, 3]{nhmrc_australian_2018}. Comprehensive records, including logs of AI interactions, prompts, and post-processing methods, should be securely maintained and made accessible to promote reproducibility.\n\nDisclosure practices vary widely across disciplines and institutions. Many journals currently require minimal disclosure following COPE guidelines \\parencite{committee_on_publication_ethics_cope_2024}. However, some have started mandating detailed disclosures. For instance, APA requires authors to explicitly document AI use in their methods sections and to provide full AI outputs as supplemental materials \\parencite*{american_psychological_association_apa_2023}. Practical disciplinary trends include attaching AI-generated logs and prompts as supplemental material \\parencite[e.g.][]{butson_ai_2024, shiferaw_spatial_2024}.\n```",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:15:54.058683Z",
      "updated_at": "2025-04-15T06:15:54.058683Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "3ff0439c-36a7-4eed-bba3-6e9b0b034aaf",
      "text": " ## Analysis of Context Section Introduction\n\n### Flow and Structure Issues:\n\n1. The context section opens with a complex, lengthy sentence that introduces multiple concepts at once, making it difficult to follow. The sentence attempts to establish the complex relationships between external and internal environments but may overwhelm the reader with too many ideas.\n\n2. The transition from the broad context introduction to the specific subsection on research integrity is somewhat abrupt. There could be a clearer link explaining how research integrity fits into the overall context framework.\n\n### Clarity Issues:\n\n1. The phrase \"in the case of AI, the relationships between these environments are complex\" is somewhat vague. It would be more helpful to specify exactly how these relationships are complex.\n\n2. The list of areas (research integrity, human research ethics, etc.) at the end of the first paragraph appears without explanation of how these connect to the main argument about contextual environments.\n\n3. The term \"extended\" in the phrase \"establishes principles that, extended, also guide responsible AI use\" is unclear - it's not immediately evident what this extension entails or who is doing the extending.\n\n### Consistency Issues:\n\n1. The section uses both \"AI\" and \"artificial intelligence\" interchangeably, which could be standardized.\n\n### Conciseness Opportunities:\n\n1. The opening paragraph contains repetitive phrasing around \"challenges\" - \"poses challenges\" and \"challenges around\" appear in close proximity.\n\n2. The phrase \"reassess institutional policy and infrastructure\" contains redundant use of \"institutional\" since it was established earlier that the focus is on research institutions.\n\n## Analysis of Research Integrity Subsection\n\n### Flow and Structure Issues:\n\n1. The paragraph on disclosure practices introduces specific examples (APA) without first establishing why disclosure is important in the AI context specifically.\n\n2. The final paragraph appears to be missing from the excerpt you provided, making it difficult to assess the overall flow of this subsection.\n\n### Clarity Issues:\n\n1. The reference to \"COPE guidelines\" assumes reader familiarity with this acronym and its significance, which may not be the case for all readers.\n\n2. The statement about \"Practical disciplinary trends\" would benefit from clearer explanation of what these disciplines are and why their practices are noteworthy.\n\n### Conciseness Opportunities:\n\n1. The phrase \"Researchers must document and transparently disclose AI integration into their research, detailing AI tools, methodologies, and potential biases\" contains some repetition that could be condensed.\n\n### Suggested Revisions:\n\nFor the context introduction paragraph:\n**\"Policy development by research institutions is shaped by both external and internal environments. AI introduces new research techniques, methods and datasets while raising questions about authorship, integrity and judgment. These developments challenge established practices, requiring institutions to update policies and infrastructure in areas including research integrity, ethics, degree requirements, data management, sector policies and terms of service.\"**\n\nFor the research integrity introduction:\n**\"The NHMRC Code for the Responsible Conduct of Research provides principles applicable to AI in research: rigour, transparency, and accountability. When using AI, researchers must document tools, methods, potential biases, and maintain records of interactions, prompts, and post-processing to ensure reproducibility.\"**\n\nWould you like me to continue with specific suggestions for the disclosure practices paragraph as well?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:15:55.034188Z",
          "stop_timestamp": "2025-04-15T06:16:07.181560Z",
          "flags": null,
          "type": "text",
          "text": " ## Analysis of Context Section Introduction\n\n### Flow and Structure Issues:\n\n1. The context section opens with a complex, lengthy sentence that introduces multiple concepts at once, making it difficult to follow. The sentence attempts to establish the complex relationships between external and internal environments but may overwhelm the reader with too many ideas.\n\n2. The transition from the broad context introduction to the specific subsection on research integrity is somewhat abrupt. There could be a clearer link explaining how research integrity fits into the overall context framework.\n\n### Clarity Issues:\n\n1. The phrase \"in the case of AI, the relationships between these environments are complex\" is somewhat vague. It would be more helpful to specify exactly how these relationships are complex.\n\n2. The list of areas (research integrity, human research ethics, etc.) at the end of the first paragraph appears without explanation of how these connect to the main argument about contextual environments.\n\n3. The term \"extended\" in the phrase \"establishes principles that, extended, also guide responsible AI use\" is unclear - it's not immediately evident what this extension entails or who is doing the extending.\n\n### Consistency Issues:\n\n1. The section uses both \"AI\" and \"artificial intelligence\" interchangeably, which could be standardized.\n\n### Conciseness Opportunities:\n\n1. The opening paragraph contains repetitive phrasing around \"challenges\" - \"poses challenges\" and \"challenges around\" appear in close proximity.\n\n2. The phrase \"reassess institutional policy and infrastructure\" contains redundant use of \"institutional\" since it was established earlier that the focus is on research institutions.\n\n## Analysis of Research Integrity Subsection\n\n### Flow and Structure Issues:\n\n1. The paragraph on disclosure practices introduces specific examples (APA) without first establishing why disclosure is important in the AI context specifically.\n\n2. The final paragraph appears to be missing from the excerpt you provided, making it difficult to assess the overall flow of this subsection.\n\n### Clarity Issues:\n\n1. The reference to \"COPE guidelines\" assumes reader familiarity with this acronym and its significance, which may not be the case for all readers.\n\n2. The statement about \"Practical disciplinary trends\" would benefit from clearer explanation of what these disciplines are and why their practices are noteworthy.\n\n### Conciseness Opportunities:\n\n1. The phrase \"Researchers must document and transparently disclose AI integration into their research, detailing AI tools, methodologies, and potential biases\" contains some repetition that could be condensed.\n\n### Suggested Revisions:\n\nFor the context introduction paragraph:\n**\"Policy development by research institutions is shaped by both external and internal environments. AI introduces new research techniques, methods and datasets while raising questions about authorship, integrity and judgment. These developments challenge established practices, requiring institutions to update policies and infrastructure in areas including research integrity, ethics, degree requirements, data management, sector policies and terms of service.\"**\n\nFor the research integrity introduction:\n**\"The NHMRC Code for the Responsible Conduct of Research provides principles applicable to AI in research: rigour, transparency, and accountability. When using AI, researchers must document tools, methods, potential biases, and maintain records of interactions, prompts, and post-processing to ensure reproducibility.\"**\n\nWould you like me to continue with specific suggestions for the disclosure practices paragraph as well?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:16:07.982058Z",
      "updated_at": "2025-04-15T06:16:07.982058Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "d4686496-15c7-4ff2-9f6c-c81955a5f41f",
      "text": "First revision good. We're rejecting the second. Please proceed.",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:16:39.734553Z",
          "stop_timestamp": "2025-04-15T06:16:39.734553Z",
          "flags": null,
          "type": "text",
          "text": "First revision good. We're rejecting the second. Please proceed.",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:18:33.550288Z",
      "updated_at": "2025-04-15T06:18:33.550288Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "f9203c54-3a5e-419a-b45a-48c553a3a3a2",
      "text": " I understand that the first suggested revision for the context introduction was helpful, but the second for research integrity was not suitable. Let's continue with the analysis of the rest of the section.\n\n## Further Analysis of Research Integrity Subsection\n\n### Disclosure Practices Paragraph:\n\n#### Flow and Structure Issues:\n1. The transition from general NHMRC principles to specific disclosure practices is somewhat abrupt without explaining the connection between transparency principles and disclosure requirements.\n\n2. The examples (COPE, APA) are presented sequentially without a clear indication of why these specific examples were chosen or how they relate to each other in a progression of disclosure requirements.\n\n#### Clarity Issues:\n1. The phrase \"practical disciplinary trends\" doesn't clearly explain which disciplines are being referenced or why their practices constitute \"trends.\"\n\n2. The parenthetical reference to Butson and Shiferaw doesn't explain what these examples demonstrate about disclosure practices.\n\n#### Conciseness Opportunities:\n1. \"Many journals currently require minimal disclosure following COPE guidelines\" could be more direct.\n\n#### Suggested Revision for Disclosure Practices:\n**\"Disclosure practices for AI use vary widely. While many journals follow minimal COPE guidelines, others like APA require explicit documentation in methods sections with complete AI outputs as supplementary materials. Recent publications have established precedent by attaching AI logs and prompts as appendices.\"**\n\nThis revision:\n- Connects more logically to the previous paragraph on transparency\n- Establishes a progression from minimal to more comprehensive disclosure\n- Clarifies the purpose of the example publications\n- Reduces word count by approximately 30%\n\nWould you like me to continue with the analysis of other paragraphs in this section that weren't included in your excerpt, or would you prefer to move to another section?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:18:34.756590Z",
          "stop_timestamp": "2025-04-15T06:18:41.304021Z",
          "flags": null,
          "type": "text",
          "text": " I understand that the first suggested revision for the context introduction was helpful, but the second for research integrity was not suitable. Let's continue with the analysis of the rest of the section.\n\n## Further Analysis of Research Integrity Subsection\n\n### Disclosure Practices Paragraph:\n\n#### Flow and Structure Issues:\n1. The transition from general NHMRC principles to specific disclosure practices is somewhat abrupt without explaining the connection between transparency principles and disclosure requirements.\n\n2. The examples (COPE, APA) are presented sequentially without a clear indication of why these specific examples were chosen or how they relate to each other in a progression of disclosure requirements.\n\n#### Clarity Issues:\n1. The phrase \"practical disciplinary trends\" doesn't clearly explain which disciplines are being referenced or why their practices constitute \"trends.\"\n\n2. The parenthetical reference to Butson and Shiferaw doesn't explain what these examples demonstrate about disclosure practices.\n\n#### Conciseness Opportunities:\n1. \"Many journals currently require minimal disclosure following COPE guidelines\" could be more direct.\n\n#### Suggested Revision for Disclosure Practices:\n**\"Disclosure practices for AI use vary widely. While many journals follow minimal COPE guidelines, others like APA require explicit documentation in methods sections with complete AI outputs as supplementary materials. Recent publications have established precedent by attaching AI logs and prompts as appendices.\"**\n\nThis revision:\n- Connects more logically to the previous paragraph on transparency\n- Establishes a progression from minimal to more comprehensive disclosure\n- Clarifies the purpose of the example publications\n- Reduces word count by approximately 30%\n\nWould you like me to continue with the analysis of other paragraphs in this section that weren't included in your excerpt, or would you prefer to move to another section?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:18:41.777035Z",
      "updated_at": "2025-04-15T06:18:41.777035Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "eb571111-1dca-4a50-a68e-58ce4013d3ec",
      "text": "\\subsubsection{Acknowledgement, disclosure, and documentation}\nAcknowledging AI use separately from general acknowledgements, such as under its own heading akin to funding or conflicts of interest, can clarify the extent and role of AI contributions. Simply acknowledging AI use is insufficient for rigorous peer assessment; detailed disclosure, proportional to the AI involvement, should include specific prompts, model versions, and outputs \\parencite*{ballsun-stanton_briefing_2024}. While no consistent standard exists yet across journals or disciplines, Macquarie and QUT have initiated comprehensive requirements for postgraduate students. Macquarie mandates documentation of AI use in each student's thesis, including interactions and supervisory approval, while QUT requires detailed written disclosure of AI use within theses \\parencite{queensland_university_of_technology_ai_2025}.\nHowever, pragmatic concerns about the practicality of these standards highlight implementation challenges. The required `active supervision and written consent' may be burdensome; thus, institutions must balance rigorous expectations with realistic supervisory practices. Both Macquarie and QUT acknowledge this tension, placing accountability largely on supervisors while recognizing a need for clearer norms and ongoing education. \nTherefore, effective, ethical, and responsible AI integration in research necessitates explicit institutional guardrails and ongoing training aligned with evolving ethical standards. Institutions should support researchers in applying FAIR principles \\parencite[see][]{wilkinson_fair_2016}, making AI-related data and methods transparent to strengthen research integrity and reproducibility. Rather than banning generative AI\u2014which risks unintended shadow use, institutions should proactively equip researchers with the skills to critically evaluate and transparently disclose AI involvement, addressing ethical challenges while leveraging AI\u2019s potential for scholarly innovation \\parencite{sadasivan_can_2023, ben_jabeur_artificial_2023}. An adaptable disclosure checklist supporting discipline-specific guidelines is provided in Appendix 1 for practical guidance across diverse research contexts.",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:06:30.260779Z",
          "stop_timestamp": "2025-04-15T06:06:30.260779Z",
          "flags": null,
          "type": "text",
          "text": "\\subsubsection{Acknowledgement, disclosure, and documentation}\nAcknowledging AI use separately from general acknowledgements, such as under its own heading akin to funding or conflicts of interest, can clarify the extent and role of AI contributions. Simply acknowledging AI use is insufficient for rigorous peer assessment; detailed disclosure, proportional to the AI involvement, should include specific prompts, model versions, and outputs \\parencite*{ballsun-stanton_briefing_2024}. While no consistent standard exists yet across journals or disciplines, Macquarie and QUT have initiated comprehensive requirements for postgraduate students. Macquarie mandates documentation of AI use in each student's thesis, including interactions and supervisory approval, while QUT requires detailed written disclosure of AI use within theses \\parencite{queensland_university_of_technology_ai_2025}.\nHowever, pragmatic concerns about the practicality of these standards highlight implementation challenges. The required `active supervision and written consent' may be burdensome; thus, institutions must balance rigorous expectations with realistic supervisory practices. Both Macquarie and QUT acknowledge this tension, placing accountability largely on supervisors while recognizing a need for clearer norms and ongoing education. \nTherefore, effective, ethical, and responsible AI integration in research necessitates explicit institutional guardrails and ongoing training aligned with evolving ethical standards. Institutions should support researchers in applying FAIR principles \\parencite[see][]{wilkinson_fair_2016}, making AI-related data and methods transparent to strengthen research integrity and reproducibility. Rather than banning generative AI\u2014which risks unintended shadow use, institutions should proactively equip researchers with the skills to critically evaluate and transparently disclose AI involvement, addressing ethical challenges while leveraging AI\u2019s potential for scholarly innovation \\parencite{sadasivan_can_2023, ben_jabeur_artificial_2023}. An adaptable disclosure checklist supporting discipline-specific guidelines is provided in Appendix 1 for practical guidance across diverse research contexts.",
          "citations": []
        }
      ],
      "sender": "human",
      "created_at": "2025-04-15T06:19:36.633533Z",
      "updated_at": "2025-04-15T06:19:36.633533Z",
      "attachments": [],
      "files": []
    },
    {
      "uuid": "fe131907-7a23-4978-a63b-a2833a180eff",
      "text": " ## Analysis of \"Acknowledgement, disclosure, and documentation\" Subsection\n\n### Flow and Structure Issues:\n\n1. The subsection begins directly with specific methods of acknowledgment without first establishing why acknowledgment is distinct from the disclosure practices discussed in the previous section. This creates a slightly disjointed transition.\n\n2. The final paragraph jumps to broad recommendations before fully resolving the tensions identified in the previous paragraph, creating a somewhat hasty conclusion.\n\n3. The mention of Appendix 1 at the very end introduces a new element without adequate preparation or explanation of how it relates to the preceding discussion.\n\n### Clarity Issues:\n\n1. The phrase \"active supervision and written consent\" appears in quotation marks but without clear attribution to its source (presumably from one of the university policies, but this isn't explicitly stated).\n\n2. The term \"FAIR principles\" is introduced with a citation but without a brief explanation of what these principles entail, which may be unfamiliar to some readers.\n\n3. The concept of \"shadow use\" is introduced without definition, assuming reader familiarity with this term.\n\n### Consistency Issues:\n\n1. There's inconsistent punctuation with quotation marks - single quotes in \"active supervision and written consent\" but regular quotations elsewhere.\n\n2. The paragraph mentions both \"Macquarie\" and \"QUT\" without restating their full names, which may need consistency with how they were introduced earlier.\n\n### Conciseness Opportunities:\n\n1. \"Acknowledging AI use separately from general acknowledgements, such as under its own heading akin to funding or conflicts of interest\" contains redundant phrasing about separation.\n\n2. The final paragraph contains several conjunctive phrases (\"effective, ethical, and responsible\") that could be streamlined.\n\n3. The concluding sentence about the appendix could be integrated more efficiently.\n\n### Suggested Revisions:\n\nFor paragraph 1:\n**\"A dedicated section for AI disclosure, similar to funding statements or conflicts of interest, helps clarify AI contributions. Rigorous disclosure should be proportional to AI involvement, including specific prompts, model versions, and outputs. While journal standards vary, both Macquarie and QUT now require comprehensive documentation for postgraduate theses, including supervisory approval.\"**\n\nFor paragraph 2:\n**\"Implementation challenges exist, particularly around the practicality of 'active supervision and written consent' requirements. Both universities acknowledge this tension, placing primary accountability on supervisors while recognizing the need for clearer norms.\"**\n\nFor paragraph 3:\n**\"Responsible AI integration requires institutional guardrails and ongoing training. Applying FAIR principles to AI use enhances research transparency and reproducibility. Rather than prohibitions that might drive shadow use, institutions should equip researchers to evaluate and disclose AI contributions ethically. Appendix 1 provides an adaptable disclosure checklist for diverse research contexts.\"**\n\nThese revisions maintain the key content while improving flow, clarity, and conciseness. Would you like me to explain any particular aspect of these suggestions in more detail?",
      "content": [
        {
          "start_timestamp": "2025-04-15T06:19:38.109965Z",
          "stop_timestamp": "2025-04-15T06:19:49.797693Z",
          "flags": null,
          "type": "text",
          "text": " ## Analysis of \"Acknowledgement, disclosure, and documentation\" Subsection\n\n### Flow and Structure Issues:\n\n1. The subsection begins directly with specific methods of acknowledgment without first establishing why acknowledgment is distinct from the disclosure practices discussed in the previous section. This creates a slightly disjointed transition.\n\n2. The final paragraph jumps to broad recommendations before fully resolving the tensions identified in the previous paragraph, creating a somewhat hasty conclusion.\n\n3. The mention of Appendix 1 at the very end introduces a new element without adequate preparation or explanation of how it relates to the preceding discussion.\n\n### Clarity Issues:\n\n1. The phrase \"active supervision and written consent\" appears in quotation marks but without clear attribution to its source (presumably from one of the university policies, but this isn't explicitly stated).\n\n2. The term \"FAIR principles\" is introduced with a citation but without a brief explanation of what these principles entail, which may be unfamiliar to some readers.\n\n3. The concept of \"shadow use\" is introduced without definition, assuming reader familiarity with this term.\n\n### Consistency Issues:\n\n1. There's inconsistent punctuation with quotation marks - single quotes in \"active supervision and written consent\" but regular quotations elsewhere.\n\n2. The paragraph mentions both \"Macquarie\" and \"QUT\" without restating their full names, which may need consistency with how they were introduced earlier.\n\n### Conciseness Opportunities:\n\n1. \"Acknowledging AI use separately from general acknowledgements, such as under its own heading akin to funding or conflicts of interest\" contains redundant phrasing about separation.\n\n2. The final paragraph contains several conjunctive phrases (\"effective, ethical, and responsible\") that could be streamlined.\n\n3. The concluding sentence about the appendix could be integrated more efficiently.\n\n### Suggested Revisions:\n\nFor paragraph 1:\n**\"A dedicated section for AI disclosure, similar to funding statements or conflicts of interest, helps clarify AI contributions. Rigorous disclosure should be proportional to AI involvement, including specific prompts, model versions, and outputs. While journal standards vary, both Macquarie and QUT now require comprehensive documentation for postgraduate theses, including supervisory approval.\"**\n\nFor paragraph 2:\n**\"Implementation challenges exist, particularly around the practicality of 'active supervision and written consent' requirements. Both universities acknowledge this tension, placing primary accountability on supervisors while recognizing the need for clearer norms.\"**\n\nFor paragraph 3:\n**\"Responsible AI integration requires institutional guardrails and ongoing training. Applying FAIR principles to AI use enhances research transparency and reproducibility. Rather than prohibitions that might drive shadow use, institutions should equip researchers to evaluate and disclose AI contributions ethically. Appendix 1 provides an adaptable disclosure checklist for diverse research contexts.\"**\n\nThese revisions maintain the key content while improving flow, clarity, and conciseness. Would you like me to explain any particular aspect of these suggestions in more detail?",
          "citations": []
        }
      ],
      "sender": "assistant",
      "created_at": "2025-04-15T06:19:50.011209Z",
      "updated_at": "2025-04-15T06:19:50.011209Z",
      "attachments": [],
      "files": []
    }
  ]
}