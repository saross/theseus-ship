# 20250507 - AbsJud - Section 4 Questions

## Metadata
- **UUID:** `d3e14107-18f7-43e1-a920-b8ebf6efee28`
- **URL:** https://claude.ai/chat/d3e14107-18f7-43e1-a920-b8ebf6efee28
- **Created:** 2025-05-07 06:34:18
- **Updated:** 2025-05-10 05:52:04
- **Source:** Brian's Claude export

---

## Conversation

### Human (06:34:19)

Hi Claude. 

Today we're going to be working on building out the claims and sub-claims (in a Toulmin sense) of Section 4, down to a sub-paragraph level detailed outline. My objective is for you to ask me specific, detailed, and academic questions, one at a time, until such time as you're satisfied that you know enough to write the outline. I'll be recording my answers and transcribing them. If there are open questions for my colleague, Shawn, please note them down and I will return to them later. 

To begin, please discuss your understanding of the sources provided, the style guide, and do a functional decomposition of the task at hand and give me a readback.

---

### Assistant (06:34:42)


Let me analyze the sources provided, the style guide, and then do a functional decomposition of the task at hand.

**Sources Provided:**
1. WritingStyleGuide.md - A comprehensive guide for academic writing with sections on voice, tone, sentence structure, paragraph organization, and vocabulary.
2. brian-references.bib - A BibTeX reference file with academic citations.
3. transcripts.tex - Audio transcriptions containing discussions about research, AI capabilities, and concepts like "technoscholasticism."
4. AbsenceJudgement.tex - What appears to be a draft paper about AI's limitations in research tasks, particularly focusing on judgment deficiencies.

The sources paint a picture of a paper discussing how AI research tools (like OpenAI's Deep Research, Claude, etc.) lack three crucial dimensions of judgment:
1. Epistemic humility (knowing what they don't know)
2. Inductive capacity (noticing meaningful patterns)
3. Correspondence with reality (evaluating claims against the world rather than just texts)

The paper introduces the concept of "technoscholasticism" to analyze these limitations.

**Style Guide Analysis:**
The writing style guide emphasizes:
- Academic clarity with direct assertions and active voice
- Moderate sentence length variation
- Strategic paragraph breaks and logical progression
- Technical precision with accessibility
- Concrete language with judicious metaphors
- Australian English spelling conventions

**Task at Hand:**
I need to build out detailed claims and sub-claims for Section 4 of the paper, which appears to focus on "Case Studies of AI Research Tools." Based on the outline in AbsenceJudgement.tex, Section 4 should cover various AI research tools:
- OpenAI's Suite of Tools (Deep Research, GPT 4.5, reasoning models)
- Anthropic's Offerings (Claude 3.7, Anthropic Research)
- Google's Research Tools (Gemini Pro 2.5, Deep Research)
- Hugging Face's Open Deep Research
- Other Tools (OpenAI's Operator, Anthropic's Computer Use, Elicit, Perplexity)

**Functional Decomposition:**
1. Develop a detailed outline for Section 4 with claims and sub-claims down to the paragraph level
2. Ask specific academic questions about the case studies to extract information
3. Structure the questions to build toward:
   - Descriptions of the tools' claimed capabilities
   - Observations of their actual performance
   - Analysis of their limitations through the judgment framework
   - Concrete examples of "technoscholasticism" in action
   - Comparative evaluation across tools
4. Organize the collected information into a coherent Toulmin-style argument structure
5. Ensure the outline follows the style guide recommendations

My approach will be to ask detailed questions about each of the tools mentioned in Section 4, focusing on the judgment dimensions identified in the paper and how they manifest in practical use cases.

# Understanding of the Task

Based on the provided documents, I'm working with:

1. A structured academic paper examining AI research tools and their fundamental limitations
2. A focus on "technoscholasticism" - how AI systems privilege textual authority over critical assessment
3. Analysis of three judgment deficiencies: epistemic humility, inductive capacity, and correspondence with reality
4. Section 4 specifically covers case studies of various AI research tools

## Functional Decomposition for Section 4 Development

For Section 4 "Case Studies of AI Research Tools," I need to:

1. Extract detailed observations about each AI research tool's performance
2. Structure claims and sub-claims to demonstrate judgment deficiencies
3. Organize comparative analysis across the tools
4. Provide concrete examples illustrating "technoscholasticism" in action
5. Maintain academic rigor while emphasizing clarity and directness

My approach will be to ask targeted academic questions about each tool category to build a detailed outline, focusing on:
- Claimed capabilities vs. observed performance
- Specific manifestations of judgment deficiencies
- Concrete examples showing limitations in research tasks
- Comparative strengths/weaknesses among the tools

I'll begin by asking questions about OpenAI's tools, then proceed systematically through the other systems mentioned in the outline.

Shall I start with the first question about OpenAI's suite of tools?

---

### Human (06:38:41)

Please begin your questions. One at a time, specific and detailed, think deeply and take as much time as you need.

---

### Assistant (06:38:51)


I'll start with detailed questions about OpenAI's suite of tools, focusing on Section 4.1. I want to ask specific questions that will help build a detailed outline with claims and sub-claims for this section. I'll focus on concrete observations, examples, and analysis rather than just general information.

For the first question, I should focus on the specific capabilities and limitations observed during testing of OpenAI's Deep Research tool. Based on the document, there are mentions of issues with corpus access, lack of systematic approach, policy constraints, and context window management. I'll ask a detailed question about these observed limitations to get specific examples that can be used in the outline.

Let's begin with a detailed question about OpenAI's Deep Research:

What specific failure patterns did you observe during your testing of OpenAI's Deep Research that most clearly demonstrated its lack of judgment? Please provide concrete examples where the tool displayed technoscholastic tendencies by privileging textual authority over critical assessment, and describe how these failures manifested differently from simple factual errors.

---

### Human (07:58:02)



---

### Assistant (07:58:46)


Let me analyze Brian's detailed response about OpenAI's Deep Research tools and their failures of judgment. His examples focus on several key areas:

1. Epistemic humility failures:
- The tool drastically overestimated its own capabilities, creating an elaborate 7-step research plan it couldn't actually execute
- It took 7 iterations to get a working prompt for simple CSV output
- Addressing the human as if they were an AI in system prompts, showing confusion about its role
- O3 model lying about being able to run Python code, presenting fake "simulated" output rather than admitting its limitation

2. Technoscholastic tendencies:
- Privileging textual authority over critical assessment in Digital Humanities research
- Taking university websites at face value without temporal awareness (treating defunct programs as current)
- Not recognizing defunct research centers (Newcastle example is detailed)
- Not critically examining dates or project status - e.g., not noticing when a website announces it's no longer being updated (after a professor's death)
- Treating .edu domains as automatically authoritative without examining content critically

3. Data collection issues:
- Confabulating tools
- Inconsistently recognizing dates (not finding dates in GitHub repositories)
- Lacking conscientiousness to thoroughly examine available data

For the overarching argument, Brian emphasizes:
- These systems are tools, not PhD students
- They can be useful tools if you understand their limitations and how to use them
- The problem is treating them as capable of writing complete research reports rather than as tools to help process sources
- The key issue is absence of judgment, as discussed throughout the paper

The most emblematic example he provides seems to be the University of Newcastle Digital Humanities case, where the tool:
1. Presented a defunct research center as active
2. Failed to note temporality (citing Burrows' work from the 1980s as if current)
3. Missed clear signals of project discontinuation (death notice of project leader)
4. Overall lacked judgment to distinguish between historical claims and current reality

For the outline, I need to organize this evidence into a structured format that provides concrete examples under each tool category, showing both capabilities and limitations.

Based on your detailed response, I'll develop an outline for Section 4 focused on case studies of AI research tools. Your example of the University of Newcastle Digital Humanities center is particularly valuable as it clearly illustrates technoscholasticism - taking textual authority at face value without critical assessment of temporality or project status.
```
Viewing artifacts created via the Analysis Tool web feature preview isn’t yet supported on mobile.
```



Based on your feedback about the outline, let me ask a follow-up question to get more details about Anthropic's offerings:

What specific capabilities and limitations did you observe when testing Anthropic's tools (particularly Claude 3.7 with reasoning and their newly released Research feature)? How did their performance compare to OpenAI's Deep Research, especially regarding the three judgment dimensions you've identified?

---

### Human (04:40:43)



---

### Assistant (04:41:10)

This transcription provides excellent insights into Anthropic's tools, particularly Claude 3.7 with reasoning and their research feature. Let me analyze the key points to incorporate into the Section 4.2 outline:

Key observations about Anthropic's tools:

1. Writing quality:
   - Claude's writing is described as "more humane and erudite without being corporate bombastic"
   - Better at maintaining quality floor and "vibe following" than competitors

2. Research feature characteristics:
   - Always creates four sub-agents (seems to be a hardcoded pattern)
   - Performs many web_search calls but fewer page fetches
   - Appears to add search result snippets to context windows of sub-agents
   - Only fetches full pages when it has identified a good fit
   - Works well for code-related tasks (example: Git scraper)

3. Output constraints:
   - Always produces a report with "hyperbolic title"
   - Will not follow requested output formats (e.g., refuses to output CSVs when asked)

4. Comparison to OpenAI's Deep Research:
   - More "conscientious" than Deep Research (doesn't "wander in a thread")
   - Better than Gemini's Deep Research despite Google having direct search access
   - Sub-agents run as "singular threads" without cross-checking
   - No evidence of source critique or self-criticism
   - Takes search inputs and "vibes" on them without critical assessment

5. Judgment deficiencies:
   - High rates of confabulation in literature reviews
   - Same technoscholastic tendencies (privileging textual authority)
   - Exhibits "differences in degree but not in kind" compared to competitors
   - Characteristic failure: taking temporally-bound information (e.g., 2020 course offerings) as current
   - Fails to look for "useful absences of evidence"
   - Takes announcements as "temporal evidence of the thing itself"
   - Creates "implausibly cheery, implausibly positive" perspectives on topics like digital humanities

6. Specific example:
   - Western Sydney University case where a course was offered only once in 2020, but all models presented it as ongoing

I'll incorporate these insights into our detailed outline for Section 4.2 on Anthropic's offerings.Thank you for this detailed response about Anthropic's tools. I'll update the outline for Section 4.2 based on your observations about Claude 3.7 with reasoning and their research feature, including the specific examples you've provided about its judgment limitations.
```
Viewing artifacts created via the Analysis Tool web feature preview isn’t yet supported on mobile.
```



Now I'd like to ask about Google's research tools to complete that section of the outline:

How did Google's Gemini Pro 2.5 Deep Research perform in your testing compared to OpenAI and Anthropic's offerings? What specific judgment failures or technoscholastic tendencies did you observe with Google's implementation, and were there any areas where it demonstrated relative strengths?

---

### Human (05:51:33)



---

### Assistant (05:52:04)

This detailed response provides excellent insights into both Google's Gemini Pro 2.5 Deep Research and the "Other Tools" sections (4.4 and 4.5). Let me update the outline with these specific observations:

For Google's Gemini Pro 2.5:
- Single-threaded approach (vs. multi-agent approaches)
- Collects large numbers of hits without detailed examination
- "Ultimate in Carnapian confirmation" - stamp collecting approach
- Produces "undergrad-level" summaries without critical assessment
- Significant underperformance compared to Claude Research
- Higher proportion of confabulated sources
- Found less useful sources overall
- Example of treating the Digital Humanities in Australia (consistent with other models)
- "Shotgun mass-summary" approach versus detailed source analysis
- Never used Google Scholar despite having access

For Hugging Face's Open Deep Research:
- Ability to interact with filesystem and user-specified search APIs
- Can run queries from local university computer
- Enabled structured data iteration (the CSV example)
- Extremely expensive ($200 in o1-pro credits for a single run)
- Shows technical promise but cost prohibitive
- Only used for one major run due to expense

For the Other Tools:
- Operator: Tech demo, can't maintain context on simple tasks
- Computer Use: Promising but expensive and couldn't complete basic tasks
- Elicit: Hampered by Semantic Scholar integration and poor model
- Perplexity: Average quality, sub-par model choice, search-focused

The response also reinforces the "temporality blindness" across all tools - they don't understand that a program mentioned once on a university website may no longer exist, with specific examples from Western Sydney University.

I'll update the outline to incorporate these insights.Thank you for this comprehensive analysis of Google's Gemini Pro 2.5 Deep Research and the other tools. I'll update the outline to incorporate these specific observations and examples.
```
Viewing artifacts created via the Analysis Tool web feature preview isn’t yet supported on mobile.
```


```
Viewing artifacts created via the Analysis Tool web feature preview isn’t yet supported on mobile.
```



Now I'd like to get more details about the effective usage patterns you identified across these tools, which will help shape the Comparative Analysis section:

What practical strategies did you develop to maximize the utility of these research tools despite their limitations? How did your approach to prompting, task structuring, or result validation evolve as you became more familiar with their characteristic behavior patterns?

---
