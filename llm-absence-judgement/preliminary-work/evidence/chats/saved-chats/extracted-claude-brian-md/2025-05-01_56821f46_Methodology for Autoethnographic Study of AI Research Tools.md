# Methodology for Autoethnographic Study of AI Research Tools

## Metadata
- **UUID:** `56821f46-8b1f-4684-a892-a013189f3179`
- **URL:** https://claude.ai/chat/56821f46-8b1f-4684-a892-a013189f3179
- **Created:** 2025-05-01 09:47:31
- **Updated:** 2025-05-05 08:19:16
- **Source:** Brian's Claude export

---

## Conversation

### Human (09:47:32)

Hi Claude. Today we're going to be working on \section{Methodology}

To begin, please review AbsenceJudgement.tex and transcripts.tex. The way I've been working is by recording myself talking about this paper on the way into uni. I want you to be my academic writing partner in this. Specifically, I would like you to match my academic register when writing. 

My colleague and I are going to have a conversation where we talk about each question you pose. My objective is for you to take the current outline (noting that the entire section is only supposed to be about 500 words) and, through asking us questions, turn it into a paragraph by paragraph outline where we figure out the claims, warrants, and evidence of each paragraph.

The way I want you to work is to ask us one, specific, critical, and very academic question at a time until you think you have enough information to resolve the specific task at hand. You'll then give me a final plan of attack, and assuming I approve it, we'll integrate it in.

To begin, give me a functional decomposition of my instructions, a detailed summary of the sources I've given you, and a readback of what I think our first task should be.

---

### Assistant (09:47:56)


Let me break down your instructions for this collaboration:

## Functional Decomposition of Instructions:
1. Review AbsenceJudgement.tex and transcripts.tex to understand the current state of your paper
2. Match your academic register in all writing
3. Focus on the \section{Methodology} which should be about 500 words total
4. Break this section into a paragraph-by-paragraph outline
5. For each paragraph, help identify claims, warrants, and evidence
6. Ask specific, critical, academic questions one at a time
7. Continue asking questions until I have enough information for the task
8. Provide a plan of attack for your approval
9. After approval, integrate the content into the outline
10. Avoid unnecessary praise or flattery
11. Apply critical thinking and check claims/counterarguments
12. Transform your audio transcription inputs into academic content

## Detailed Summary of Sources:

### AbsenceJudgement.tex:
This is the main paper draft with a LaTeX structure. The paper discusses the limitations of AI in research tasks, particularly focusing on "deep research" capabilities. The paper introduces the concept of "technoscholasticism" to explain fundamental limitations in AI systems. The paper argues that despite claims of "agentic" capabilities, current AI systems lack three crucial dimensions of judgment: epistemic humility, inductive capacity, and correspondence with reality.

Key sections in the paper include:
1. Introduction
2. Theoretical Framework (with subsections on agency taxonomies, judgment deficiencies)
3. Methodology (our current focus)
4. Case Studies of AI Research Tools
5. Reflective Analysis
6. Research Tool Design Requirements
7. Conclusion

The Methodology section is currently an outline with these subsections:
- Autoethnographic Investigation
- Tool Selection Criteria
- Research Tasks Investigated
- Evaluation Framework

### transcripts.tex:
This contains transcriptions of your audio recordings discussing various aspects of the paper. The transcripts include your thoughts on:
- Discussion of utility and experiences
- Mundane utility observations
- Judgment and epistemic humility
- Current capabilities of tool-using non-agentic systems
- Agentic definitions
- Various methodological considerations
- Reflective analysis questions and answers

There are also specific sections labeled as responses to questions about Sections 5 and 6, methodology, inductive reasoning, and correspondence with reality.

## First Task Understanding:
Based on your instructions, I believe our first task is to develop a detailed paragraph-by-paragraph outline for the Methodology section, which should only be about 500 words total. I should ask specific academic questions to help flesh out the claims, warrants, and evidence for each paragraph, starting with the overall approach to the Methodology section.

I'll begin by asking a specific academic question about your methodological approach to gather information before proposing an outline structure.

# Functional Decomposition of Instructions

1. Review provided documents to understand paper context and content
2. Focus on drafting the Methodology section (~500 words total)
3. Match your academic register in all writing
4. Ask specific, critical academic questions one at a time
5. Use your answers to develop paragraph-by-paragraph outline with claims, warrants, and evidence
6. Present a plan of attack for your approval
7. Upon approval, integrate content into final outline
8. Apply critical thinking throughout and avoid unnecessary praise

# Source Summary

## AbsenceJudgement.tex
This draft paper introduces "technoscholasticism" as a framework to analyze fundamental limitations in AI research tools. It argues that despite marketing claims of "agentic" capabilities, current AI systems lack three crucial dimensions of judgment: epistemic humility, inductive capacity, and correspondence with reality. The paper employs autoethnographic investigation of frontier models from February 2025. The Methodology section currently contains placeholder subsections on autoethnographic investigation, tool selection, research tasks, and evaluation framework.

## transcripts.tex
Contains extensive audio transcriptions of your discussions about the paper, including reflections on judgment deficiencies in AI systems, experiences with various research tools, and responses to specific questions about different sections. The transcripts include detailed commentary on your autoethnographic approach, tool selection rationales, and evaluation methods.

# First Task
Based on your instructions, I understand our first task is to develop a detailed paragraph-by-paragraph outline for the Methodology section, beginning with targeted questions about your methodological approach to gather the necessary information before proposing a structure.

What specific methodological justification underlies your choice of autoethnography as the primary investigative approach for studying AI research tools, especially given potential critiques of subjectivity in this method?

---

### Human (10:02:31)



---

### Assistant (10:02:48)


This transcript provides valuable insights into their methodological approach, though it's somewhat conversational and jumps between topics. Let me extract the key points about their autoethnographic approach:

1. Expertise justification:
   - Both researchers have extensive experience (15 years) in digital humanities and building software tools
   - Experience with LLMs since late 2022 (particularly intensive use)
   - Background in history and philosophy of science
   - Research leadership positions at university and national level
   - Skill in prompting ("words good with AI" - described as an art)

2. Appropriateness of method:
   - "If not us, who?" - positioning themselves as uniquely qualified to evaluate
   - They embrace subjectivity rather than seeing it as a limitation
   - Compare to Latour's laboratory studies - getting "hands dirty" vs. just surveys
   - Need to "interrogate our own thoughts and observations" while building research
   - Practical necessity: they can't "sit with other people and watch them build a research paper"
   - Not generalizing or "revealing truths" but providing a case study

3. Timeliness considerations:
   - Studying a rapidly evolving field - "as we've written this, three new models with new capabilities have come out"
   - Implication that more traditional/lengthy methods would be outdated before completion

4. Methodological framing:
   - They characterize it as an "autoethnographic case study"
   - Focus on exploring "the frontier" - latest models, premium versions, APIs
   - Want to avoid "bad prompting" interfering with evaluation
   - Socio-technical research background informs approach

The transcript doesn't directly address the second part of O3's question about "concrete episodes from February-March audio logs" - they dismiss this as irrelevant since their "audio logs are just answering questions."

Based on this, I can formulate a more focused question that builds on what I've learned and helps develop their methodological justification further.

Your discussion provides valuable insights into your methodological justification for autoethnography. I understand you're emphasizing your combined expertise (15 years in digital humanities, tool development, LLM usage since 2022), the necessity of direct engagement rather than surveys, and the practical reality of studying rapidly evolving technologies.

Could you elaborate on how you structured your autoethnographic investigation specifically? What documentation methods did you employ beyond audio recordings, and how did you ensure analytical rigor despite the acknowledged subjectivity of your approach?

---

### Human (10:24:45)



---

### Assistant (10:25:03)


From this second discussion, I'm gaining a clearer picture of your methodology and tool selection approach:

1. On Autoethnographic Structure:
   - You used AI to interview yourselves about your experiences (self-interview technique)
   - You acknowledge this is fully subjective - there is no claim to analytical rigor in a traditional sense
   - You emphasize "intersubjective agreement" between the two of you as a form of validation
   - Your goal is persuasiveness through demonstration of proper research mode, not traditional qualitative rigor

2. On Tool Selection:
   - Selection was primarily based on what was available in the market
   - You evaluated multiple systems based on quick initial tests of utility
   - You disaggregated your research into different tasks (dataset creation, literature review, ideation/writing)
   - Different tools performed better for different tasks
   - OpenAI Deep Research and Hugging Face's Open Deep Research were most useful
   - Elicit and Perplexity performed poorly by comparison
   - You're considering creating a comparative table showing different tools' performance on the same task

3. On Counter-Arguments to Selection:
   - Cost limitations (requires significant financial investment)
   - Rapidly evolving landscape (tools change week by week)
   - Snapshot in time limitations

4. On Documentation:
   - Audio recordings of AI-prompted self-interviews
   - Zotero tagging to track which models found which sources
   - Comparative results tracking

I think I have enough information now to draft an outline for the Methodology section. I'll focus on structuring it to address the autoethnographic approach, tool selection, investigated tasks, and evaluation framework within the ~500 word constraint.

Based on your discussions, I now understand your methodology involved:
- Using AI to interview yourselves about your experiences
- Relying on intersubjective agreement between co-authors as validation
- Explicitly embracing subjectivity rather than claiming traditional rigor
- Systematically testing available tools based on practical utility

For the tools selection criteria, you primarily selected based on:
1. Commercial availability (what was on the market during your testing window)
2. Performance in initial screening tests (many failed to produce useful results)
3. Task-specific utility (some tools worked for literature reviews, others for data collection)

I have one more specific question before I can develop a complete paragraph-by-paragraph outline: What specific research tasks did you systematically evaluate these tools on, and what were your primary evaluation criteria for success or failure in each task?

---

### Human (10:37:36)



---

### Assistant (10:38:05)


Based on the discussions provided, I'll now synthesize the information about the methodology section of this paper. The authors conducted an autoethnographic investigation of AI research tools, specifically examining their utility in various research tasks.

Key points from the transcripts:

1. Autoethnographic approach:
   - Self-aware subjectivity with no claims to traditional rigor
   - Using AI to interview themselves about their experiences (self-interview technique)
   - Intersubjective agreement between co-authors as a form of validation
   - Leveraging their extensive experience (15+ years) in digital humanities and tool development
   - First-hand engagement rather than relying on surveys

2. Tool selection criteria:
   - Availability (what was on market during testing period)
   - Performance in initial screening tests
   - Task-specific utility (different tools for different research tasks)
   - They tested OpenAI Deep Research, Perplexity, Elicit, Hugging Face's Open Deep Research, plus various frontier models

3. Research tasks investigated:
   - They disaggregated the research process into distinct tasks:
     - Literature review/search
     - Dataset creation/collection
     - Ideation
     - Composition (including collaborative writing)
     - Revision (mentioned as a future step)

4. Evaluation framework:
   - Primary criterion was "mundane utility" - practical usefulness in research
   - This was broken down into:
     - Efficiency (does it make research faster?)
     - Quality (does it improve research outputs?)
   - Specific success indicators varied by task
   - They found literature review and data collection tasks showed higher utility
   - The composition process using AI interviews was "spooky" effective but time-consuming
   - They emphasized "richer data not faster data"

5. Key methodological insights:
   - Using LLMs as interlocutors to develop ideas and improve writing rigor
   - Contrasting with common usage patterns that focus on speed rather than quality
   - Documenting the process is onerous but essential

Now I need to structure this into a clear paragraph-by-paragraph outline for a ~500 word methodology section.

From your discussions, I understand your research tasks and evaluation criteria now include:

1. Literature review: Evaluated based on comprehensiveness and efficiency compared to traditional methods
2. Dataset creation: Success measured by ability to systematically identify relevant tools from specified journals
3. Ideation and composition: Assessed through the "auto-interview" technique that uses AI to elicit reflections

Your evaluation emphasizes "mundane utility" across two dimensions:
- Efficiency (making research faster or even possible)
- Quality (producing richer data, though sometimes at a time cost)

Let me propose a paragraph-by-paragraph outline for your 500-word Methodology section:
```
Viewing artifacts created via the Analysis Tool web feature preview isn’t yet supported on mobile.
```



Would this structure capture the essence of your methodology appropriately? The outline reflects your emphasis on autoethnography as a valid approach given your expertise, the practical selection criteria for tools, the systematic breakdown of research tasks, and your "mundane utility" evaluation framework.

Is there anything specific you'd like to adjust before we move forward with drafting the actual paragraphs?

---

### Human (07:17:15)

Write it.

---

### Assistant (07:17:39)


I'll now write the full Methodology section based on the approved outline. I need to keep the total word count to about 500 words, which means I should aim for concise, focused paragraphs that capture the essence of the methodology without unnecessary elaboration. I'll use the information provided in the transcripts and discussions to craft an academically rigorous section that matches their register and style.

I'll follow this structure:
1. Autoethnographic Investigation (~150 words)
2. Tool Selection Criteria (~100 words)
3. Research Tasks Investigated (~125 words)
4. Evaluation Framework (~125 words)

I'll make sure to emphasize:
- Their justification for autoethnography based on expertise
- The intersubjective validation approach
- The practical selection criteria for tools
- The systematic breakdown of research tasks
- The "mundane utility" evaluation framework with its efficiency and quality dimensions

Let me draft each subsection carefully while maintaining a cohesive flow throughout.

# Section 3: Methodology

## 3.1 Autoethnographic Investigation

This study employs an autoethnographic approach to investigate the capabilities and limitations of AI research tools. This methodological choice acknowledges the fundamental subjectivity of tool evaluation while leveraging the authors' combined expertise—spanning fifteen years in digital humanities, tool development, and socio-technical research—to provide informed critical assessment. As frontier models deployed in February 2025 represent emergent technologies experiencing rapid iteration, conventional empirical methodologies would prove impractical, requiring significant resources while producing quickly outdated results. Our investigation instead embraces what Latour terms "science in action," providing direct insight into the situated practice of research tool use. We structured our investigation through AI-facilitated self-interviews, employing large language models as interlocutors to elicit reflections on our experiences. This approach creates a recursive analytical framework where the tools under examination simultaneously facilitate their own evaluation. Rigor emerges through intersubjective validation between co-authors with complementary expertise in digital humanities practice and philosophy of science, ensuring that observations achieve consensus despite their inherently subjective nature.

## 3.2 Tool Selection Criteria

Our selection criteria prioritized three factors. First, commercial availability during our February-March 2025 testing window determined our initial candidate pool, as we sought to evaluate technologies accessible to researchers. Second, demonstrated minimal utility in screening tests eliminated several candidates that failed to produce usable outputs despite extensive prompt engineering. Third, functional capability within specific research tasks determined final inclusion, as some tools excelled in literature reviews while others proved more effective for data collection. We ultimately focused on OpenAI's Deep Research, Perplexity, Elicit, and Hugging Face's Open Deep Research for systematic evaluation, supplemented by frontier models (Claude 3.7, GPT-4.5) for ideation tasks. This selection inherently creates a "snapshot in time" limitation, as several new research-oriented tools were released during manuscript preparation. However, as our findings demonstrate, the fundamental capabilities and limitations identified transcend specific implementations.

## 3.3 Research Tasks Investigated

We systematically disaggregated the research process into discrete evaluable components representing the complete research lifecycle. For literature discovery, we assessed the tools' capacity to comprehensively identify and organize relevant scholarship while appropriately evaluating source quality. Dataset creation tasks tested systematic identification of software tools mentioned within specified academic journals, requiring sustained attention and consistent application of selection criteria. Ideation and composition tasks evaluated assistance in theoretical framework development and paper drafting, examining how effectively systems could advance arguments, identify weaknesses, and improve reasoning. This disaggregation enabled precise identification of differential utility across research stages, revealing where AI assistance provides genuine value versus where human judgment remains essential. Additionally, this task breakdown allowed direct comparison of tool performance across similar functions, highlighting capability differences among competing implementations.

## 3.4 Evaluation Framework

Our evaluation framework centers on "mundane utility"—practical research value rather than theoretical capability claims. This framework examines two dimensions: efficiency (speed and feasibility improvements compared to traditional methods) and quality (depth and thoroughness of outputs). We assessed each tool against the criterion of whether it demonstrably enhanced the research process or merely satisfied technological curiosity. Particular attention was directed toward instances where systems demonstrated—or failed to demonstrate—judgment capabilities claimed in marketing materials. Success indicators included task-specific metrics such as research coverage completeness, output accuracy, and researcher satisfaction. Documentation through conversation logs and output artifacts facilitated comparative analysis of system performance. This approach prioritizes what matters for working researchers: whether these systems make research "faster" or "better," recognizing that these dimensions often involve trade-offs—what one author characterized as producing "richer data, not faster data" in some contexts.

---

### Human (07:18:21)

Ok, we don't need subsections per paragraph. Rather than making these subsections, let's make this flow as a single section, please.

---

### Assistant (07:18:38)


The request is straightforward - the user wants me to reformulate the methodology section I just wrote to remove the subsection headers and make it flow as a single continuous section. I'll need to:

1. Remove all the subsection headers (3.1, 3.2, 3.3, 3.4)
2. Add transition sentences between the former subsections to create a smooth flow
3. Keep all the content but make it read as one cohesive section
4. Maintain the overall organization and key points

I'll transform the four separate subsections into a single flowing methodology section while keeping the content intact.

# 3. Methodology

This study employs an autoethnographic approach to investigate the capabilities and limitations of AI research tools. This methodological choice acknowledges the fundamental subjectivity of tool evaluation while leveraging the authors' combined expertise—spanning fifteen years in digital humanities, tool development, and socio-technical research—to provide informed critical assessment. As frontier models deployed in February 2025 represent emergent technologies experiencing rapid iteration, conventional empirical methodologies would prove impractical, requiring significant resources while producing quickly outdated results. Our investigation instead embraces what Latour terms "science in action," providing direct insight into the situated practice of research tool use. We structured our investigation through AI-facilitated self-interviews, employing large language models as interlocutors to elicit reflections on our experiences. This approach creates a recursive analytical framework where the tools under examination simultaneously facilitate their own evaluation. Rigor emerges through intersubjective validation between co-authors with complementary expertise in digital humanities practice and philosophy of science, ensuring that observations achieve consensus despite their inherently subjective nature.

The selection of tools for evaluation prioritized three factors. First, commercial availability during our February-March 2025 testing window determined our initial candidate pool, as we sought to evaluate technologies accessible to researchers. Second, demonstrated minimal utility in screening tests eliminated several candidates that failed to produce usable outputs despite extensive prompt engineering. Third, functional capability within specific research tasks determined final inclusion, as some tools excelled in literature reviews while others proved more effective for data collection. We ultimately focused on OpenAI's Deep Research, Perplexity, Elicit, and Hugging Face's Open Deep Research for systematic evaluation, supplemented by frontier models (Claude 3.7, GPT-4.5) for ideation tasks. This selection inherently creates a "snapshot in time" limitation, as several new research-oriented tools were released during manuscript preparation. However, as our findings demonstrate, the fundamental capabilities and limitations identified transcend specific implementations.

To facilitate systematic assessment, we disaggregated the research process into discrete evaluable components representing the complete research lifecycle. For literature discovery, we assessed the tools' capacity to comprehensively identify and organize relevant scholarship while appropriately evaluating source quality. Dataset creation tasks tested systematic identification of software tools mentioned within specified academic journals, requiring sustained attention and consistent application of selection criteria. Ideation and composition tasks evaluated assistance in theoretical framework development and paper drafting, examining how effectively systems could advance arguments, identify weaknesses, and improve reasoning. This disaggregation enabled precise identification of differential utility across research stages, revealing where AI assistance provides genuine value versus where human judgment remains essential. Additionally, this task breakdown allowed direct comparison of tool performance across similar functions, highlighting capability differences among competing implementations.

Our evaluation framework centers on "mundane utility"—practical research value rather than theoretical capability claims. This framework examines two dimensions: efficiency (speed and feasibility improvements compared to traditional methods) and quality (depth and thoroughness of outputs). We assessed each tool against the criterion of whether it demonstrably enhanced the research process or merely satisfied technological curiosity. Particular attention was directed toward instances where systems demonstrated—or failed to demonstrate—judgment capabilities claimed in marketing materials. Success indicators included task-specific metrics such as research coverage completeness, output accuracy, and researcher satisfaction. Documentation through conversation logs and output artifacts facilitated comparative analysis of system performance. This approach prioritizes what matters for working researchers: whether these systems make research "faster" or "better," recognizing that these dimensions often involve trade-offs—what one author characterized as producing "richer data, not faster data" in some contexts.

---

### Human (08:17:22)

Ok. Here's a bit from section 2 (to establish register) and then my edits of Section 3. (Remember, we're using Australian academic english here)

What revisions would you suggest?

---

### Assistant (08:17:54)


Let me analyze the text you've provided and identify any needed revisions for the Methodology section.

First, I note that you've included a portion from section 2 and then your edited version of Section 3. The section 3 content appears to be based on what I wrote but with significant customization:

1. You've added specific citations to your previous work
2. You've maintained the structure I proposed (autoethnographic approach, tool selection, research tasks, evaluation framework)
3. You've added nuance about the recursive nature of the AI questioning methodology
4. You've specified which tools you focused on (OpenAI Deep Research and Hugging Face's Open Deep Research primarily)
5. You're using Australian English spelling (e.g., "prioritised" instead of "prioritized")

For revisions, I'll look at:
1. Overall flow and coherence
2. Academic register consistency
3. Logical structure and transitions
4. Paragraph balance
5. Citation formatting and integration
6. Clarity and precision

Notable aspects:
- You've given more emphasis to the limitations of your methodology (acknowledging subjectivity)
- You've incorporated specific relevant citations throughout
- You've added explanations about your AI questioning methodology in Appendix I
- You've clarified the exact AI systems used and how you evaluated them

The overall section is strong, but I'll identify any potential improvements.

I think your edits strengthen the methodology section significantly, particularly through the added citations and enhanced explanations of your recursive AI-facilitated approach. A few suggested revisions:

1. **Paragraph balance**: The first paragraph of the autoethnographic approach section is quite long compared to others. Consider splitting after "...tools under examination simultaneously facilitate their own evaluation."

2. **Theoretical framing**: While you've cited Latour effectively, you might strengthen the autoethnographic justification by briefly referencing Schön's "reflective practitioner" concept since your methodology essentially involves reflection-on-action.

3. **Consistency in model naming**: In paragraph 3, you use both "frontier models" and specific model names. Consider standardizing how you refer to these (perhaps "frontier models (specifically Claude 3.7, GPT-4.5, and Gemini 2.5 Pro 03-25)").

4. **Transition clarification**: The transition between paragraphs 3 and 4 could be strengthened. Perhaps add a linking sentence like "This systematic task breakdown provided the structure for our evaluation framework."

5. **Mundane utility elaboration**: You might briefly explain that the concept of "mundane utility" comes from Mowshowitz to distinguish it from more speculative considerations of AI capabilities.

Overall, the section effectively communicates your methodological approach while maintaining appropriate academic register and acknowledging limitations. The recursive nature of using AI to interrogate AI capabilities is particularly well articulated.

---

### Human (08:19:09)

Great, give me a sentence for Schon, please?

---

### Assistant (08:19:16)


I need to craft a sentence about Schön's concept of the "reflective practitioner" that would fit well in the methodology section of the academic paper. The sentence should:

1. Reference Donald Schön's work on reflective practice
2. Connect it to their autoethnographic methodology
3. Maintain the Australian academic English register
4. Be concise but informative
5. Potentially include a citation

I know Schön's work focuses on how professionals reflect on their practice both during (reflection-in-action) and after (reflection-on-action) their activities. This seems very relevant to their autoethnographic approach where they're using AI to prompt reflection on their experiences using AI research tools.

Let me craft a sentence that could be inserted in the first paragraph of their methodology section:

Our investigation draws upon Schön's concept of the "reflective practitioner" where professionals engage in continuous reflection-on-action to develop insights from their experiences, embracing what Latour terms "science in action," to provide direct insight into the situated practice of research tool use.

---
