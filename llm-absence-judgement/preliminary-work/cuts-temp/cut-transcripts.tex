\subsection*{Additional Evaluative Discussion}
Beyond the epistemic humility failures and technoscholastic tendencies detailed above, our extended evaluations revealed further judgment deficiencies in Deep Research's literature review capabilities. The system consistently produces what we characterize as "undergraduate source soup"—a heterogeneous mixture of sources ranging from high-quality journal articles and government reports to random blog posts of dubious scholarly value.

A particularly telling pattern is its prioritization of less authoritative sources when more authoritative alternatives exist. The system frequently cites a researcher's blog rather than their peer-reviewed journal articles, or links to GitHub repositories instead of academic websites. This demonstrates a failure to hierarchically evaluate source authority within disciplinary contexts.

While the system does provide utility—accelerating the initial gathering phase of research materials—it requires substantial human judgment to filter and evaluate its outputs. Approximately two-thirds of the sources in our bibliographic dataset originated from Deep Research, but these required extensive critical evaluation, with many ultimately discarded as unsuitable. The remaining sources required manual discovery through traditional methods including citation chasing and disciplinary database searches.

This positions Deep Research as comparable to an entry-level research assistant rather than the "agentic" system claimed in marketing materials. It provides net research value but must remain under close supervision, with researchers applying their own judgment to compensate for the system's inability to distinguish between authoritative sources and peripheral content. The system fundamentally cannot replace scholarly judgment at any stage of the research process, despite its utility in accelerating certain mechanical aspects of discovery.

Its performance varies with context window management, with observable degradation in output quality as prompts approach length limits. This technical constraint further undermines its reliability, as the system does not consistently indicate when it has reached operational capacity—instead producing increasingly compromised outputs without warning.







\section*{Section 4.2: Anthropic Tool Assessment - Question 1}

\subsection*{Question Text}
For section 4.2, considering Anthropic's offerings, Claude 3.7 reasoning and Anthropic research, what specific failure patterns related to judgment or manifestations did you observe? How did they contrast?

\subsection*{Transcription}
Uh, section four.
This one's from Gemini.
Actually no, let's go with 4.5. So, um, [Jippet / dʒɪpɪt] 4.5.
In your practical interactions with the... No, let's go with Gemini. I don't know.
For section 4.2, considering Anthropic's offerings, Claude 3.7 reasoning and Anthropic research, what specific failure patterns related to judgment or manifestations did you observe? How did they contrast? Cool.

So,
this is a day after my previous answer.
Um,
I put
the same digital humanities prompt
through all three.
Um,
there were differences in degree but not in kind,
especially in terms of characteristic errors.
Um,
and and I would say that the character- characteristic
technē scholastic failure
of these models
is the lack of context, i.e. application of judgment,
where
they can infer
from
useful absences of evidence.
Um, so,
I want to say WSU.
What?
Western Sydney.
Um,
ah, I'll need to look up the the specifics, but like the thing that I remember that that's quite useful here
is
they saw that there was a course
from search results, or probably a unit. Let's
an offering
in search results from WSU.
This was in
2020.
Now, I,
a human,
knowing how universities work,
immediately
clicked on the little
year selector
and
I looked to see
when that course was offered,
or when that unit was offered.
It was offered exactly once in 2020.
Um,
all of all of them fell afoul of this pattern,
um,
in one way or the other.
Uh, also this section shouldn't be too long, so I'm I'm happy to speak
to use this as the characteristic
failure mode.
Because like we're talking
Gemini made a 47 page report.
Claude did its own thing, so on and so forth.
I was checking Australia's results specifically,
mostly because I I know the venue, right?
And so what I found was
they all
I mean, they were all a lot more positive
than they should have been.
Um,
they are
extrapolating
quite a lot from a single
um, conference in Australia, but like the problem is is that
they found evidence
to support their claims.
E- now, this was a revised prompt, so I ran it on all three,
where I explicitly told it to look for um, confirmation, discontinued, and whatnot.
The problem is
it was looking for announcements
of discontinuation.
Um,
wasn't looking for death announcements, because one,
I and I think a previous answer touched on this that
one lab was was shut down because the Professor Emeritus of the lab died.
Um,
they weren't checking to see if the names associated with the labs still worked there.
But more to the point,
they were taking announcements of things that happened in 2020
as
evidence of continuity.
Um,
Now, obviously,
I could adjust my prompt
and tell it how to
sensemake
around
absences of evidence.
Right, like
I am not treating this problem
as a fundamental failure.
I am treating it as characteristic
of
a broader
epistemic issue
with the judgment
of these models.
So like,
Claude spun up six
sub-agents to do this, that's fine.
But
all of the sub-agents
ran as singular threads.
All the sub-agents ran as singular threads,
which meant that there was no
cross-checking going on.
Um,
there was no trying to build a general picture through induction
of
what was going on at any one moment.
It was taking announcements,
taking broad
um,
taking the words on the page
as
temporal evidence of the thing itself.
Um,
so, okay.
So like,
this is a perfect illustration of technē scholasticism,
because it it's saying
the words on the page are this, the words from the university,
therefore this is positive evidence of this thing
without any need to go, cool,
why did they write this,
when did they write this, and
if our expectation of these as lively extant programs is to be taken,
what patterns of writing should they should we see. And so what what we found, what I found
with this these agents is they did the task but failed to accomplish the goal
because they painted this this
implausibly cheery, implausibly positive
um,
perspective
of digital humanities that simply isn't so.
And so,
that actually concludes my answer.